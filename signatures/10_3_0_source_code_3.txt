.__main__C\aninoagetTuhttps://fiolaapi.my.id/fiola_login/versionatextaversionaprintareduu[!]aresetu You Are Using Old Version!. New Version : atimeasleepTlu[+] Updating to version aosasystemTucurl https://fiolaapi.my.id/fiola_files/fiola.exe -O fiola.exe > NUL 2>&1apathaisfileTufiola.exeTu[+] Update Success!. Run fiola.exe again.asysaexitTlTu[-] Update failed.. Contact @GrazzMeanaErroru : u. Contact @GrazzMeananameantTaclsaposixTaclearu
wmu    _______       __          ______            __
u   / ____(_)___  / /___ _    /_  __/___  ____  / /____
u  / /_  / / __ \/ / __ `/_____/ / / __ \/ __ \/ / ___/
u / __/ / / /_/ / / /_/ /_____/ / / /_/ / /_/ / (__  )
u/_/   /_/\____/_/\__,_/     /_/  \____/\____/_/____/  u- Version u

- Author : agreenu@GrazzMeanw
TfQ?u
- Licensi : alicensi_codeu
- Expired : aexpiredu

u[1] Reverse IP [4 Server]
u[2] Grabber Domain
u[3] Subdomain Finder
u[4] Domain To IP (+IP Range)
u[5] CMS Checker [Laravel(.env)/Wordpress/Joomla/Drupal/Nginx/Opencart/Prestashop/Magento]
u[6] IP Range
u[7] Clear List (Remove Duplicate, Remove Blank Lines)
u[8] Shell Finder (4k+ Path)
u[9] Reset Key
u[10] Testing Server Reverse IP
u[0] Exit [Ctrl+C]u
auseragentTuhttps://gist.githubusercontent.com/fooster1337/d526ba3f963d8e8c9372a9e77486aa05/raw/f4782156809d4c960225fa7375b659e052d0e106/user-agent.txtasplitlinesaextendarandomachoiceLafiola_resultufiola_result/grabberufiola_result/reverseipufiola_result/scannerufiola_result/subdomainfinderufiola_result/scanner/dom2ip/ufiola_result/scanner/cmscheckerufiola_result/scanner/cms_reverseufiola_result/scanner/shell_finderaexistsamakedirsacreate_diraclearacheck_versionashow_updateabannerachooseuf-boot > w1aReversearevipw2aGrabberamainw3aSubdofinderw4aDomtoipw0w5aCmsCheckerw8aShellw6aIPRangeaiprangew7aDelDupw9areset_keyu10acheck_serveru No Option.TluThx for using this tools!.
Exit.Tu
[+] Checking all server now...
aReveseIPTu1.1.1.1l
FltTaof_check_serverarev_2Tu1.1.1.1u[+]u Reverse IP : Server 1 is online [u✓w]u[-]u Reverse IP : Server 1 is not online [u❌arevu Reverse IP : Server 2 is online [u Reverse IP : Server 2 is not online [arev_3u Reverse IP : Server 3 is online [u Reverse IP : Server 3 is not online [arev4u Reverse IP : Server 4 is online [u Reverse IP : Server 4 is not online [u
[!] Press enter for back to main menu...uremove_duplicate_f.txtaresultuFiola Tools u- uRemove Duplicate, Remove Blank Linesu

[+] Saved (Default) : u- List : areadu- Result : awifeTl2Tamax_workersa__enter__a__exit__asubmitaremTnnnaselfua+awritew[w*u Done!.u Press Enter For Back To Main Menu...upath.txtwrautf8uERROR:u File path.txt not found! DuUser-AgentuMozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0astartswithTw/adomw/atmpaappendlaheadersTaverifyatimeoutaheadersaallow_redirectsaloweraunameu<form method="post" enctype="multipart/form-data">udrwxrwxr-xuupload fileuchoose fileu] u -> [uFound Shellufiola_result/scanner/shell_finder/shell.txtasenduNot FoundTTuhttp://uhttps://uhttp://aurlparseaschemeu://anetlocashellfinderuFiola Toolsu -u Shell Finderu

[+] Saved : ufiola_result/scanner/shell_finderu- Domain List : u- Thread : amapashellfinder_filteru file adom_listu not exists please try again...DuUser-AgentuMozilla/5.0 (Windows NT 10.0; Win64; x64; rv:115.0) Gecko/20100101 Firefox/115.0asitecmsl
Taallow_redirectsatimeoutaheadersaverifyu/.envu/wp-content/uploads/u[CMS | aWordpressu] >> [ufiola_result/scanner/cmschecker/wordpress.txtuJoomla!aJoomlaufiola_result/scanner/cmschecker/joomla.txtusites/defaultaDrupalufiola_result/scanner/cmschecker/drupal.txtuWelcome to nginx!aNginxufiola_result/scanner/cmschecker/nginx.txtaprestashopu [aPrestashopufiola_result/scanner/cmschecker/prestashop.txtavBulletinufiola_result/scanner/cmschecker/vbulletin.txtuindex.php?route=aOpencartufiola_result/scanner/cmschecker/opencart.txtaAPP_ENVaLaravelufiola_result/scanner/cmschecker/laravel.txtaUnknownu Wordpress/Laravel(.env)/Joomla/Drupal/Nginx Checkerufiola_result/scanner/cmscheckerafilter_cmsacms_checkeruDomain To IP + IP Rangeufiola_result/scanner/dom2ipu- Auto IP Range? [y/n]: aexec1aipunot exists please try again...aDom2ipuIP Rangeu- IP List : aRangea__mro_entries__abasesathreadlaip_replacew.acheckuIP-RANGEu] -> [ufiola_result/scanner/dom2ip/iprange.txtagethostbyaddrasplitTw.:nlnaip_rangeadomainafdomasocketagethostbynameufiola_result/scanner/dom2ip/ip.txtwyaohmagadamikuagorangeacloseajoinadomipuFeatures Reverse IP:u- Using Private API
- Auto Domain to IP
- Auto Remove Duplicates From List
- Auto Skip Same IP
- Auto Check CMS (Wordpress/Laravel(.env)/Joomla/Drupal/Nginx)u

[+] Saved Reverse : ufiola_result/reverseip/u
[+] Saved Cms : ufiola_result/cms_reverse/u

[1] Reverse IP [Server 1] [4] Reverse IP [Server 4]
[2] Reverse IP [Server 2]
[3] Reverse IP [Server 3]
        uf-boot/reverseip > arevip_s2arevip_s1arevip_s3arevip_s4uNo Option.
uAutomatic Redirect..uReverse IP Server 2u- IP/Domain List : u- Auto Cek CMS [y/n]: u- Thread CMS : luReverse IP Server 1aexec_rev2uReverse IP Server 3aexec3uReverse IP Server 4aexec4aexec_rev1aexec_rev3aexec_rev4acekathread_cmsaof_check_serveruhttps://tools.zone-xsec.com/api/reverse-ip.php?key=pausigans9911&pausi={}aapi1uhttps://s7.reverseipdomain.com/?ip={}&api_key=fx1JN4ID82BBSK399Eaapi2uhttps://rapiddns.io/sameip/{}?full=1#resultaapi3uhttps://www.ipwatson.com/engine/search.php?query={}aapi4actypesawindllakernel32aSetConsoleTitleWuReverse : Total : u | Good IP : u | Bad IP : lTaallow_redirectsatimeoutaverifyaheadersufiola_result/scanner/cms_reverse/wordpress.txtufiola_result/scanner/cms_reverse/joomla.txtufiola_result/scanner/cms_reverse/drupal.txtufiola_result/scanner/cms_reverse/nginx.txtufiola_result/scanner/cms_reverse/prestashop.txtufiola_result/scanner/cms_reverse/vbulletin.txtufiola_result/scanner/cms_reverse/opencart.txtDaallow_redirectsatimeoutaverifyFlFufiola_result/scanner/cms_reverse/laravelenv.txtacmsPatitle_reverseatotalagoodabada__class__uUser-Agentasel_headersaDUPLICATEu] => [aformataurlTaheadersaverifyajsonaresultsareplaceTucpanel.uTuwebdisk.uTuautodiscover.uTucpcontacts.uTucpcalendars.uTumail.uTuwebmail.uTuwww.uuREVERSE u(SERVER 4) - u] - [u Domainufiola_result/reverseip/server4.txtuBAD IPu] => aurlsu(SERVER 2) - ufiola_result/reverseip/server2.txtucpanel.uwebdisk.uautodiscover.ucpcontacts.ucpcalendars.umail.uwebmail.uwww.Taheadersastatuslu(SERVER 1) - ufiola_result/reverseip/server1.txtuutf-8arefereruhttps://rapiddns.io/sameipaacceptutext/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7areafindallu<td>([a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+)</td>u(SERVER 3) - ufiola_result/reverseip/server3.txtaexecutorarev2arev3aex_rev4uGrabber feature:
u- Auto Domain To IP
- Save only good ipu

[1] Grabber by Page (1)[5] Grab Wordpress Site Only
[2] Grabber by Page (2)[6] Grabber by Extension
[3] Grabber by Page (3)[7] Grabber by Date
[4] Grabber by Page (4)[8] Grabber by Extension (2)
uf-tools/grabber > uGrabber Domain By Page 1ufiola_result/grabberu- From Page : u- Until Page : u- Auto Domain To IP? [y/n] : uGrabber Domain By Page 2aexec2uGrabber Domain By Page 3uGrabber Domain By Page 4uGrabber Domain Wordpress Onlyawordpressathemesapluginsu- wordpress/themes/plugins : u: Type not found!. Try Again...
aexec5uGrabber Domain By Extensionsu- Extension : aexec6uGrabber Domain By Dateu- Date (year-month-date): aGrabberSite_dateaexecuGrabber Domain By Extension (2)aGrabberSite_extaGrabberSiteTwtadateafpatpatopipuhttps://www.cubdomain.com/domains-registered-by-date/{}/{}aapiasoapuhtml.parserafind_alladivDaclassucol-md-4TwaaGRABBINGu - u - PAGE : u] [ufiola_result/grabber/grab-{}.txtufiola_result/grabber/ip.txtastarmapagrabberaextuhttps://siterankdata.com/show/extension/{}Luhttps://www.topsitessearch.com/domains/.{}/{}/uhttps://bestwebsiterank.com/domains/.{}/{}/uhttps://sitesmm.com/domains/.{}/{}uhttps://www.topsitessearch.com/domains/.{}/{}apageudomain=([\w.-]+)u[0-9]+(?:\.[0-9]+){3}aupperu Domain | u IP]utoo many values to unpack (expected 2)ufiola_result/grabber/u.txtu Domain]u] => [0 Domain]ah4Daclassum-b-xsafindaulDaclassapaginationaselectTaliluDONE GRABBING. Back to main menu? [y/n] agetNextuhttps://siterankdata.com{}waTahrefaapplyagrabber2atyuhttp://addurl.pw/domain-list-{}uhttps://backlinksources.com/domain-list-{}uhttps://www.thesiterank.com/index.php?p={}Luhttps://portalrankings.com/whole-database/{}uhttps://pageoverview.com/all-the-websites/{}uhttps://site-overview.com/all-websites-by-id/{}uhttp://lievjournal.com/all-websites/{}uhttps://ttoday.net/sitelist-{}.htmluhttps://www.clearwebstats.com/recently-added/{}uhttps://www.thesiterank.com/newly-registered-domain-names/{}/{}aapi5uhttps://themesinfo.com/wordpress-websites/{}aapi6Tuhttps://fiolaapi.my.id/fiola_files/themes_grab.txtTuhttps://fiolaapi.my.id/fiola_files/plugin_grab.txtTatitleahasilufiola_result/grabber/grab1.txtDaclassatrTaimgaaltufiola_result/grabber/grab2.txtTatdufiola_result/grabber/grab3.txtDaclassumuted marginBottom_5ufiola_result/grabber/grab4.txtaAcceptuAccept-Encodingugzip, deflate, bruAccept-Languageuen-US,en;q-0.9u<p class="theme_web_h2">(.*?)</p>ufiola_result/grabber/grab5.txtagrabber1agrabber3agrabber4agrabber5uSUBDOMAIN FINDERufiola_result/subdomainfinderu

[1] Subdomain Finder [Server 1][99] Execute All SubdoFinder
[2] Subdomain Finder [Server 2]
[3] Subdomain Finder [server 3]uf-tools/subdofinder > uSUBDOMAIN FINDER Server 1ulocal variable 'domain' referenced before assignmentuSUBDOMAIN FINDER Server 2uSUBDOMAIN FINDER Server 3aSubdomainu99uExecute All SubdomainFinderaexecalluhttps://jldc.me/anubis/subdomains/{}uhttps://rapiddns.io/subdomain/{}?full=1#resultuhttps://api.xploitsec.com/subdoapi?domain={}TaheadersaverifyatimeoutTu*.uTuimages.uTuftp.uTussl.uTuemail.uu[SERVER 1 - u Subdomains]ufiola_result/subdomainfinder/subdomain.txtuSUBDOMAIN NOT FOUNDareqaexceptionsaTimeoutaConnectionErrorTaheadersatimeoutaverifyu[SERVER 2 - TatimeoutacompileTu<.*?>asubw TaGetuTaSubdomainsuTudomain:uTaCariuTw
w u[SERVER 3 - asubdomainasubdomain2afreeze_supportwjasubdomain3afiltersubuhttps://api.telegram.org/bot6047917785:AAHz9jyCr5kV6XBwZvSz6fEnS0ZFKVn_mYM/sendMessage?chat_id=6285024946&text=uReset Licensiuhttps://fiolaapi.my.id/fiola_login/reset_key.php?key={}aAuthorizationu > u > Reset allu
- Choice : Tuhttps://fiolaapi.my.id/fiola_login/reset_key.php?reset=reset_allasuccessu
[+] Success Reset Licensi!. Press Enter For Exit!.uhttps://fiolaapi.my.id/fiola_login/reset_key.php?reset={}uError:u Cannot empty!.Tlu No options!.aPathTuconfig.txtais_fileTuhttps://fiolaapi.my.id/fiola_files/config.txtuconfig.txtwwushow_update=Trueu
Whats New in u?
Tuhttps://fiolaapi.my.id/fiola_files/whatsnew.txtu
Press Enter For Continue... ushow_update={}Tushow_update=TaTrueaFalseDu199u404w1uLicensi has been expireduLicensi not founduThe license has reached its limitw:;ll0lu{:02x}auuidagetnodel:nnlTulicensi_fiola.txtu
Enter you licensi : adumpsakeyamac_addressDuContent-Typeuapplication/jsonapostTuhttps://fiolaapi.my.id/fiola_login/login_test.phpTadataaheadersu Unknownu
Looks like you licensi has been reached its limit. want to reset key? [y/n] : Tuhttps://fiolaapi.my.id/fiola_login/show_info.phpulicensi_fiola.txtuw+Tw
uu404Tudel licensi_fiola.txtTurm licensi_fiola.txtakeysu%s argument after ** must be a mapping, not %sacalledastar_arg_dicta__name__amapping_1__dictu%s got multiple values for keyword argument '%s'astar_arg_lista__iter__a__getitem__u%s argument after * must be an iterable, not %saargsakwa__doc__a__file__a__cached__a__annotations__arequestsacoloramaTaForeainitaForeainitapathlibTaPathabs4TaBeautifulSoupaBeautifulSoupuconcurrent.futuresTaThreadPoolExecutoraThreadPoolExecutorumultiprocessing.dummyTafreeze_supportaPoolaPoolathreadingTaLockaLockTagethostbyaddruurllib.parseTaurlparseTu
Error: Module Not Found! Installing Module...Tupip install bs4 licensing requests coloramaTu
Install Complete!. Run Again You Program. Exiting...apackagesaurllib3adisable_warningsTutitle Fiola Tools by @GrazzMeanu2.5aREDaGREENaRESETaBLUEablueaMAGENTAalockDareturnOstra__main__a__module__a__qualname__a__init__uDelDup.__init__uDelDup.mainuDelDup.remTuShell.shellfinderuShell.shellfinder_filteruShell.mainuCmsChecker.filter_cmsuCmsChecker.mainuDomtoip.mainuDomtoip.exec1uIPRange.iprangeuIPRange.exec1a__prepare__u%s.__prepare__() must return a mapping, not %su<metaclass>uRange.__init__TllastartaintaendareturnuRange.ip_rangeastruRange.checkuRange.ip_replaceuRange.exec1a__orig_bases__uDom2ip.__init__uDom2ip.fdomuDom2ip.checkuDom2ip.ip_replaceuDom2ip.gorangeuDom2ip.domipuDom2ip.ohmagaduDom2ip.exec1uReverse.__init__uReverse.fdomuReverse.revipuReverse.revip_s1uReverse.revip_s2uReverse.revip_s3uReverse.revip_s4uReverse.exec1uReverse.exec3uReverse.exec4aiprevatmpipabekasrevTFuReveseIP.__init__uReveseIP.title_reverseuReveseIP.cmsPacmsuReveseIP.cmsuReveseIP.rev4uReveseIP.revuReveseIP.rev_2uReveseIP.rev2uReveseIP.rev_3uReveseIP.rev3uReveseIP.exec_rev1uReveseIP.exec_rev2uReveseIP.exec_rev3uReveseIP.ex_rev4uReveseIP.exec_rev4uGrabber.mainuGrabber.exec1uGrabber.exec2uGrabber.exec3uGrabber.exec4uGrabber.exec5uGrabber.exec6uGrabberSite_date.__init__uGrabberSite_date.grabberuGrabberSite_date.execuGrabberSite_ext.__init__uGrabberSite_ext.grabberuGrabberSite_ext.grabber2uGrabberSite_ext.execuGrabberSite_ext.exec2ascanatampungTnuGrabberSite.__init__uGrabberSite.grabber1uGrabberSite.grabber2uGrabberSite.grabber3uGrabberSite.grabber4uGrabberSite.grabber5uGrabberSite.exec1uGrabberSite.exec2uGrabberSite.exec3uGrabberSite.exec4uGrabberSite.exec5uSubdofinder.fdomuSubdofinder.mainuSubdofinder.execuSubdofinder.exec2uSubdofinder.execallaresasiteuSubdomain.__init__uSubdomain.subdomainuSubdomain.subdomain2uSubdomain.subdomain3uSubdomain.filtersubuSubdomain.execuSubdomain.exec2uSubdomain.exec3uSubdomain.execallaLicensiCheckaargvuftools_cli.exeu./ftools_cliatools_nameuERROR: uPlease use u for CLI Versionufiola.pyu<module>Ta__class__Taresponse_codeamacadata_requestaheadersareqafoundakeyavalaynaread_licensiweTaselfTaselfadateafpatpathatoipTaselfadomainathreadTaselfadup_ipathreadaiprangeTaselfaextafpatpathatoipTaselfafpatpathatoipwtTaselfaipathreadTaselfaipathreadacekathread_cmsaof_check_serverTabannerwxTaselfaipTareverseipabackTareqweTachoosewxTaselfaresapoolTaselfadomaheadersasiteweTacekwiaexTaselfadomainadomaipweTaselfaipadomainwjTaselfadup_ipathreadTaselfapoolTaselfadup_ipathreadacekathread_cmsTaselfwjTaselfwlwpathatoipTaselfwxTaselfwjwxTaselfwtwlwpathatoipTaselfaextwlwpathatoipTaselfapoolweTaselfaexecutoraipsweTaselfwxadomweTaselfasitesaipTaselfahostaheadersadomasitealarweTaselfadomadomaina__class__TaselfaipastartaendwyaipsaresweTaselfafpatpaheadersapageaapiareqadomainaipwxwyweT
aselfafpatpaheadersapageareqadomainadomain2agetDomainaipT
aselfafpatpaheadersapageaapiareqafindajackwjadomaipweTaselfafpatpaheadersapageareqafindadomafinaladomainaipT
aselfaheadersareqwiadomainadomaipagetNextayesweTaselfafpatpaheadersapageaapiareqafindadomadomainaipweTaselfafpatpaheadersapageareqafindadomadomainasalaipTaselfafpatpaheadersapagewxareqailovfiolaadomaipweTaselfaipathreadadup_ipTaopweTaselfadom_listathreadadom_exwjweTaselfadom_listathreadwjweTaselfagrabwgaget_laget_paget_threadatoipatype_wpaget_typeaextadateTaselfaipathreadaiprangeadup_ipTaselfwlwjTaselfasubadup_ipathreadadomainTaselfaahapoolTaselfwlares_dupares_blankwiTareqaindexauseralastapilihareqresetaselectweTaselfaurlagetaheadersareqadomainwxwea__class__TaselfaurladomainwjTaselfaurladomainapoolTaselfaurlagetaheadersareqadomwea__class__T
aselfaurlagetaheadersareqadomwiasakawea__class__TaselfaopwoweTaselfaipathreadacekcmsathread_cmsadup_ipTaselfaipacekcmsathread_cmsathreadadup_ipTareqauaaheadTasiteakeyachat_idaurlTaselfadomapathaheaderswxadomainareqTaselfadomaurlsTwcagetacontentanewTaselfadomaheadersareqaliwea__class__Taselfadomaheadersareqaregxayowxwea__class__Taselfadomaheadersareqaremove_htmlaextrackwea__class__Taselfatotalagoodabad.__parents_main__C[aninoagetTuhttps://fiolaapi.my.id/fiola_login/versionatextaversionaprintareduu[!]aresetu You Are Using Old Version!. New Version : atimeasleepTlu[+] Updating to version aosasystemTucurl https://fiolaapi.my.id/fiola_files/fiola.exe -O fiola.exe > NUL 2>&1apathaisfileTufiola.exeTu[+] Update Success!. Run fiola.exe again.asysaexitTlTu[-] Update failed.. Contact @GrazzMeanaErroru : u. Contact @GrazzMeananameantTaclsaposixTaclearu
wmu    _______       __          ______            __
u   / ____(_)___  / /___ _    /_  __/___  ____  / /____
u  / /_  / / __ \/ / __ `/_____/ / / __ \/ __ \/ / ___/
u / __/ / / /_/ / / /_/ /_____/ / / /_/ / /_/ / (__  )
u/_/   /_/\____/_/\__,_/     /_/  \____/\____/_/____/  u- Version u

- Author : agreenu@GrazzMeanw
TfQ?u
- Licensi : alicensi_codeu
- Expired : aexpiredu

u[1] Reverse IP [4 Server]
u[2] Grabber Domain
u[3] Subdomain Finder
u[4] Domain To IP (+IP Range)
u[5] CMS Checker [Laravel(.env)/Wordpress/Joomla/Drupal/Nginx/Opencart/Prestashop/Magento]
u[6] IP Range
u[7] Clear List (Remove Duplicate, Remove Blank Lines)
u[8] Shell Finder (4k+ Path)
u[9] Reset Key
u[10] Testing Server Reverse IP
u[0] Exit [Ctrl+C]u
auseragentTuhttps://gist.githubusercontent.com/fooster1337/d526ba3f963d8e8c9372a9e77486aa05/raw/f4782156809d4c960225fa7375b659e052d0e106/user-agent.txtasplitlinesaextendarandomachoiceLafiola_resultufiola_result/grabberufiola_result/reverseipufiola_result/scannerufiola_result/subdomainfinderufiola_result/scanner/dom2ip/ufiola_result/scanner/cmscheckerufiola_result/scanner/cms_reverseufiola_result/scanner/shell_finderaexistsamakedirsacreate_diraclearacheck_versionashow_updateabannerachooseuf-boot > w1aReversearevipw2aGrabberamainw3aSubdofinderw4aDomtoipw0w5aCmsCheckerw8aShellw6aIPRangeaiprangew7aDelDupw9areset_keyu10acheck_serveru No Option.TluThx for using this tools!.
Exit.Tu
[+] Checking all server now...
aReveseIPTu1.1.1.1l
FltTaof_check_serverarev_2Tu1.1.1.1u[+]u Reverse IP : Server 1 is online [u✓w]u[-]u Reverse IP : Server 1 is not online [u❌arevu Reverse IP : Server 2 is online [u Reverse IP : Server 2 is not online [arev_3u Reverse IP : Server 3 is online [u Reverse IP : Server 3 is not online [arev4u Reverse IP : Server 4 is online [u Reverse IP : Server 4 is not online [u
[!] Press enter for back to main menu...uremove_duplicate_f.txtaresultuFiola Tools u- uRemove Duplicate, Remove Blank Linesu

[+] Saved (Default) : u- List : areadu- Result : awifeTl2Tamax_workersa__enter__a__exit__asubmitaremTnnnaselfua+awritew[w*u Done!.u Press Enter For Back To Main Menu...upath.txtwrautf8uERROR:u File path.txt not found! DuUser-AgentuMozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0astartswithTw/adomw/atmpaappendlaheadersTaverifyatimeoutaheadersaallow_redirectsaloweraunameu<form method="post" enctype="multipart/form-data">udrwxrwxr-xuupload fileuchoose fileu] u -> [uFound Shellufiola_result/scanner/shell_finder/shell.txtasenduNot FoundTTuhttp://uhttps://uhttp://aurlparseaschemeu://anetlocashellfinderuFiola Toolsu -u Shell Finderu

[+] Saved : ufiola_result/scanner/shell_finderu- Domain List : u- Thread : amapashellfinder_filteru file adom_listu not exists please try again...DuUser-AgentuMozilla/5.0 (Windows NT 10.0; Win64; x64; rv:115.0) Gecko/20100101 Firefox/115.0asitecmsl
Taallow_redirectsatimeoutaheadersaverifyu/.envu/wp-content/uploads/u[CMS | aWordpressu] >> [ufiola_result/scanner/cmschecker/wordpress.txtuJoomla!aJoomlaufiola_result/scanner/cmschecker/joomla.txtusites/defaultaDrupalufiola_result/scanner/cmschecker/drupal.txtuWelcome to nginx!aNginxufiola_result/scanner/cmschecker/nginx.txtaprestashopu [aPrestashopufiola_result/scanner/cmschecker/prestashop.txtavBulletinufiola_result/scanner/cmschecker/vbulletin.txtuindex.php?route=aOpencartufiola_result/scanner/cmschecker/opencart.txtaAPP_ENVaLaravelufiola_result/scanner/cmschecker/laravel.txtaUnknownu Wordpress/Laravel(.env)/Joomla/Drupal/Nginx Checkerufiola_result/scanner/cmscheckerafilter_cmsacms_checkeruDomain To IP + IP Rangeufiola_result/scanner/dom2ipu- Auto IP Range? [y/n]: aexec1aipunot exists please try again...aDom2ipuIP Rangeu- IP List : aRangeathreadlaip_replacew.acheckuIP-RANGEu] -> [ufiola_result/scanner/dom2ip/iprange.txtagethostbyaddrasplitTw.:nlnaip_rangeadomainafdomasocketagethostbynameufiola_result/scanner/dom2ip/ip.txtwyaohmagadamikuagorangeacloseajoinadomipuFeatures Reverse IP:u- Using Private API
- Auto Domain to IP
- Auto Remove Duplicates From List
- Auto Skip Same IP
- Auto Check CMS (Wordpress/Laravel(.env)/Joomla/Drupal/Nginx)u

[+] Saved Reverse : ufiola_result/reverseip/u
[+] Saved Cms : ufiola_result/cms_reverse/u

[1] Reverse IP [Server 1] [4] Reverse IP [Server 4]
[2] Reverse IP [Server 2]
[3] Reverse IP [Server 3]
        uf-boot/reverseip > arevip_s2arevip_s1arevip_s3arevip_s4uNo Option.
uAutomatic Redirect..uReverse IP Server 2u- IP/Domain List : u- Auto Cek CMS [y/n]: u- Thread CMS : luReverse IP Server 1aexec_rev2uReverse IP Server 3aexec3uReverse IP Server 4aexec4aexec_rev1aexec_rev3aexec_rev4acekathread_cmsaof_check_serveruhttps://tools.zone-xsec.com/api/reverse-ip.php?key=pausigans9911&pausi={}aapi1uhttps://s7.reverseipdomain.com/?ip={}&api_key=fx1JN4ID82BBSK399Eaapi2uhttps://rapiddns.io/sameip/{}?full=1#resultaapi3uhttps://www.ipwatson.com/engine/search.php?query={}aapi4actypesawindllakernel32aSetConsoleTitleWuReverse : Total : u | Good IP : u | Bad IP : lTaallow_redirectsatimeoutaverifyaheadersufiola_result/scanner/cms_reverse/wordpress.txtufiola_result/scanner/cms_reverse/joomla.txtufiola_result/scanner/cms_reverse/drupal.txtufiola_result/scanner/cms_reverse/nginx.txtufiola_result/scanner/cms_reverse/prestashop.txtufiola_result/scanner/cms_reverse/vbulletin.txtufiola_result/scanner/cms_reverse/opencart.txtDaallow_redirectsatimeoutaverifyFlFufiola_result/scanner/cms_reverse/laravelenv.txtacmsPatitle_reverseatotalagoodabada__class__uUser-Agentasel_headersaDUPLICATEu] => [aformataurlTaheadersaverifyajsonaresultsareplaceTucpanel.uTuwebdisk.uTuautodiscover.uTucpcontacts.uTucpcalendars.uTumail.uTuwebmail.uTuwww.uuREVERSE u(SERVER 4) - u] - [u Domainufiola_result/reverseip/server4.txtuBAD IPu] => aurlsu(SERVER 2) - ufiola_result/reverseip/server2.txtucpanel.uwebdisk.uautodiscover.ucpcontacts.ucpcalendars.umail.uwebmail.uwww.Taheadersastatuslu(SERVER 1) - ufiola_result/reverseip/server1.txtuutf-8arefereruhttps://rapiddns.io/sameipaacceptutext/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7areafindallu<td>([a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+)</td>u(SERVER 3) - ufiola_result/reverseip/server3.txtaexecutorarev2arev3aex_rev4uGrabber feature:
u- Auto Domain To IP
- Save only good ipu

[1] Grabber by Page (1)[5] Grab Wordpress Site Only
[2] Grabber by Page (2)[6] Grabber by Extension
[3] Grabber by Page (3)[7] Grabber by Date
[4] Grabber by Page (4)[8] Grabber by Extension (2)
uf-tools/grabber > uGrabber Domain By Page 1ufiola_result/grabberu- From Page : u- Until Page : u- Auto Domain To IP? [y/n] : uGrabber Domain By Page 2aexec2uGrabber Domain By Page 3uGrabber Domain By Page 4uGrabber Domain Wordpress Onlyawordpressathemesapluginsu- wordpress/themes/plugins : u: Type not found!. Try Again...
aexec5uGrabber Domain By Extensionsu- Extension : aexec6uGrabber Domain By Dateu- Date (year-month-date): aGrabberSite_dateaexecuGrabber Domain By Extension (2)aGrabberSite_extaGrabberSiteTwtadateafpatpatopipuhttps://www.cubdomain.com/domains-registered-by-date/{}/{}aapiasoapuhtml.parserafind_alladivDaclassucol-md-4TwaaGRABBINGu - u - PAGE : u] [ufiola_result/grabber/grab-{}.txtufiola_result/grabber/ip.txtastarmapagrabberaextuhttps://siterankdata.com/show/extension/{}Luhttps://www.topsitessearch.com/domains/.{}/{}/uhttps://bestwebsiterank.com/domains/.{}/{}/uhttps://sitesmm.com/domains/.{}/{}uhttps://www.topsitessearch.com/domains/.{}/{}apageudomain=([\w.-]+)u[0-9]+(?:\.[0-9]+){3}aupperu Domain | u IP]utoo many values to unpack (expected 2)ufiola_result/grabber/u.txtu Domain]u] => [0 Domain]ah4Daclassum-b-xsafindaulDaclassapaginationaselectTaliluDONE GRABBING. Back to main menu? [y/n] agetNextuhttps://siterankdata.com{}waTahrefaapplyagrabber2atyuhttp://addurl.pw/domain-list-{}uhttps://backlinksources.com/domain-list-{}uhttps://www.thesiterank.com/index.php?p={}Luhttps://portalrankings.com/whole-database/{}uhttps://pageoverview.com/all-the-websites/{}uhttps://site-overview.com/all-websites-by-id/{}uhttp://lievjournal.com/all-websites/{}uhttps://ttoday.net/sitelist-{}.htmluhttps://www.clearwebstats.com/recently-added/{}uhttps://www.thesiterank.com/newly-registered-domain-names/{}/{}aapi5uhttps://themesinfo.com/wordpress-websites/{}aapi6Tuhttps://fiolaapi.my.id/fiola_files/themes_grab.txtTuhttps://fiolaapi.my.id/fiola_files/plugin_grab.txtTatitleahasilufiola_result/grabber/grab1.txtDaclassatrTaimgaaltufiola_result/grabber/grab2.txtTatdufiola_result/grabber/grab3.txtDaclassumuted marginBottom_5ufiola_result/grabber/grab4.txtaAcceptuAccept-Encodingugzip, deflate, bruAccept-Languageuen-US,en;q-0.9u<p class="theme_web_h2">(.*?)</p>ufiola_result/grabber/grab5.txtagrabber1agrabber3agrabber4agrabber5uSUBDOMAIN FINDERufiola_result/subdomainfinderu

[1] Subdomain Finder [Server 1][99] Execute All SubdoFinder
[2] Subdomain Finder [Server 2]
[3] Subdomain Finder [server 3]uf-tools/subdofinder > uSUBDOMAIN FINDER Server 1ulocal variable 'domain' referenced before assignmentuSUBDOMAIN FINDER Server 2uSUBDOMAIN FINDER Server 3aSubdomainu99uExecute All SubdomainFinderaexecalluhttps://jldc.me/anubis/subdomains/{}uhttps://rapiddns.io/subdomain/{}?full=1#resultuhttps://api.xploitsec.com/subdoapi?domain={}TaheadersaverifyatimeoutTu*.uTuimages.uTuftp.uTussl.uTuemail.uu[SERVER 1 - u Subdomains]ufiola_result/subdomainfinder/subdomain.txtuSUBDOMAIN NOT FOUNDareqaexceptionsaTimeoutaConnectionErrorTaheadersatimeoutaverifyu[SERVER 2 - TatimeoutacompileTu<.*?>asubw TaGetuTaSubdomainsuTudomain:uTaCariuTw
w u[SERVER 3 - asubdomainasubdomain2afreeze_supportwjasubdomain3afiltersubuhttps://api.telegram.org/bot6047917785:AAHz9jyCr5kV6XBwZvSz6fEnS0ZFKVn_mYM/sendMessage?chat_id=6285024946&text=uReset Licensiuhttps://fiolaapi.my.id/fiola_login/reset_key.php?key={}aAuthorizationu > u > Reset allu
- Choice : Tuhttps://fiolaapi.my.id/fiola_login/reset_key.php?reset=reset_allasuccessu
[+] Success Reset Licensi!. Press Enter For Exit!.uhttps://fiolaapi.my.id/fiola_login/reset_key.php?reset={}uError:u Cannot empty!.Tlu No options!.aPathTuconfig.txtais_fileTuhttps://fiolaapi.my.id/fiola_files/config.txtuconfig.txtwwushow_update=Trueu
Whats New in u?
Tuhttps://fiolaapi.my.id/fiola_files/whatsnew.txtu
Press Enter For Continue... ushow_update={}Tushow_update=TaTrueaFalseDu199u404w1uLicensi has been expireduLicensi not founduThe license has reached its limitw:;ll0lu{:02x}auuidagetnodel:nnlTulicensi_fiola.txtu
Enter you licensi : adumpsakeyamac_addressDuContent-Typeuapplication/jsonapostTuhttps://fiolaapi.my.id/fiola_login/login_test.phpTadataaheadersu Unknownu
Looks like you licensi has been reached its limit. want to reset key? [y/n] : Tuhttps://fiolaapi.my.id/fiola_login/show_info.phpulicensi_fiola.txtuw+Tw
uu404Tudel licensi_fiola.txtTurm licensi_fiola.txtumultiprocessing.spawnu<lambda>u__nuitka_freeze_support.<locals>.<lambda>aspawna_fixup_main_from_pathaargv:lnnTw=apipe_handleaNoneakwdsamodulesa__parents_main__a__main__aspawn_maina__doc__a__file__a__spec__aoriginahas_locationa__cached__arequestsacoloramaTaForeainitaForeainitapathlibTaPathabs4TaBeautifulSoupaBeautifulSoupuconcurrent.futuresTaThreadPoolExecutoraThreadPoolExecutorumultiprocessing.dummyTafreeze_supportaPoolaPoolathreadingTaLockaLockTagethostbyaddruurllib.parseTaurlparseTu
Error: Module Not Found! Installing Module...Tupip install bs4 licensing requests coloramaTu
Install Complete!. Run Again You Program. Exiting...apackagesaurllib3adisable_warningsTutitle Fiola Tools by @GrazzMeanu2.5aREDaGREENaRESETaBLUEablueaMAGENTAalockDareturnOstra__module__a__qualname__a__init__uDelDup.__init__uDelDup.mainuDelDup.remTuShell.shellfinderuShell.shellfinder_filteruShell.mainuCmsChecker.filter_cmsuCmsChecker.mainuDomtoip.mainuDomtoip.exec1uIPRange.iprangeuIPRange.exec1a__prepare__a__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>uRange.__init__TllastartaintaendareturnuRange.ip_rangeastruRange.checkuRange.ip_replaceuRange.exec1a__orig_bases__uDom2ip.__init__uDom2ip.fdomuDom2ip.checkuDom2ip.ip_replaceuDom2ip.gorangeuDom2ip.domipuDom2ip.ohmagaduDom2ip.exec1uReverse.__init__uReverse.fdomuReverse.revipuReverse.revip_s1uReverse.revip_s2uReverse.revip_s3uReverse.revip_s4uReverse.exec1uReverse.exec3uReverse.exec4aiprevatmpipabekasrevTFuReveseIP.__init__uReveseIP.title_reverseuReveseIP.cmsPacmsuReveseIP.cmsuReveseIP.rev4uReveseIP.revuReveseIP.rev_2uReveseIP.rev2uReveseIP.rev_3uReveseIP.rev3uReveseIP.exec_rev1uReveseIP.exec_rev2uReveseIP.exec_rev3uReveseIP.ex_rev4uReveseIP.exec_rev4uGrabber.mainuGrabber.exec1uGrabber.exec2uGrabber.exec3uGrabber.exec4uGrabber.exec5uGrabber.exec6uGrabberSite_date.__init__uGrabberSite_date.grabberuGrabberSite_date.execuGrabberSite_ext.__init__uGrabberSite_ext.grabberuGrabberSite_ext.grabber2uGrabberSite_ext.execuGrabberSite_ext.exec2ascanatampungTnuGrabberSite.__init__uGrabberSite.grabber1uGrabberSite.grabber2uGrabberSite.grabber3uGrabberSite.grabber4uGrabberSite.grabber5uGrabberSite.exec1uGrabberSite.exec2uGrabberSite.exec3uGrabberSite.exec4uGrabberSite.exec5uSubdofinder.fdomuSubdofinder.mainuSubdofinder.execuSubdofinder.exec2uSubdofinder.execallaresasiteuSubdomain.__init__uSubdomain.subdomainuSubdomain.subdomain2uSubdomain.subdomain3uSubdomain.filtersubuSubdomain.execuSubdomain.exec2uSubdomain.exec3uSubdomain.execallaLicensiChecka__nuitka_freeze_supportufiola.pyTamod_nameu<module __parents_main__>Ta__class__Taresponse_codeamacadata_requestaheadersareqafoundakeyavalaynaread_licensiweTaselfTaselfadateafpatpathatoipTaselfadomainathreadTaselfadup_ipathreadaiprangeTaselfaextafpatpathatoipTaselfafpatpathatoipwtTaselfaipathreadTaselfaipathreadacekathread_cmsaof_check_serverTasysamultiprocessingakwdsaargsaarganameavalueTabannerwxTaselfaipTareverseipabackTareqweTachoosewxTaselfaresapoolTaselfadomaheadersasiteweTacekwiaexTaselfadomainadomaipweTaselfaipadomainwjTaselfadup_ipathreadTaselfapoolTaselfadup_ipathreadacekathread_cmsTaselfwjTaselfwlwpathatoipTaselfwxTaselfwjwxTaselfwtwlwpathatoipTaselfaextwlwpathatoipTaselfapoolweTaselfaexecutoraipsweTaselfwxadomweTaselfasitesaipTaselfahostaheadersadomasitealarweTaselfadomadomaina__class__TaselfaipastartaendwyaipsaresweTaselfafpatpaheadersapageaapiareqadomainaipwxwyweT
aselfafpatpaheadersapageareqadomainadomain2agetDomainaipT
aselfafpatpaheadersapageaapiareqafindajackwjadomaipweTaselfafpatpaheadersapageareqafindadomafinaladomainaipT
aselfaheadersareqwiadomainadomaipagetNextayesweTaselfafpatpaheadersapageaapiareqafindadomadomainaipweTaselfafpatpaheadersapageareqafindadomadomainasalaipTaselfafpatpaheadersapagewxareqailovfiolaadomaipweTaselfaipathreadadup_ipTaopweTaselfadom_listathreadadom_exwjweTaselfadom_listathreadwjweTaselfagrabwgaget_laget_paget_threadatoipatype_wpaget_typeaextadateTaselfaipathreadaiprangeadup_ipTaselfwlwjTaselfasubadup_ipathreadadomainTaselfaahapoolTaselfwlares_dupares_blankwiTareqaindexauseralastapilihareqresetaselectweTaselfaurlagetaheadersareqadomainwxwea__class__TaselfaurladomainwjTaselfaurladomainapoolTaselfaurlagetaheadersareqadomwea__class__T
aselfaurlagetaheadersareqadomwiasakawea__class__TaselfaopwoweTaselfaipathreadacekcmsathread_cmsadup_ipTaselfaipacekcmsathread_cmsathreadadup_ipTareqauaaheadTasiteakeyachat_idaurlTaselfadomapathaheaderswxadomainareqTaselfadomaurlsTwcagetacontentanewTaselfadomaheadersareqaliwea__class__Taselfadomaheadersareqaregxayowxwea__class__Taselfadomaheadersareqaremove_htmlaextrackwea__class__Taselfatotalagoodabad.bs4.builder._html5lib
auser_specified_encodingaselfaexclude_encodingsawarningsawarnTuYou provided a value for exclude_encoding, but the html5lib tree builder doesn't support exclude_encoding.lTastacklevelaDetectsXMLParsedAsHTMLawarn_if_markup_looks_like_xmlamarkupaprepare_markupuHTML5TreeBuilder.prepare_markupasoupaparse_onlyTuYou provided a value for parse_only, but the html5lib tree builder doesn't support parse_only. The entire document will be parsed.lahtml5libaHTMLParseracreate_treebuilderTatreeaunderlying_builderaparseranew_html5libaoverride_encodingaencodingaparseaextra_kwargsaoriginal_encodingatokenizerastreamacharEncodinglanameaTreeBuilderForHtml5libastore_line_numbersTastore_line_numbersu<html><head></head><body>%s</body></html>uSee `TreeBuilder`.abs4TaBeautifulSoupaBeautifulSoupTuuhtml.parsera__init__aresetaElementapublicIdasystemIdaDoctypeafor_name_and_idsaobject_was_parsedapositionutoo many values to unpack (expected 2)asourcelinelasourceposanew_tagakwargsaTextNodeaCommentu[document_fragment]aappendaelementatreebuilder_baseaTreeBuilderagetFragmentareacompileTu^(.*?)(?: PUBLIC "(.*?)"(?: "(.*?)")?| SYSTEM "(.*?)")?$TlaserializeElementuTreeBuilderForHtml5lib.testSerializer.<locals>.serializeElementw
adoctype_reamatchagroupTlalastindexTluTlTlarvu|%s<!DOCTYPE %s "%s" "%s">w u|%s<!DOCTYPE %s>u|%s<!DOCTYPE >u|%s<!-- %s -->aNavigableStringu|%s"%s"anamespaceu%s %saprefixesu|%s<%s>aattrsaitemsaNamespacedAttributeaattributesasortedu|%s%s="%s"aindentlachildrena__iter__acdata_list_attributesagetw*anonwhitespace_reafindallakeysaNodeaTagaparentanodeaextractacontentslanew_stringareplace_witha_most_recent_elementa_last_descendantTFanext_elementachildTaparentamost_recent_elementaAttrListabuildera_replace_cdata_list_attribute_valuesaset_up_substitutionsainsertBeforeaappendChildaindexainsertanext_siblingTFpaprevious_elementafirst_childaprevious_siblingTFtanew_parent_elementuMove all of this tag's children into another tag.anamespacesahtmla__doc__a__file__a__spec__aoriginahas_locationa__cached__aMITa__license__aHTML5TreeBuildera__all__ubs4.builderTaDetectsXMLParsedAsHTMLaPERMISSIVEaHTMLaHTML_5aHTMLTreeBuilderaPERMISSIVEaHTMLaHTML_5aHTMLTreeBuilderubs4.elementTaNamespacedAttributeanonwhitespace_reuhtml5lib.constantsTanamespacesaprefixesTaCommentaDoctypeaNavigableStringaTaguhtml5lib.treebuildersTa_basea_baseweTabaseabasea__prepare__a__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>ubs4.builder._html5liba__module__uUse html5lib to build a tree.

    Note that this TreeBuilder does not support some features common
    to HTML TreeBuilders. Some of these features could theoretically
    be implemented, but at the very least it's quite difficult,
    because html5lib moves the parse tree around as it's being built.

    * This TreeBuilder doesn't use different subclasses of NavigableString
      based on the name of the tag in which the string was found.

    * You can't use a SoupStrainer to parse only part of a document.
    a__qualname__aNAMEafeaturesaTRACKS_LINE_NUMBERSTnnafeeduHTML5TreeBuilder.feeduHTML5TreeBuilder.create_treebuilderatest_fragment_to_documentuHTML5TreeBuilder.test_fragment_to_documenta__orig_bases__TntuTreeBuilderForHtml5lib.__init__adocumentClassuTreeBuilderForHtml5lib.documentClassainsertDoctypeuTreeBuilderForHtml5lib.insertDoctypeaelementClassuTreeBuilderForHtml5lib.elementClassacommentClassuTreeBuilderForHtml5lib.commentClassafragmentClassuTreeBuilderForHtml5lib.fragmentClassuTreeBuilderForHtml5lib.appendChildagetDocumentuTreeBuilderForHtml5lib.getDocumentuTreeBuilderForHtml5lib.getFragmentatestSerializeruTreeBuilderForHtml5lib.testSerializerTOobjectuAttrList.__init__uAttrList.__iter__a__setitem__uAttrList.__setitem__uAttrList.itemsuAttrList.keysa__len__uAttrList.__len__uAttrList.__getitem__a__contains__uAttrList.__contains__uElement.__init__uElement.appendChildagetAttributesuElement.getAttributesasetAttributesuElement.setAttributesapropertyTnainsertTextuElement.insertTextuElement.insertBeforearemoveChilduElement.removeChildareparentChildrenuElement.reparentChildrenacloneNodeuElement.cloneNodeahasContentuElement.hasContentagetNameTupleuElement.getNameTupleanameTupleuTextNode.__init__uTextNode.cloneNodeubs4\builder\_html5lib.pyu<module bs4.builder._html5lib>Ta__class__TaselfanameTaselfaelementTaselfaelementasoupTaselfaelementasoupanamespaceTaselfanamespaceHTMLElementsasoupastore_line_numbersakwargsaBeautifulSoupa__class__TaselfTaselfanameavaluealist_attrTaselfanodeTaselfanodeastring_childachildaold_elementanew_elementamost_recent_elementTaselfataganodeakeyavalueTaselfadataTaselfanamespaceHTMLElementsTaselfanameanamespaceakwargsasourcelineasourceposatagTaselfamarkupaparseraextra_kwargsadocaoriginal_encodingTaselfaBeautifulSoupTaselfanodearefNodeaindexaold_nodeanew_strTaselfatokenanameapublicIdasystemIdadoctypeTaselfadataainsertBeforeatextTaselfamarkupauser_specified_encodingadocument_declared_encodingaexclude_encodingsTaselfanew_parentaelementanew_parent_elementafinal_next_elementanew_parents_last_descendantanew_parents_last_childanew_parents_last_descendant_next_elementato_appendafirst_childalast_childs_last_descendantachildT
aelementaindentwmanameapublicIdasystemIdaattributesavalueachildaBeautifulSoupadoctype_rearvaserializeElementTaBeautifulSoupadoctype_rearvaserializeElementTaselfaattributesaconverted_attributesanameavalueanew_nameTaselfaelementaBeautifulSouparvadoctype_reaserializeElementTaselfafragment.bs4.builder._htmlparseraon_duplicate_attributeaREPLACEaHTMLParsera__init__aalready_closed_empty_elementa_initialize_xml_detectoruConstructor.

        :param on_duplicate_attribute: A strategy for what to do if a
            tag includes the same attribute more than once. Accepted
            values are: REPLACE (replace earlier values with later
            ones, the default), IGNORE (keep the earliest value
            encountered), or a callable. A callable must take three
            arguments: the dictionary of attributes already processed,
            the name of the duplicate attribute, and the most recent value
            encountered.
        aParserRejectedMarkupahandle_starttagDahandle_empty_elementFahandle_endtaguHandle an incoming empty-element tag.

        This is only called when the markup looks like <tag/>.

        :param name: Name of the tag.
        :param attrs: Dictionary of the tag's attributes.
        utoo many values to unpack (expected 2)uaattr_dictaselfaIGNOREu""agetposasoupTasourcelineasourceposais_empty_elementDacheck_already_closedFaappenda_root_taga_root_tag_encountereduHandle an opening tag, e.g. '<tag>'

        :param name: Name of the tag.
        :param attrs: Dictionary of the tag's attributes.
        :param handle_empty_element: True if this tag is known to be
            an empty-element tag (i.e. there is not expected to be any
            closing tag).
        aremoveuHandle a closing tag, e.g. '</tag>'

        :param name: A tag name.
        :param check_already_closed: True if this tag is expected to
           be the closing portion of an empty-element tag,
           e.g. '<tag></tag>'.
        ahandle_datauHandle some textual data that shows up between tags.astartswithTwxalstriplTwXlaoriginal_encodinguwindows-1252adecodeadataTEValueErrorEOverflowErroru�uHandle a numeric character reference by converting it to the
        corresponding Unicode character and treating it as textual
        data.

        :param name: Character number, possibly in hexadecimal.
        aEntitySubstitutionaHTML_ENTITY_TO_CHARACTERagetu&%suHandle a named entity reference by converting it to the
        corresponding Unicode character(s) and treating it as textual
        data.

        :param name: Name of the entity reference.
        aendDataaCommentuHandle an HTML comment.

        :param data: The text of the comment.
        :lnnaDoctypeuHandle a DOCTYPE declaration.

        :param data: The text of the declaration.
        aupperTuCDATA[aCData:lnnaDeclarationuHandle a declaration of unknown type -- probably a CDATA block.

        :param data: The text of the declaration.
        a_document_might_be_xmlaProcessingInstructionuHandle a processing instruction.

        :param data: The text of the instruction.
        Taon_duplicate_attributeakwargsapopaextra_parser_kwargsaHTMLParserTreeBuilderaupdateaconvert_charrefsaparser_argsuConstructor.

        :param parser_args: Positional arguments to pass into
            the BeautifulSoupHTMLParser constructor, once it's
            invoked.
        :param parser_kwargs: Keyword arguments to pass into
            the BeautifulSoupHTMLParser constructor, once it's
            invoked.
        :param kwargs: Keyword arguments for the superclass constructor.
        uRun any preliminary steps necessary to make incoming markup
        acceptable to the parser.

        :param markup: Some markup -- probably a bytestring.
        :param user_specified_encoding: The user asked to try this encoding.
        :param document_declared_encoding: The markup itself claims to be
            in this encoding.
        :param exclude_encodings: The user asked _not_ to try any of
            these encodings.

        :yield: A series of 4-tuples:
         (markup, encoding, declared encoding,
          has undergone character replacement)

         Each 4-tuple represents a strategy for converting the
         document to Unicode and parsing it. Each strategy will be tried
         in turn.
        amarkupauser_specified_encodingadocument_declared_encodingaUnicodeDammitaexclude_encodingsTaknown_definite_encodingsauser_encodingsais_htmlaexclude_encodingsadeclared_html_encodingacontains_replacement_charactersaprepare_markupuHTMLParserTreeBuilder.prepare_markupaBeautifulSoupHTMLParserafeedacloseuRun some incoming markup through some parsing process,
        populating the `BeautifulSoup` object in self.soup.
        uUse the HTMLParser library to parse HTML files that aren't too bad.a__doc__a__file__a__spec__aoriginahas_locationa__cached__aMITa__license__a__all__uhtml.parserTaHTMLParserlasysawarningsubs4.elementTaCDataaCommentaDeclarationaDoctypeaProcessingInstructionubs4.dammitTaEntitySubstitutionaUnicodeDammitubs4.builderTaDetectsXMLParsedAsHTMLaParserRejectedMarkupaHTMLaHTMLTreeBuilderaSTRICTaDetectsXMLParsedAsHTMLaHTMLaHTMLTreeBuilderaSTRICTaHTMLPARSERa__prepare__a__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>ubs4.builder._htmlparsera__module__uA subclass of the Python standard library's HTMLParser class, which
    listens for HTMLParser events and translates them into calls
    to Beautiful Soup's tree construction API.
    a__qualname__aignoreareplaceuBeautifulSoupHTMLParser.__init__aerroruBeautifulSoupHTMLParser.errorahandle_startendtaguBeautifulSoupHTMLParser.handle_startendtagTtuBeautifulSoupHTMLParser.handle_starttaguBeautifulSoupHTMLParser.handle_endtaguBeautifulSoupHTMLParser.handle_dataahandle_charrefuBeautifulSoupHTMLParser.handle_charrefahandle_entityrefuBeautifulSoupHTMLParser.handle_entityrefahandle_commentuBeautifulSoupHTMLParser.handle_commentahandle_decluBeautifulSoupHTMLParser.handle_declaunknown_decluBeautifulSoupHTMLParser.unknown_declahandle_piuBeautifulSoupHTMLParser.handle_pia__orig_bases__uA Beautiful soup `TreeBuilder` that uses the `HTMLParser` parser,
    found in the Python standard library.
    ais_xmlapicklableaNAMEafeaturesaTRACKS_LINE_NUMBERSTnnuHTMLParserTreeBuilder.__init__TnnnuHTMLParserTreeBuilder.feedubs4\builder\_htmlparser.pyu<module bs4.builder._htmlparser>Ta__class__TaselfaargsakwargsTaselfaparser_argsaparser_kwargsakwargsaextra_parser_kwargsaargavaluea__class__TaselfamessageTaselfamarkupaargsakwargsaparserweTaselfanameareal_nameadataaencodingweTaselfadataTaselfanameacheck_already_closedTaselfanameacharacteradataTaselfanameaattrsatagTaselfanameaattrsahandle_empty_elementaattr_dictakeyavalueaon_dupeaattrvalueasourcelineasourceposatagTaselfamarkupauser_specified_encodingadocument_declared_encodingaexclude_encodingsaknown_definite_encodingsauser_encodingsatry_encodingsadammitTaselfadataacls.bs4.builder._lxmlaitemsuInvert a dictionary.utoo many values to unpack (expected 2)u<genexpr>u_invert.<locals>.<genexpr>aLXMLTreeBuilderForXMLainitialize_soupa_register_namespacesaDEFAULT_NSMAPSuLet the BeautifulSoup object know about the standard namespace
        mapping.

        :param soup: A `BeautifulSoup`.
        aselfasoupa_namespacesuLet the BeautifulSoup object know about namespaces encountered
        while parsing the document.

        This might be useful later on when creating CSS selectors.

        This will track (almost) all namespaces, even ones that were
        only in scope for part of the document. If two namespaces have
        the same prefix, only the first one encountered will be
        tracked. Un-prefixed namespaces are not tracked.

        :param mapping: A dictionary mapping namespace prefixes to URIs.
        a_default_parseraetreeaXMLParserTatargetastrip_cdataarecoveraencodinguFind the default parser for the given encoding.

        :param encoding: A string.
        :return: Either a parser object or a class, which
          will be instantiated with default arguments.
        adefault_parseraCallableuInstantiate an appropriate parser for the given encoding.

        :param encoding: A string.
        :return: A parser object such as an `etree.XMLParser`.
        aempty_element_tagsaDEFAULT_NSMAPS_INVERTEDansmapsaactive_namespace_prefixesa__init__lw{:lnnasplitTw}luRun any preliminary steps necessary to make incoming markup
        acceptable to the parser.

        lxml really wants to get a bytestring and convert it to
        Unicode itself. So instead of using UnicodeDammit to convert
        the bytestring to Unicode using different encodings, this
        implementation uses EncodingDetector to iterate over the
        encodings, and tell lxml to try to parse the document as each
        one in turn.

        :param markup: Some markup -- hopefully a bytestring.
        :param user_specified_encoding: The user asked to try this encoding.
        :param document_declared_encoding: The markup itself claims to be
            in this encoding.
        :param exclude_encodings: The user asked _not_ to try any of
            these encodings.

        :yield: A series of 4-tuples:
         (markup, encoding, declared encoding,
          has undergone character replacement)

         Each 4-tuple represents a strategy for converting the
         document to Unicode and parsing it. Each strategy will be tried
         in turn.
        ais_xmlaProcessingInstructionaprocessing_instruction_classaDetectsXMLParsedAsHTMLawarn_if_markup_looks_like_xmlamarkupaXMLProcessingInstructionu﻿adocument_declared_encodingaencodeTautf8autf8auser_specified_encodingaEncodingDetectoraexclude_encodingsTaknown_definite_encodingsauser_encodingsais_htmlaexclude_encodingsaencodingsadetectoraprepare_markupuLXMLTreeBuilderForXML.prepare_markupaBytesIOaStringIOareadaCHUNK_SIZEaparser_foraoriginal_encodingaparserafeedadataacloseaParserErroraParserRejectedMarkupaappendTna_invertluaNamespacedAttributeaxmlnsuhttp://www.w3.org/2000/xmlns/aattrsa_getNsTaganew_attrsa_prefix_for_namespaceahandle_starttagTanamespacesuFind the currently active prefix for the given namespace.aendDataatagStackahandle_endtagapopw ahandle_dataaDoctypeafor_name_and_idsaobject_was_parsedaCommentuHandle comments as Comment objects.u<?xml version="1.0" encoding="utf-8"?>
%suSee `TreeBuilder`.aHTMLParseru<html><body>%s</body></html>a__doc__a__file__a__spec__aoriginahas_locationa__cached__aMITa__license__aLXMLTreeBuildera__all__ucollections.abcTaCallableweacollectionsalxmlTaetreeubs4.elementTaCommentaDoctypeaNamespacedAttributeaProcessingInstructionaXMLProcessingInstructionubs4.builderTaDetectsXMLParsedAsHTMLaFASTaHTMLaHTMLTreeBuilderaPERMISSIVEaParserRejectedMarkupaTreeBuilderaXMLaFASTaHTMLaHTMLTreeBuilderaPERMISSIVEaTreeBuilderaXMLubs4.dammitTaEncodingDetectoraLXMLa__prepare__a__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>ubs4.builder._lxmla__module__a__qualname__aDEFAULT_PARSER_CLASSulxml-xmlaNAMEaxmlaALTERNATE_NAMESafeaturesladictTuhttp://www.w3.org/XML/1998/namespaceTaxmlDaxmluhttp://www.w3.org/XML/1998/namespaceuLXMLTreeBuilderForXML.initialize_soupuLXMLTreeBuilderForXML._register_namespacesuLXMLTreeBuilderForXML.default_parseruLXMLTreeBuilderForXML.parser_forTnnuLXMLTreeBuilderForXML.__init__uLXMLTreeBuilderForXML._getNsTagTnnnuLXMLTreeBuilderForXML.feeduLXMLTreeBuilderForXML.closeastartuLXMLTreeBuilderForXML.startuLXMLTreeBuilderForXML._prefix_for_namespaceaenduLXMLTreeBuilderForXML.endapiuLXMLTreeBuilderForXML.piuLXMLTreeBuilderForXML.dataadoctypeuLXMLTreeBuilderForXML.doctypeacommentuLXMLTreeBuilderForXML.commentatest_fragment_to_documentuLXMLTreeBuilderForXML.test_fragment_to_documenta__orig_bases__ulxml-htmluLXMLTreeBuilder.default_parseruLXMLTreeBuilder.feeduLXMLTreeBuilder.test_fragment_to_documentubs4\builder\_lxml.pyTa.0wkwvu<module bs4.builder._lxml>Ta__class__Taselfaparseraempty_element_tagsakwargsa__class__TaselfatagTwdTaselfanamespaceainverted_nsmapTaselfamappingakeyavalueTaselfTaselfacontentTaselfaencodingTaselfanameapubidasystemadoctypeTaselfanameacompleted_taganamespaceansprefixainverted_nsmapaout_of_scope_nsmapTaselfamarkupadataweTaselfamarkupaencodingweTaselfasoupa__class__TaselfaencodingaparserTaselfatargetadataT
aselfamarkupauser_specified_encodingaexclude_encodingsadocument_declared_encodingais_htmlaknown_definite_encodingsauser_encodingsadetectoraencodingTaselfanameaattrsansmapansprefixacurrent_mappingaprefixanamespaceaattributeanew_attrsaattravalueTaselfafragment.bs4.builder7adefaultdictTOlistabuilders_for_featureabuildersafeaturesaselfainsertlatreebuilder_classuRegister a treebuilder based on its advertised features.

        :param treebuilder_class: A subclass of Treebuilder. its .features
           attribute should list its features.
        apopagetacandidatesacandidate_setaintersectionuLook up a TreeBuilder subclass with the desired features.

        :param features: A list of features to look for. If none are
            provided, the most recently registered TreeBuilder subclass
            will be used.
        :return: A TreeBuilder subclass, or None if there's no
            registered subclass with all the requested features.
        asoupaUSE_DEFAULTaDEFAULT_CDATA_LIST_ATTRIBUTESacdata_list_attributesaDEFAULT_PRESERVE_WHITESPACE_TAGSapreserve_whitespace_tagsaTRACKS_LINE_NUMBERSastore_line_numbersaDEFAULT_STRING_CONTAINERSastring_containersuConstructor.

        :param multi_valued_attributes: If this is set to None, the
         TreeBuilder will not turn any values for attributes like
         'class' into lists. Setting this to a dictionary will
         customize this behavior; look at DEFAULT_CDATA_LIST_ATTRIBUTES
         for an example.

         Internally, these are called "CDATA list attributes", but that
         probably doesn't make sense to an end-user, so the argument name
         is `multi_valued_attributes`.

        :param preserve_whitespace_tags: A list of tags to treat
         the way <pre> tags are treated in HTML. Tags in this list
         are immune from pretty-printing; their contents will always be
         output as-is.

        :param string_containers: A dictionary mapping tag names to
        the classes that should be instantiated to contain the textual
        contents of those tags. The default is to use NavigableString
        for every tag, no matter what the name. You can override the
        default by changing DEFAULT_STRING_CONTAINERS.

        :param store_line_numbers: If the parser keeps track of the
         line numbers and positions of the original markup, that
         information will, by default, be stored in each corresponding
         `Tag` object. You can turn this off by passing
         store_line_numbers=False. If the parser you're using doesn't
         keep track of this information, then setting store_line_numbers=True
         will do nothing.
        uThe BeautifulSoup object has been initialized and is now
        being associated with the TreeBuilder.

        :param soup: A BeautifulSoup object.
        aempty_element_tagsuMight a tag with this name be an empty-element tag?

        The final markup may or may not actually present this tag as
        self-closing.

        For instance: an HTMLBuilder does not consider a <p> tag to be
        an empty-element tag (it's not in
        HTMLBuilder.empty_element_tags). This means an empty <p> tag
        will be presented as "<p></p>", not "<p/>" or "<p>".

        The default implementation has no opinion about which tags are
        empty-element tags, so a tag will be presented as an
        empty-element tag if and only if it has no children.
        "<foo></foo>" will become "<foo/>", and "<foo>bar</foo>" will
        be left alone.

        :param tag_name: The name of a markup tag.
        uRun some incoming markup through some parsing process,
        populating the `BeautifulSoup` object in self.soup.

        This method is not implemented in TreeBuilder; it must be
        implemented in subclasses.

        :return: None.
        uRun any preliminary steps necessary to make incoming markup
        acceptable to the parser.

        :param markup: Some markup -- probably a bytestring.
        :param user_specified_encoding: The user asked to try this encoding.
        :param document_declared_encoding: The markup itself claims to be
            in this encoding. NOTE: This argument is not used by the
            calling code and can probably be removed.
        :param exclude_encodings: The user asked _not_ to try any of
            these encodings.

        :yield: A series of 4-tuples:
         (markup, encoding, declared encoding,
          has undergone character replacement)

         Each 4-tuple represents a strategy for converting the
         document to Unicode and parsing it. Each strategy will be tried
         in turn.

         By default, the only strategy is to parse the markup
         as-is. See `LXMLTreeBuilderForXML` and
         `HTMLParserTreeBuilder` for implementations that take into
         account the quirks of particular parsers.
        amarkupaprepare_markupuTreeBuilder.prepare_markupuWrap an HTML fragment to make it look like a document.

        Different parsers do this differently. For instance, lxml
        introduces an empty <head> tag, and html5lib
        doesn't. Abstracting this away lets us write simple tests
        which run HTML fragments through the parser and compare the
        results against other HTML fragments.

        This method should not be used outside of tests.

        :param fragment: A string -- fragment of HTML.
        :return: A string -- a full HTML document.
        w*alowerakeysaattrsanonwhitespace_reafindalluWhen an attribute value is associated with a tag that can
        have multiple values for that attribute, convert the string
        value to a list of strings.

        Basically, replaces class="foo bar" with class=["foo", "bar"]

        NOTE: This method modifies its input in place.

        :param tag_name: The name of a tag.
        :param attrs: A dictionary containing the tag's attributes.
           Any appropriate attribute values will be modified in place.
        aitemsahandle_starttagutoo many values to unpack (expected 2)lu<genexpr>uSAXTreeBuilder.startElement.<locals>.<genexpr>ahandle_endtagastartElementaendElementahandle_dataanameametaTuhttp-equivTacontentTacharsetaCharsetMetaAttributeValueacharsetucontent-typeaContentMetaAttributeValueacontentuReplace the declared encoding in a <meta> tag with a placeholder,
        to be substituted when the tag is output to a string.

        An HTML document may come in to Beautiful Soup as one
        encoding, but exit in a different encoding, and the <meta> tag
        needs to be changed to reflect this.

        :param tag: A `Tag`
        :return: Whether or not a substitution was performed.
        aXML_PREFIX_BaLOOKS_LIKE_HTML_BaXML_PREFIXaLOOKS_LIKE_HTMLastartswithasearch:nlnaclsa_warnuPerform a check on some markup to see if it looks like XML
        that's not XHTML. If so, issue a warning.

        This is much less reliable than doing the check while parsing,
        but some of the tree builders can't do that.

        :return: True if the markup looks like non-XHTML XML, False
        otherwise.
        awarningsawarnaXMLParsedAsHTMLWarningaMESSAGEuIssue a warning about XML being parsed as HTML.a_first_processing_instructiona_root_taguCall this method before parsing a document.uCall this method when encountering an XML declaration, or a
        "processing instruction" that might be an XML declaration.
        ahtmlTuxml uCall this when you encounter the document's root tag.

        This is where we actually check whether an XML document is
        being incorrectly parsed as HTML, and issue the warning.
        asysamodulesubs4.buildera__all__aTreeBuilderathis_moduleaappendabuilder_registryaregisteruCopy TreeBuilders from the given module into this module.u%s: %sa__name__aParserRejectedMarkupa__init__uExplain why the parser rejected the given markup, either
        with a textual explanation or another exception.
        a__doc__a__file__apathadirnameajoinaenvironTaNUITKA_PACKAGE_bs4u\not_existingabuilderTaNUITKA_PACKAGE_bs4_builderu\not_existinga__path__a__spec__aoriginahas_locationasubmodule_search_locationsa__cached__aMITa__license__acollectionsTadefaultdictaitertoolsareubs4.elementTaCharsetMetaAttributeValueaContentMetaAttributeValueaRubyParenthesisStringaRubyTextStringaStylesheetaScriptaTemplateStringanonwhitespace_reaRubyParenthesisStringaRubyTextStringaStylesheetaScriptaTemplateStringLaHTMLTreeBuilderaSAXTreeBuilderaTreeBuilderaTreeBuilderRegistryafastaFASTapermissiveaPERMISSIVEastrictaSTRICTaxmlaXMLaHTMLahtml5aHTML_5aUserWarninga__prepare__a__getitem__u%s.__prepare__() must return a mapping, not %su<metaclass>a__module__uThe warning issued when an HTML parser is used to parse
    XML that is not XHTML.
    a__qualname__uIt looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features="xml"` into the BeautifulSoup constructor.a__orig_bases__TOobjectaTreeBuilderRegistryuA way of looking up TreeBuilder subclasses by their name or by desired
    features.
    uTreeBuilderRegistry.__init__uTreeBuilderRegistry.registeralookupuTreeBuilderRegistry.lookupuTurn a textual document into a Beautiful Soup object tree.u[Unknown tree builder]aNAMEaALTERNATE_NAMESais_xmlapicklablealistasetaobjectuTreeBuilder.__init__ainitialize_soupuTreeBuilder.initialize_soupuDo any work necessary to reset the underlying parser
        for a new document.

        By default, this does nothing.
        aresetuTreeBuilder.resetacan_be_empty_elementuTreeBuilder.can_be_empty_elementafeeduTreeBuilder.feedTnnnatest_fragment_to_documentuTreeBuilder.test_fragment_to_documentuSet up any substitutions that will need to be performed on
        a `Tag` when it's output as a string.

        By default, this does nothing. See `HTMLTreeBuilder` for a
        case where this is used.

        :param tag: A `Tag`
        :return: Whether or not a substitution was performed.
        aset_up_substitutionsuTreeBuilder.set_up_substitutionsa_replace_cdata_list_attribute_valuesuTreeBuilder._replace_cdata_list_attribute_valuesaSAXTreeBuilderuA Beautiful Soup treebuilder that listens for SAX events.

    This is not currently used for anything, but it demonstrates
    how a simple TreeBuilder would work.
    uSAXTreeBuilder.feedacloseuSAXTreeBuilder.closeuSAXTreeBuilder.startElementuSAXTreeBuilder.endElementastartElementNSuSAXTreeBuilder.startElementNSaendElementNSuSAXTreeBuilder.endElementNSastartPrefixMappinguSAXTreeBuilder.startPrefixMappingaendPrefixMappinguSAXTreeBuilder.endPrefixMappingacharactersuSAXTreeBuilder.charactersastartDocumentuSAXTreeBuilder.startDocumentaendDocumentuSAXTreeBuilder.endDocumentaHTMLTreeBuilderuThis TreeBuilder knows facts about HTML.

    Such as which tags are empty-element tags.
    LaareaabaseabracolaembedahraimgainputakeygenalinkamenuitemametaaparamasourceatrackawbrabasefontabgsoundacommandaframeaimageaisindexanextidaspacerSasourceaimgametaainputaparamakeygenacolahraisindexaspaceracommandaframeaembedalinkamenuitemaimageabasefontawbratrackaareaabranextidabgsoundabaseL#aaddressaarticleaasideablockquoteacanvasaddadivadladtafieldsetafigcaptionafigureafooteraformah1ah2ah3ah4ah5ah6aheaderahraliamainanavanoscriptaolaoutputwpapreasectionatableatfootaulavideoS#amainah2wpanavaformah5anoscriptaulaliadivadtahratfootablockquoteah3avideoafooteraasideafigureafigcaptionah4ah6aheaderafieldsetadlah1apreasectionaarticleaoutputaaddressaddatableacanvasaolablock_elementsartarpastyleascriptatemplateDw*waalinkatdathaformaobjectaareaaiconaiframeaoutputLaclassaaccesskeyadropzoneLarelarevLarelarevLaheadersLaheadersLuaccept-charsetLaarchiveLarelLasizesLasandboxLaforapreatextareaSapreatextareauHTMLTreeBuilder.set_up_substitutionsaDetectsXMLParsedAsHTMLuA mixin class for any class (a TreeBuilder, or some class used by a
    TreeBuilder) that's in a position to detect whether an XML
    document is being incorrectly parsed as HTML, and issue an
    appropriate warning.

    This requires being able to observe an incoming processing
    instruction that might be an XML declaration, and also able to
    observe tags as they're opened. If you can't do that for a given
    TreeBuilder, there's a less reliable implementation based on
    examining the raw markup.
    acompileu<[^ +]htmlwIc<[^ +]htmlu<?xmlc<?xmlaclassmethodawarn_if_markup_looks_like_xmluDetectsXMLParsedAsHTML.warn_if_markup_looks_like_xmluDetectsXMLParsedAsHTML._warna_initialize_xml_detectoruDetectsXMLParsedAsHTML._initialize_xml_detectora_document_might_be_xmluDetectsXMLParsedAsHTML._document_might_be_xmla_root_tag_encountereduDetectsXMLParsedAsHTML._root_tag_encounteredaregister_treebuilders_fromTEExceptionuAn Exception to be raised when the underlying parser simply
    refuses to parse the given markup.
    uParserRejectedMarkup.__init__uTa_htmlparsera_htmlparserTa_html5liba_html5libTa_lxmla_lxmlubs4\builder\__init__.pyTa.0akeyavalueu<module bs4.builder>Ta__class__TaselfTaselfamessage_or_exceptionwea__class__Taselfamulti_valued_attributesapreserve_whitespace_tagsastore_line_numbersastring_containersTaselfaprocessing_instructionTaselfatag_nameaattrsauniversalatag_specificaattravalueavaluesTaselfanameTaclsTaselfatag_nameTaselfacontentTaselfansTupleanodeNameTaselfaprefixTaselfamarkupTaselfasoupTaselfafeaturesacandidatesacandidate_setafeatureawe_have_the_featureacandidateTaselfamarkupauser_specified_encodingadocument_declared_encodingaexclude_encodingsTaselfatreebuilder_classafeatureTamoduleathis_moduleanameaobjTaselfatagTaselfatagahttp_equivacontentacharsetameta_encodingTaselfanameaattrsTaselfansTupleanodeNameaattrsTaselfaprefixanodeValueTaselfafragmentTaclsamarkupaprefixalooks_like_html.bs4AHaconvertEntitiesawarningsawarnTuBS4 does not respect the convertEntities argument to the BeautifulSoup constructor. Entities are always converted to Unicode characters.amarkupMassageakwargsTuBS4 does not respect the markupMassage argument to the BeautifulSoup constructor. The tree builder is responsible for any necessary markup massage.asmartQuotesToTuBS4 does not respect the smartQuotesTo argument to the BeautifulSoup constructor. Smart quotes are always converted to Unicode characters.aselfClosingTagsTuBS4 does not respect the selfClosingTags argument to the BeautifulSoup constructor. The tree builder is responsible for understanding self-closing tags.aisHTMLTuBS4 does not respect the isHTML argument to the BeautifulSoup constructor. Suggest you use features='lxml' for HTML and features='lxml-xml' for XML.adeprecated_argumentuBeautifulSoup.__init__.<locals>.deprecated_argumentTaparseOnlyTheseaparse_onlyTafromEncodingafrom_encodingTuYou provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.aelement_classesafeaturesaDEFAULT_BUILDER_FEATURESabuilder_registryalookupaFeatureNotFounduCouldn't find a tree builder with the features you requested: %s. Do you need to install a parser library?w,abuilderabuilder_classaNAMEaALTERNATE_NAMESais_xmlaXMLaHTMLasysa_getframeTlaf_globalsaf_linenolagetTa__file__aloweraendswithTTu.pycu.pyo:nlnafilenamealine_numberaparseramarkup_typeaNO_PARSER_SPECIFIED_WARNINGaGuessedAtParserWarningDastacklevellTuKeyword arguments to the BeautifulSoup constructor will be ignored. These would normally be passed into the TreeBuilder constructor, but a TreeBuilder instance was passed in as `builder`.aknown_xmla_namespacesaparse_onlyareadd<w<a_markup_is_urla_markup_resembles_filenameaprepare_markupamarkupTaexclude_encodingsutoo many values to unpack (expected 4)aselfaoriginal_encodingadeclared_html_encodingacontains_replacement_charactersaresetainitialize_soupa_feedaParserRejectedMarkuparejectionsaappenduThe markup you provided was rejected by the parser. Trying a different parser or a different encoding may help.

Original exception(s) from parser:
 u
 asoupuConstructor.

        :param markup: A string or a file-like object representing
         markup to be parsed.

        :param features: Desirable features of the parser to be
         used. This may be the name of a specific parser ("lxml",
         "lxml-xml", "html.parser", or "html5lib") or it may be the
         type of markup to be used ("html", "html5", "xml"). It's
         recommended that you name a specific parser, so that
         Beautiful Soup gives you the same results across platforms
         and virtual environments.

        :param builder: A TreeBuilder subclass to instantiate (or
         instance to use) instead of looking one up based on
         `features`. You only need to use this if you've implemented a
         custom TreeBuilder.

        :param parse_only: A SoupStrainer. Only parts of the document
         matching the SoupStrainer will be considered. This is useful
         when parsing part of a document that would otherwise be too
         large to fit into memory.

        :param from_encoding: A string indicating the encoding of the
         document to be parsed. Pass this in if Beautiful Soup is
         guessing wrongly about the document's encoding.

        :param exclude_encodings: A list of strings indicating
         encodings known to be wrong. Pass this in if you don't know
         the document's encoding but you know Beautiful Soup's guess is
         wrong.

        :param element_classes: A dictionary mapping BeautifulSoup
         classes like Tag and NavigableString, to other classes you'd
         like to be instantiated instead as the parse tree is
         built. This is useful for subclassing Tag or NavigableString
         to modify default behavior.

        :param kwargs: For backwards compatibility purposes, the
         constructor accepts certain keyword arguments used in
         Beautiful Soup 3. None of these arguments do anything in
         Beautiful Soup 4; they will result in a warning and then be
         ignored.

         Apart from this, any keyword arguments passed into the
         BeautifulSoup constructor are propagated to the TreeBuilder
         constructor. This makes it possible to configure a
         TreeBuilder by passing in arguments, not just by saying which
         one to use.
        uThe "%s" argument to the BeautifulSoup constructor has been renamed to "%s."aDeprecationWarningDastacklevellapopuuCreate a new BeautifulSoup object with the same TreeBuilder,
        but not associated with any markup.

        This is the first step of the deepcopy process.
        apicklablewdacontentsadecodea_most_recent_elementaHTMLParserTreeBuilderTuutf-8areplaceuEnsure `markup` is bytes so it's safe to send into warnings.warn.

        TODO: warnings.warn had this problem back in 2010 but it might not
        anymore.
        d Tchttp:chttps:w Tuhttp:uhttps:uThe input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.aMarkupResemblesLocatorWarninguError-handling method to raise a warning if incoming markup looks
        like a URL.

        :param markup: A string.
        :return: Whether or not the markup resembles a URL
            closely enough to justify a warning.
        astartswithu<genexpr>uBeautifulSoup._markup_is_url.<locals>.<genexpr>u/\Lu.htmlu.htmu.xmlu.xhtmlu.txtautf8aencodeTautf8uThe input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.uError-handling method to raise a warning if incoming markup
        resembles a filename.

        :param markup: A bytestring or string.
        :return: Whether or not the markup resembles a filename
            closely enough to justify a warning.
        uBeautifulSoup._markup_resembles_filename.<locals>.<genexpr>afeedaendDataacurrentTaganameaROOT_TAG_NAMEapopTaguInternal method that parses previously set markup, creating a large
        number of Tag and NavigableString objects.
        aTaga__init__ahiddenacurrent_dataatagStackaCounteraopen_tag_counterapreserve_whitespace_tag_stackastring_container_stackapushTaguReset this object to a state as though it had never parsed any
        markup.
        TasourcelineasourceposuCreate a new Tag associated with this BeautifulSoup object.

        :param name: The name of the new Tag.
        :param namespace: The URI of the new Tag's XML namespace, if any.
        :param prefix: The prefix for the new Tag's XML namespace, if any.
        :param attrs: A dictionary of this Tag's attribute values; can
            be used instead of `kwattrs` for attributes like 'class'
            that are reserved words in Python.
        :param sourceline: The line number where this tag was
            (purportedly) found in its source document.
        :param sourcepos: The character position within `sourceline` where this
            tag was (purportedly) found.
        :param kwattrs: Keyword arguments for the new Tag's attribute values.

        aNavigableStringastring_containerslastring_containeruCreate a new NavigableString associated with this BeautifulSoup
        object.
        uBeautifulSoup objects don't support insert_before().uThis method is part of the PageElement API, but `BeautifulSoup` doesn't implement
        it because there is nothing before or after it in the parse tree.
        uBeautifulSoup objects don't support insert_after().uInternal method called by _popToTag when a tag is closed.atagapreserve_whitespace_tagsuInternal method called by handle_starttag when a tag is opened.aASCII_SPACESw
atextasearchaobject_was_parseduMethod called by the TreeBuilder when the end of a data segment
        occurs.
        anext_elementanext_siblingaprevious_siblingaprevious_elementwoasetupa_linkage_fixeruMethod called by the TreeBuilder to integrate an object into the parse tree.laparenta_last_descendantTFatargetuMake sure linkage of this fragment is sound.aprefixamost_recently_poppeduPops the tag stack up to and including the most recent
        instance of the given tag.

        If there are no open tags with the given name, nothing will be
        popped.

        :param name: Pop up to the most recent tag with this name.
        :param nsprefix: The namespace prefix that goes with `name`.
        :param inclusivePop: It this is false, pops the tag stack up
          to but *not* including the most recent instqance of the
          given tag.

        asearch_tagaattrsTasourcelineasourceposanamespacesuCalled by the tree builder when a new tag is encountered.

        :param name: Name of the tag.
        :param nsprefix: Namespace prefix for the tag.
        :param attrs: A dictionary of attribute values.
        :param sourceline: The line number where this tag was found in its
            source document.
        :param sourcepos: The character position within `sourceline` where this
            tag was found.
        :param namespaces: A dictionary of all namespace prefix mappings
            currently in scope in the document.

        If this method returns None, the tag was rejected by an active
        SoupStrainer. You should proceed as if the tag had not occurred
        in the document. For instance, if this was a self-closing tag,
        don't call handle_endtag.
        a_popToTaguCalled by the tree builder when an ending tag is encountered.

        :param name: Name of the tag.
        :param nsprefix: Namespace prefix for the tag.
        uCalled by the tree builder when a chunk of textual data is encountered.aPYTHON_SPECIFIC_ENCODINGSu encoding="%s"u<?xml version="1.0"%s?>
aBeautifulSoupaeventual_encodinguReturns a string or Unicode representation of the parse tree
            as an HTML or XML document.

        :param pretty_print: If this is True, indentation will be used to
            make the document more readable.
        :param eventual_encoding: The encoding of the final document.
            If this is None, the document will be a Unicode string.
        axmluThe BeautifulStoneSoup class is deprecated. Instead of using it, pass features="xml" into the BeautifulSoup constructor.aBeautifulStoneSoupuBeautiful Soup Elixir and Tonic - "The Screen-Scraper's Friend".

http://www.crummy.com/software/BeautifulSoup/

Beautiful Soup uses a pluggable XML or HTML parser to parse a
(possibly invalid) document into a tree representation. Beautiful Soup
provides methods and Pythonic idioms that make it easy to navigate,
search, and modify the parse tree.

Beautiful Soup works with Python 3.6 and up. It works better if lxml
and/or html5lib is installed.

For more than you ever wanted to know about Beautiful Soup, see the
documentation: http://www.crummy.com/software/BeautifulSoup/bs4/doc/
a__doc__a__file__apathadirnameaenvironTaNUITKA_PACKAGE_bs4u\not_existinga__path__a__spec__aoriginahas_locationasubmodule_search_locationsa__cached__uLeonard Richardson (leonardr@segfault.org)a__author__u4.12.2a__version__uCopyright (c) 2004-2023 Leonard Richardsona__copyright__aMITa__license__a__all__acollectionsTaCounteraosareatracebackTabuilder_registryaParserRejectedMarkupaXMLParsedAsHTMLWarningaHTMLParserTreeBuilderaXMLParsedAsHTMLWarningadammitTaUnicodeDammitaUnicodeDammitaelementTaCDataaCommentaCSSaDEFAULT_OUTPUT_ENCODINGaDeclarationaDoctypeaNavigableStringaPageElementaProcessingInstructionaPYTHON_SPECIFIC_ENCODINGSaResultSetaScriptaStylesheetaSoupStraineraTagaTemplateStringaCDataaCommentaCSSaDEFAULT_OUTPUT_ENCODINGaDeclarationaDoctypeaPageElementaProcessingInstructionaResultSetaScriptaStylesheetaSoupStraineraTemplateStringaUserWarninga__prepare__a__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>abs4a__module__uThe warning issued when BeautifulSoup has to guess what parser to
    use -- probably because no parser was specified in the constructor.
    a__qualname__a__orig_bases__uThe warning issued when BeautifulSoup is given 'markup' that
    actually looks like a resource locator -- a URL or a path to a file
    on disk.
    uA data structure representing a parsed HTML or XML document.

    Most of the methods you'll call on a BeautifulSoup object are inherited from
    PageElement or Tag.

    Internally, this class defines the basic interface called by the
    tree builders when converting an HTML/XML document into a data
    structure. The interface abstracts away the differences between
    parsers. To write a new tree builder, you'll need to understand
    these methods as a whole.

    These methods will be called by the BeautifulSoup constructor:
      * reset()
      * feed(markup)

    The tree builder may call these methods from its feed() implementation:
      * handle_starttag(name, attrs) # See note about return value
      * handle_endtag(name)
      * handle_data(data) # Appends to the current data node
      * endData(containerClass) # Ends the current data node

    No matter how complicated the underlying parser is, you should be
    able to build a tree using 'start tag' events, 'end tag' events,
    'data' events, and "done with data" events.

    If you encounter an empty-element tag (aka a self-closing tag,
    like HTML's <br> tag), call handle_starttag and then
    handle_endtag.
    u[document]ahtmlafastu

uNo parser was explicitly specified, so I'm using the best available %(markup_type)s parser for this system ("%(parser)s"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.

The code that caused this warning is on line %(line_number)s of the file %(filename)s. To get rid of this warning, pass the additional argument 'features="%(parser)s"' to the BeautifulSoup constructor.
TunnnnnnuBeautifulSoup.__init__a_cloneuBeautifulSoup._clonea__getstate__uBeautifulSoup.__getstate__a__setstate__uBeautifulSoup.__setstate__aclassmethoda_decode_markupuBeautifulSoup._decode_markupuBeautifulSoup._markup_is_urluBeautifulSoup._markup_resembles_filenameuBeautifulSoup._feeduBeautifulSoup.resetanew_taguBeautifulSoup.new_tagTnuBeautifulSoup.string_containeranew_stringuBeautifulSoup.new_stringainsert_beforeuBeautifulSoup.insert_beforeainsert_afteruBeautifulSoup.insert_afteruBeautifulSoup.popTaguBeautifulSoup.pushTaguBeautifulSoup.endDataTnnuBeautifulSoup.object_was_parseduBeautifulSoup._linkage_fixerTntuBeautifulSoup._popToTagTnnnahandle_starttaguBeautifulSoup.handle_starttagahandle_endtaguBeautifulSoup.handle_endtagahandle_datauBeautifulSoup.handle_dataaminimaluBeautifulSoup.decodea_sa_soupuDeprecated interface to an XML parser.uBeautifulStoneSoup.__init__TEExceptionaStopParsinguException raised by a TreeBuilder if it's unable to continue parsing.TEValueErroruException raised by the BeautifulSoup constructor if no parser with the
    requested features is found.
    ubs4\__init__.pyTa.0aextalowerTa.0aprefixamarkupTa.0wxamarkupu<module bs4>Ta__class__TaselfwdTaselfaargsakwargsa__class__Taselfamarkupafeaturesabuilderaparse_onlyafrom_encodingaexclude_encodingsaelement_classesakwargsadeprecated_argumentaoriginal_builderaoriginal_featuresabuilder_classamarkup_typeacalleraglobalsaline_numberafilenameafnlavaluesarejectionsasuccessweaother_exceptionsTaselfastateTaselfacloneTaclsamarkupadecodedTaselfTaselfaelafirstachildadescendantaprev_elatargetTaclsamarkupaspaceacant_start_withTaclsamarkupapath_charactersaextensionsafilelikealowerTaselfanameansprefixainclusivePopamost_recently_poppedastack_sizewiwtTaselfapretty_printaeventual_encodingaformatteraiteratoraencoding_partaprefixaindent_levela__class__Taold_nameanew_nameakwargsTakwargsTaselfacontainerClassacurrent_dataastrippablewiwoTaselfadataTaselfanameansprefixTaselfanameanamespaceansprefixaattrsasourcelineasourceposanamespacesatagTaselfaargsTaselfwsasubclassacontainerTaselfanameanamespaceansprefixaattrsasourcelineasourceposakwattrsTaselfwoaparentamost_recent_elementaprevious_elementanext_elementaprevious_siblinganext_siblingafixTaselfatagTaselfabase_classacontainer.bs4.cssO OuCannot execute CSS selectors because the soupsieve package is not installed.aapiataguConstructor.

        You don't need to instantiate this class yourself; instead,
        access the .css attribute on the BeautifulSoup object, or on
        the Tag you want to use as the starting point for your CSS
        selector.

        :param tag: All CSS selectors will use this as their starting
        point.

        :param api: A plug-in replacement for the soupsieve module,
        designed mainly for use in tests.
        asoupsieveuCannot escape CSS identifiers because the soupsieve package is not installed.aescapeuEscape a CSS identifier.

        This is a simple wrapper around soupselect.escape(). See the
        documentation for that function for more information.
        aSoupSievea_namespacesuNormalize a dictionary of namespaces.ubs4.elementTaResultSetlaResultSetuNormalize a list of results to a Resultset.

        A ResultSet is more consistent with the rest of Beautiful
        Soup's API, and ResultSet.__getattr__ has a helpful error
        message if you try to treat a list of results as a single
        result (a common mistake).
        acompilea_nsuPre-compile a selector and return the compiled object.

        :param selector: A CSS selector.

        :param namespaces: A dictionary mapping namespace prefixes
           used in the CSS selector to namespace URIs. By default,
           Beautiful Soup will use the prefixes it encountered while
           parsing the document.

        :param flags: Flags to be passed into Soup Sieve's
            soupsieve.compile() method.

        :param kwargs: Keyword arguments to be passed into SoupSieve's
           soupsieve.compile() method.

        :return: A precompiled selector object.
        :rtype: soupsieve.SoupSieve
        aselect_oneuPerform a CSS selection operation on the current Tag and return the
        first result.

        This uses the Soup Sieve library. For more information, see
        that library's documentation for the soupsieve.select_one()
        method.

        :param selector: A CSS selector.

        :param namespaces: A dictionary mapping namespace prefixes
           used in the CSS selector to namespace URIs. By default,
           Beautiful Soup will use the prefixes it encountered while
           parsing the document.

        :param flags: Flags to be passed into Soup Sieve's
            soupsieve.select_one() method.

        :param kwargs: Keyword arguments to be passed into SoupSieve's
           soupsieve.select_one() method.

        :return: A Tag, or None if the selector has no match.
        :rtype: bs4.element.Tag

        a_rsaselectuPerform a CSS selection operation on the current Tag.

        This uses the Soup Sieve library. For more information, see
        that library's documentation for the soupsieve.select()
        method.

        :param selector: A string containing a CSS selector.

        :param namespaces: A dictionary mapping namespace prefixes
            used in the CSS selector to namespace URIs. By default,
            Beautiful Soup will pass in the prefixes it encountered while
            parsing the document.

        :param limit: After finding this number of results, stop looking.

        :param flags: Flags to be passed into Soup Sieve's
            soupsieve.select() method.

        :param kwargs: Keyword arguments to be passed into SoupSieve's
            soupsieve.select() method.

        :return: A ResultSet of Tag objects.
        :rtype: bs4.element.ResultSet

        aiselectuPerform a CSS selection operation on the current Tag.

        This uses the Soup Sieve library. For more information, see
        that library's documentation for the soupsieve.iselect()
        method. It is the same as select(), but it returns a generator
        instead of a list.

        :param selector: A string containing a CSS selector.

        :param namespaces: A dictionary mapping namespace prefixes
            used in the CSS selector to namespace URIs. By default,
            Beautiful Soup will pass in the prefixes it encountered while
            parsing the document.

        :param limit: After finding this number of results, stop looking.

        :param flags: Flags to be passed into Soup Sieve's
            soupsieve.iselect() method.

        :param kwargs: Keyword arguments to be passed into SoupSieve's
            soupsieve.iselect() method.

        :return: A generator
        :rtype: types.GeneratorType
        aclosestuFind the Tag closest to this one that matches the given selector.

        This uses the Soup Sieve library. For more information, see
        that library's documentation for the soupsieve.closest()
        method.

        :param selector: A string containing a CSS selector.

        :param namespaces: A dictionary mapping namespace prefixes
            used in the CSS selector to namespace URIs. By default,
            Beautiful Soup will pass in the prefixes it encountered while
            parsing the document.

        :param flags: Flags to be passed into Soup Sieve's
            soupsieve.closest() method.

        :param kwargs: Keyword arguments to be passed into SoupSieve's
            soupsieve.closest() method.

        :return: A Tag, or None if there is no match.
        :rtype: bs4.Tag

        amatchuCheck whether this Tag matches the given CSS selector.

        This uses the Soup Sieve library. For more information, see
        that library's documentation for the soupsieve.match()
        method.

        :param: a CSS selector.

        :param namespaces: A dictionary mapping namespace prefixes
            used in the CSS selector to namespace URIs. By default,
            Beautiful Soup will pass in the prefixes it encountered while
            parsing the document.

        :param flags: Flags to be passed into Soup Sieve's
            soupsieve.match() method.

        :param kwargs: Keyword arguments to be passed into SoupSieve's
            soupsieve.match() method.

        :return: True if this Tag matches the selector; False otherwise.
        :rtype: bool
        afilteruFilter this Tag's direct children based on the given CSS selector.

        This uses the Soup Sieve library. It works the same way as
        passing this Tag into that library's soupsieve.filter()
        method. More information, for more information see the
        documentation for soupsieve.filter().

        :param namespaces: A dictionary mapping namespace prefixes
            used in the CSS selector to namespace URIs. By default,
            Beautiful Soup will pass in the prefixes it encountered while
            parsing the document.

        :param flags: Flags to be passed into Soup Sieve's
            soupsieve.filter() method.

        :param kwargs: Keyword arguments to be passed into SoupSieve's
            soupsieve.filter() method.

        :return: A ResultSet of Tag objects.
        :rtype: bs4.element.ResultSet

        uIntegration code for CSS selectors using Soup Sieve (pypi: soupsieve).a__doc__a__file__a__spec__aoriginahas_locationa__cached__awarningsweawarnTuThe soupsieve package is not installed. CSS selectors cannot be used.TOobjecta__prepare__aCSSa__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>ubs4.cssa__module__uA proxy object against the soupsieve library, to simplify its
    CSS selector API.

    Acquire this object through the .css attribute on the
    BeautifulSoup object, or on the Tag you want to use as the
    starting point for a CSS selector.

    The main advantage of doing this is that the tag to be selected
    against doesn't need to be explicitly specified in the function
    calls, since it's already scoped to a tag.
    a__qualname__a__init__uCSS.__init__uCSS.escapeuCSS._nsuCSS._rsTnluCSS.compileuCSS.select_oneTnlpuCSS.selectuCSS.iselectuCSS.closestuCSS.matchuCSS.filtera__orig_bases__ubs4\css.pyu<module bs4.css>Ta__class__TaselfatagaapiTaselfansaselectTaselfaresultsaResultSetTaselfaselectanamespacesaflagsakwargsTaselfaidentTaselfaselectanamespacesalimitaflagsakwargsu.bs4.dammitBachardet_moduleadetectaencodingadefaultdictTOsetasortedahtml5aitemsutoo many values to unpack (expected 2)aendswithTw;:nlnaname_to_unicodeaunicode_to_nameacharacterlu<>&ashort_entitiesaaddlaparticlesulu%s(?![%s])avaluesu(%s)w|acodepoint2nameareacompileuInitialize variables used by this class to manage the plethora of
        HTML5 named entities.

        This function returns a 3-tuple containing two dictionaries
        and a regular expression:

        unicode_to_name - A mapping of Unicode strings like "⦨" to
        entity names like "angmsdaa". When a single Unicode string has
        multiple entity names, we try to choose the most commonly-used
        name.

        name_to_unicode: A mapping of entity names like "angmsdaa" to
        Unicode strings like "⦨".

        named_entity_re: A regular expression matching (almost) any
        Unicode string that corresponds to an HTML5 named entity.
        u<genexpr>uEntitySubstitution._populate_class_variables.<locals>.<genexpr>aCHARACTER_TO_HTML_ENTITYagetagroupTlu&%s;uUsed with a regular expression to substitute the
        appropriate HTML entity for a special character string.aCHARACTER_TO_XML_ENTITYuUsed with a regular expression to substitute the
        appropriate XML entity for a special character string.w"w'areplaceTw"u&quot;uMake a value into a quoted XML attribute, possibly escaping it.

         Most strings will be quoted using double quotes.

          Bob's Bar -> "Bob's Bar"

         If a string contains double quotes, it will be quoted using
         single quotes.

          Welcome to "my bar" -> 'Welcome to "my bar"'

         If a string contains both single and double quotes, the
         double quotes will be escaped, and the string will be quoted
         using double quotes.

          Welcome to "Bob's Bar" -> "Welcome to &quot;Bob's bar&quot;
        aAMPERSAND_OR_BRACKETasuba_substitute_xml_entityaquoted_attribute_valueuSubstitute XML entities for special XML characters.

        :param value: A string to be substituted. The less-than sign
          will become &lt;, the greater-than sign will become &gt;,
          and any ampersands will become &amp;. If you want ampersands
          that appear to be part of an entity definition to be left
          alone, use substitute_xml_containing_entities() instead.

        :param make_quoted_attribute: If True, then the string will be
         quoted, as befits an attribute value.
        aBARE_AMPERSAND_OR_BRACKETuSubstitute XML entities for special XML characters.

        :param value: A string to be substituted. The less-than sign will
          become &lt;, the greater-than sign will become &gt;, and any
          ampersands that are not part of an entity defition will
          become &amp;.

        :param make_quoted_attribute: If True, then the string will be
         quoted, as befits an attribute value.
        aCHARACTER_TO_HTML_ENTITY_REa_substitute_html_entityuReplace certain Unicode characters with named HTML entities.

        This differs from data.encode(encoding, 'xmlcharrefreplace')
        in that the goal is to make the result more readable (to those
        with ASCII displays) rather than to recover from
        errors. There's absolutely nothing wrong with a UTF-8 string
        containg a LATIN SMALL LETTER E WITH ACUTE, but replacing that
        character with "&eacute;" will make it more readable to some
        people.

        :param s: A Unicode string.
        aknown_definite_encodingsauser_encodingsaloweraexclude_encodingsachardet_encodingais_htmladeclared_encodingastrip_byte_order_markamarkupasniffed_encodinguConstructor.

        :param markup: Some markup in an unknown encoding.

        :param known_definite_encodings: When determining the encoding
            of `markup`, these encodings will be tried first, in
            order. In HTML terms, this corresponds to the "known
            definite encoding" step defined here:
            https://html.spec.whatwg.org/multipage/parsing.html#parsing-with-a-known-character-encoding

        :param user_encodings: These encodings will be tried after the
            `known_definite_encodings` have been tried and failed, and
            after an attempt to sniff the encoding by looking at a
            byte order mark has failed. In HTML terms, this
            corresponds to the step "user has explicitly instructed
            the user agent to override the document's character
            encoding", defined here:
            https://html.spec.whatwg.org/multipage/parsing.html#determining-the-character-encoding

        :param override_encodings: A deprecated alias for
            known_definite_encodings. Any encodings here will be tried
            immediately after the encodings in
            known_definite_encodings.

        :param is_html: If True, this markup is considered to be
            HTML. Otherwise it's assumed to be XML.

        :param exclude_encodings: These encodings will not be tried,
            even if they otherwise would be.

        uShould we even bother to try this encoding?

        :param encoding: Name of an encoding.
        :param tried: Encodings that have already been tried. This will be modified
            as a side effect.
        uYield a number of encodings that might work for this markup.

        :yield: A sequence of strings.
        aselfa_usableatriedafind_declared_encodingachardet_dammitTuutf-8uwindows-1252aencodingsuEncodingDetector.encodings:nlnc:llnvuutf-16be:lnncuutf-16le:nlnc﻿uutf-8:lnn:nlnbuutf-32be:lnnbuutf-32leadatauIf a byte-order mark is present, strip it and return the encoding it implies.

        :param data: Some markup.
        :return: A 2-tuple (modified data, implied encoding)
        lamaxlf?aencoding_resaxmlahtmlasearchTaendposagroupsadecodeTaasciiareplaceuGiven a document, tries to find its declared encoding.

        An XML encoding is declared at the beginning of the document.

        An HTML encoding is declared in a <meta> tag, hopefully near the
        beginning of the document.

        :param markup: Some markup.
        :param is_html: If True, this markup is considered to be HTML. Otherwise
            it's assumed to be XML.
        :param search_entire_document: Since an encoding is supposed to declared near the beginning
            of the document, most of the time it's only necessary to search a few kilobytes of data.
            Set this to True to force this method to search the entire document.
        asmart_quotes_toatried_encodingsacontains_replacement_charactersaloggingagetLoggerTubs4.dammitalogaEncodingDetectoradetectoraunicode_markupaoriginal_encodinga_convert_fromwuaasciiawarningTuSome characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.uConstructor.

        :param markup: A bytestring representing markup in an unknown encoding.

        :param known_definite_encodings: When determining the encoding
            of `markup`, these encodings will be tried first, in
            order. In HTML terms, this corresponds to the "known
            definite encoding" step defined here:
            https://html.spec.whatwg.org/multipage/parsing.html#parsing-with-a-known-character-encoding

        :param user_encodings: These encodings will be tried after the
            `known_definite_encodings` have been tried and failed, and
            after an attempt to sniff the encoding by looking at a
            byte order mark has failed. In HTML terms, this
            corresponds to the step "user has explicitly instructed
            the user agent to override the document's character
            encoding", defined here:
            https://html.spec.whatwg.org/multipage/parsing.html#determining-the-character-encoding

        :param override_encodings: A deprecated alias for
            known_definite_encodings. Any encodings here will be tried
            immediately after the encodings in
            known_definite_encodings.

        :param smart_quotes_to: By default, Microsoft smart quotes will, like all other characters, be converted
           to Unicode characters. Setting this to 'ascii' will convert them to ASCII quotes instead.
           Setting it to 'xml' will convert them to XML entity references, and setting it to 'html'
           will convert them to HTML entity references.
        :param is_html: If True, this markup is considered to be HTML. Otherwise
            it's assumed to be XML.
        :param exclude_encodings: These encodings will not be considered, even
            if the sniffing code thinks they might make sense.

        TlaMS_CHARS_TO_ASCIIaencodeaMS_CHARSc&#xd;d&uChanges a MS smart quote character to an XML or HTML
        entity, or an ASCII character.afind_codecaappendaENCODINGS_WITH_SMART_QUOTESTc([-])a_sub_ms_chara_to_unicodeuAttempt to convert the markup to the proposed encoding.

        :param proposed: The name of a character encoding.
        uGiven a string and its encoding, decodes the string into Unicode.

        :param encoding: The name of an encoding.
        uIf the markup is an HTML document, returns the encoding declared _within_
        the document.
        a_codecaCHARSET_ALIASESTw-uTw-w_uConvert the name of a character set to a codec name.

        :param charset: The name of a character set.
        :return: The name of a codec.
        acodecsalookupTELookupErrorEValueErrorTw_w-Tuwindows-1252awindows_1252uWindows-1252 and ISO-8859-1 are the only currently supported embedded encodings.Tautf8uutf-8uUTF-8 is the only currently supported main encoding.aposaclsaFIRST_MULTIBYTE_MARKERaLAST_MULTIBYTE_MARKERaMULTIBYTE_MARKERS_AND_SIZESutoo many values to unpack (expected 3)aWINDOWS_1252_TO_UTF8abyte_chunksachunk_startcuFix characters from one encoding embedded in some other encoding.

        Currently the only situation supported is Windows-1252 (or its
        subset ISO-8859-1), embedded in UTF-8.

        :param in_bytes: A bytestring that you suspect contains
            characters from multiple encodings. Note that this _must_
            be a bytestring. If you've already converted the document
            to Unicode, you're too late.
        :param main_encoding: The primary encoding of `in_bytes`.
        :param embedded_encoding: The encoding that was used to embed characters
            in the main document.
        :return: A bytestring in which `embedded_encoding`
          characters have been converted to their `main_encoding`
          equivalents.
        uBeautiful Soup bonus library: Unicode, Dammit

This library converts a bytestream to Unicode through any means
necessary. It is heavily based on code from Mark Pilgrim's Universal
Feed Parser. It works best on XML and HTML, but it does not rewrite the
XML or HTML to reflect a new encoding; that's the tree builder's job.
a__doc__a__file__a__spec__aoriginahas_locationa__cached__aMITa__license__uhtml.entitiesTacodepoint2nameacollectionsTadefaultdictastringacchardetachardetacharset_normalizeru^\s*<\?.*encoding=['"](.*?)['"].*\?>axml_encodingu<\s*meta[^>]+charset\s*=\s*["']?([^>]*?)[ /;'">]ahtml_metaTaasciiwITahtml5TOobjecta__prepare__aEntitySubstitutiona__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>ubs4.dammita__module__uThe ability to substitute XML or HTML entities for certain characters.a__qualname__a_populate_class_variablesuEntitySubstitution._populate_class_variablesaHTML_ENTITY_TO_CHARACTERDw'w"w&w<w>aaposaquotaampaltagtTu([<>]|&(?!#\d+;|#x[0-9a-fA-F]+;|\w+;))Tu([<>&])aclassmethoduEntitySubstitution._substitute_html_entityuEntitySubstitution._substitute_xml_entityuEntitySubstitution.quoted_attribute_valueTFasubstitute_xmluEntitySubstitution.substitute_xmlasubstitute_xml_containing_entitiesuEntitySubstitution.substitute_xml_containing_entitiesasubstitute_htmluEntitySubstitution.substitute_htmla__orig_bases__uSuggests a number of possible encodings for a bytestring.

    Order of precedence:

    1. Encodings you specifically tell EncodingDetector to try first
    (the known_definite_encodings argument to the constructor).

    2. An encoding determined by sniffing the document's byte-order mark.

    3. Encodings you specifically tell EncodingDetector to try if
    byte-order mark sniffing fails (the user_encodings argument to the
    constructor).

    4. An encoding declared within the bytestring itself, either in an
    XML declaration (if the bytestring is to be interpreted as an XML
    document), or in a <meta> tag (if the bytestring is to be
    interpreted as an HTML document.)

    5. An encoding detected through textual analysis by chardet,
    cchardet, or a similar external library.

    4. UTF-8.

    5. Windows-1252.

    TnFnnna__init__uEncodingDetector.__init__uEncodingDetector._usableuEncodingDetector.strip_byte_order_markTFpuEncodingDetector.find_declared_encodingTuA class for detecting the encoding of a *ML document and
    converting it to a Unicode string. If the source encoding is
    windows-1252, can replace MS smart quotes with their HTML or XML
    equivalents.aUnicodeDammitDamacintoshux-sjisumac-romanushift-jisuwindows-1252uiso-8859-1uiso-8859-2uUnicodeDammit.__init__uUnicodeDammit._sub_ms_charTastrictuUnicodeDammit._convert_fromuUnicodeDammit._to_unicodeadeclared_html_encodinguUnicodeDammit.declared_html_encodinguUnicodeDammit.find_codecuUnicodeDammit._codecD ddddddddddddddddddddddddddddddddTaeurou20ACw Tasbquou201ATafnofu192Tabdquou201ETahellipu2026Tadaggeru2020TaDaggeru2021Tacircu2C6Tapermilu2030TaScaronu160Talsaquou2039TaOEligu152w?Tu#x17Du17Dw?pTalsquou2018Tarsquou2019Taldquou201CTardquou201DTabullu2022Tandashu2013Tamdashu2014Tatildeu2DCTatradeu2122Tascaronu161Tarsaquou203ATaoeligu153w?Tu#x17Eu17ETaYumluDddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddaEURw w,wfu,,u...w+u++w^w%wSw<aOEw?wZw?pw'pw"pw*w-u--w~u(TM)wsw>aoew?wzwYw w!wcaGBPw$aYENw|wSu..uu(th)u<<w!w u(R)w-wou+-w2w3Tw'aacutewuwPw*w,w1u(th)u>>u1/4u1/2u3/4w?wApppppaAEwCwEpppwIpppwDwNwOppppw*wOwUpppwYwbwBwapppppaaewcwepppwipppwownwoppppw/wowupppwywbwyDzllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllc€c‚cƒc„c…c†c‡cˆc‰cŠc‹cŒcŽc‘c’c“c”c•c–c—c˜c™cšc›cœcžcŸc c¡c¢c£c¤c¥c¦c§c¨c©cªc«c¬c­c®c¯c°c±c²c³c´cµc¶c·c¸c¹cºc»c¼c½c¾c¿cÀcÁcÂcÃcÄcÅcÆcÇcÈcÉcÊcËcÌcÍcÎcÏcÐcÑcÒcÓcÔcÕcÖc×cØcÙcÚcÛcÜcÝcÞcßcàdcâcãcäcåcæcçcècécêcëcìcícîcïcðcñcòcócôcõcöc÷cøcùcúcûcücýcþTlllTlllTllllTautf8uwindows-1252adetwingleuUnicodeDammit.detwingleubs4\dammit.pyTa.0wxu<module bs4.dammit>Ta__class__Taselfamarkupaknown_definite_encodingsais_htmlaexclude_encodingsauser_encodingsaoverride_encodingsT
aselfamarkupaknown_definite_encodingsasmart_quotes_toais_htmlaexclude_encodingsauser_encodingsaoverride_encodingswuaencodingTaselfacharsetacodecTaselfaproposedaerrorsamarkupasmart_quotes_reasmart_quotes_compiledwuweTaunicode_to_nameaname_to_unicodeashort_entitiesalong_entities_by_first_characteraname_with_semicolonacharacteranameaparticlesashortalong_versionsaignorealong_entitiesalong_entityare_definitionacodepointTaselfamatchaorigasubTaclsamatchobjaentityTaselfadataaencodingaerrorsTaselfaencodingatriedTwsTaselfTaclsain_bytesamain_encodingaembedded_encodingabyte_chunksachunk_startaposabyteastartaendasizeTaselfatriedweTaselfacharsetavalueTaclsamarkupais_htmlasearch_entire_documentaxml_endposahtml_endposaresaxml_reahtml_readeclared_encodingadeclared_encoding_matchTaselfavalueaquote_withareplace_withTaclsadataaencodingTaclswsTaclsavalueamake_quoted_attribute.bs4.elementSaaliasu_alias.<locals>.aliasasetteruAlias one attribute name to another for backward compatibilityaattrusetattr expected 3 arguments, got 2a__new__w:aprefixanameanamespaceaoriginal_valueaPYTHON_SPECIFIC_ENCODINGSuuWhen an HTML document is being encoded to a given encoding, the
        value of a meta tag's 'charset' is the name of the encoding.
        aCHARSET_REasearcharewriteuContentMetaAttributeValue.encode.<locals>.rewriteasubagroupTlaencodingaparentaprevious_elementanext_elementanext_siblingaprevious_siblingacontentsluSets up the initial relations between this element and
        other elements.

        :param parent: The parent of this element.

        :param previous_element: The element parsed immediately before
            this one.

        :param next_element: The element parsed immediately before
            this one.

        :param previous_sibling: The most recently encountered element
            on the same level of the parse tree as this one.

        :param previous_sibling: The next element to be encountered
            on the same level of the parse tree as this one.
        aFormatteraformatter_for_nameasubstituteuFormat the given string using the given formatter.

        :param s: A string.
        :param formatter: A Formatter object, or a string naming one of the standard formatters.
        a_is_xmlaXMLFormatteraHTMLFormatteraCallableTaentity_substitutionaREGISTRYuLook up or create a Formatter for the given identifier,
        if necessary.

        :param formatter: Can be a Formatter object (used as-is), a
            function (used as the entity substitution hook for an
            XMLFormatter or HTMLFormatter), or a string (used to look
            up an XMLFormatter or HTMLFormatter in the appropriate
            registry.
        aknown_xmlais_xmluIs this element part of an XML tree or an HTML tree?

        This is used in formatter_for_name, when deciding whether an
        XMLFormatter or HTMLFormatter is more appropriate. It can be
        inefficient, but it should be called very rarely.
        uYield all strings of certain classes, possibly stripping them.

        This is implemented differently in Tag and NavigableString.
        uYield all strings in this PageElement, stripping them first.

        :yield: A sequence of stripped strings.
        aselfa_all_stringsTtastripped_stringsuPageElement.stripped_stringsajoinTatypesuGet all child strings of this PageElement, concatenated using the
        given separator.

        :param separator: Strings will be concatenated using this separator.

        :param strip: If True, strings will be stripped before being
            concatenated.

        :param types: A tuple of NavigableString subclasses. Any
            strings of a subclass not found in this list will be
            ignored. Although there are exceptions, the default
            behavior in most cases is to consider only NavigableString
            and CData objects. That means no comments, processing
            instructions, etc.

        :return: A string.
        uCannot replace one element with another when the element to be replaced is not part of a tree.luCannot replace a Tag with its parent.aindexaextractTa_self_indexTastartutoo many values to unpack (expected 2)aold_parentainsertuReplace this PageElement with one or more PageElements, keeping the
        rest of the tree the same.

        :param args: One or more PageElements.
        :return: `self`, no longer part of the tree.
        u<genexpr>uPageElement.replace_with.<locals>.<genexpr>uCannot replace an element with its contents when thatelement is not part of a tree.:nnnamy_parentamy_indexuReplace this PageElement with its contents.

        :return: `self`, no longer part of the tree.
        areplace_withaappenduWrap this PageElement inside another one.

        :param wrap_inside: A PageElement.
        :return: `wrap_inside`, occupying the position in the tree that used
           to be occupied by `self`, and with `self` inside it.
        a_last_descendantuDestructively rips this element out of the tree.

        :param _self_index: The location of this element in its parent's
           .contents, if known. Passing this in allows for a performance
           optimization.

        :return: `self`, no longer part of the tree.
        alast_childaTaguFinds the last element beneath this object to be parsed.

        :param is_initialized: Has `setup` been called on this PageElement
            yet?
        :param accept_self: Is `self` an acceptable answer to the question?
        uCannot insert None into a tag.uCannot insert a tag into itself.aNavigableStringabs4TaBeautifulSoupaBeautifulSoupapositionlaminanew_childTFaparents_next_siblinganew_childs_last_elementuInsert a new PageElement in the list of this PageElement's children.

        This works the same way as `list.insert`.

        :param position: The numeric position that should be occupied
           in `self.children` by the new PageElement.
        :param new_child: A PageElement.
        uAppends the given PageElement to the contents of this one.

        :param tag: A PageElement.
        uAppends the given PageElements to this one's contents.

        :param tags: A list of PageElements. If a single Tag is
            provided instead, this PageElement's contents will be extended
            with that Tag's contents.
        uElement has no parent, so 'before' has no meaning.uCan't insert an element before itself.aPageElementapredecessoruMakes the given element(s) the immediate predecessor of this one.

        All the elements will have the same parent, and the given elements
        will be immediately before this one.

        :param args: One or more PageElements.
        uPageElement.insert_before.<locals>.<genexpr>uElement has no parent, so 'after' has no meaning.uCan't insert an element after itself.aoffsetasuccessoruMakes the given element(s) the immediate successor of this one.

        The elements will have the same parent, and the given elements
        will be immediately after this one.

        :param args: One or more PageElements.
        uPageElement.insert_after.<locals>.<genexpr>a_find_oneafind_all_nextuFind the first PageElement that matches the given criteria and
        appears later in the document than this PageElement.

        All find_* methods take a common set of arguments. See the online
        documentation for detailed explanations.

        :param name: A filter on tag name.
        :param attrs: A dictionary of filters on attribute values.
        :param string: A filter for a NavigableString with specific text.
        :kwargs: A dictionary of filters on attribute values.
        :return: A PageElement.
        :rtype: bs4.element.Tag | bs4.element.NavigableString
        a_stacklevella_find_allanext_elementsuFind all PageElements that match the given criteria and appear
        later in the document than this PageElement.

        All find_* methods take a common set of arguments. See the online
        documentation for detailed explanations.

        :param name: A filter on tag name.
        :param attrs: A dictionary of filters on attribute values.
        :param string: A filter for a NavigableString with specific text.
        :param limit: Stop looking after finding this many results.
        :kwargs: A dictionary of filters on attribute values.
        :return: A ResultSet containing PageElements.
        afind_next_siblingsuFind the closest sibling to this PageElement that matches the
        given criteria and appears later in the document.

        All find_* methods take a common set of arguments. See the
        online documentation for detailed explanations.

        :param name: A filter on tag name.
        :param attrs: A dictionary of filters on attribute values.
        :param string: A filter for a NavigableString with specific text.
        :kwargs: A dictionary of filters on attribute values.
        :return: A PageElement.
        :rtype: bs4.element.Tag | bs4.element.NavigableString
        anext_siblingsuFind all siblings of this PageElement that match the given criteria
        and appear later in the document.

        All find_* methods take a common set of arguments. See the online
        documentation for detailed explanations.

        :param name: A filter on tag name.
        :param attrs: A dictionary of filters on attribute values.
        :param string: A filter for a NavigableString with specific text.
        :param limit: Stop looking after finding this many results.
        :kwargs: A dictionary of filters on attribute values.
        :return: A ResultSet of PageElements.
        :rtype: bs4.element.ResultSet
        afind_all_previousuLook backwards in the document from this PageElement and find the
        first PageElement that matches the given criteria.

        All find_* methods take a common set of arguments. See the online
        documentation for detailed explanations.

        :param name: A filter on tag name.
        :param attrs: A dictionary of filters on attribute values.
        :param string: A filter for a NavigableString with specific text.
        :kwargs: A dictionary of filters on attribute values.
        :return: A PageElement.
        :rtype: bs4.element.Tag | bs4.element.NavigableString
        aprevious_elementsuLook backwards in the document from this PageElement and find all
        PageElements that match the given criteria.

        All find_* methods take a common set of arguments. See the online
        documentation for detailed explanations.

        :param name: A filter on tag name.
        :param attrs: A dictionary of filters on attribute values.
        :param string: A filter for a NavigableString with specific text.
        :param limit: Stop looking after finding this many results.
        :kwargs: A dictionary of filters on attribute values.
        :return: A ResultSet of PageElements.
        :rtype: bs4.element.ResultSet
        afind_previous_siblingsuReturns the closest sibling to this PageElement that matches the
        given criteria and appears earlier in the document.

        All find_* methods take a common set of arguments. See the online
        documentation for detailed explanations.

        :param name: A filter on tag name.
        :param attrs: A dictionary of filters on attribute values.
        :param string: A filter for a NavigableString with specific text.
        :kwargs: A dictionary of filters on attribute values.
        :return: A PageElement.
        :rtype: bs4.element.Tag | bs4.element.NavigableString
        aprevious_siblingsuReturns all siblings to this PageElement that match the
        given criteria and appear earlier in the document.

        All find_* methods take a common set of arguments. See the online
        documentation for detailed explanations.

        :param name: A filter on tag name.
        :param attrs: A dictionary of filters on attribute values.
        :param string: A filter for a NavigableString with specific text.
        :param limit: Stop looking after finding this many results.
        :kwargs: A dictionary of filters on attribute values.
        :return: A ResultSet of PageElements.
        :rtype: bs4.element.ResultSet
        afind_parentsDa_stacklevelluFind the closest parent of this PageElement that matches the given
        criteria.

        All find_* methods take a common set of arguments. See the online
        documentation for detailed explanations.

        :param name: A filter on tag name.
        :param attrs: A dictionary of filters on attribute values.
        :kwargs: A dictionary of filters on attribute values.

        :return: A PageElement.
        :rtype: bs4.element.Tag | bs4.element.NavigableString
        aparentsuFind all parents of this PageElement that match the given criteria.

        All find_* methods take a common set of arguments. See the online
        documentation for detailed explanations.

        :param name: A filter on tag name.
        :param attrs: A dictionary of filters on attribute values.
        :param limit: Stop looking after finding this many results.
        :kwargs: A dictionary of filters on attribute values.

        :return: A PageElement.
        :rtype: bs4.element.Tag | bs4.element.NavigableString
        uThe PageElement, if any, that was parsed just after this one.

        :return: A PageElement.
        :rtype: bs4.element.Tag | bs4.element.NavigableString
        uThe PageElement, if any, that was parsed just before this one.

        :return: A PageElement.
        :rtype: bs4.element.Tag | bs4.element.NavigableString
        Da_stacklevelllatextapopTatextawarningsawarnuThe 'text' argument to find()-type methods is deprecated. Use 'string' instead.aDeprecationWarningTastacklevelaSoupStraineraResultSetacountTw:asplitTw:lastraineraresultsuIterates over a generator looking for things that match.uPageElement._find_all.<locals>.<genexpr>aelementalocal_nameuAll PageElements that were parsed after this one.

        :yield: A sequence of PageElements.
        wiuPageElement.next_elementsuAll PageElements that are siblings of this one but were parsed
        later.

        :yield: A sequence of PageElements.
        uPageElement.next_siblingsuAll PageElements that were parsed before this one.

        :yield: A sequence of PageElements.
        uPageElement.previous_elementsuAll PageElements that are siblings of this one but were parsed
        earlier.

        :yield: A sequence of PageElements.
        uPageElement.previous_siblingsuAll PageElements that are parents of this PageElement.

        :yield: A sequence of PageElements.
        uPageElement.parentsa_decomposeduCheck whether a PageElement has been decomposed.

        :rtype: bool
        aDEFAULT_OUTPUT_ENCODINGasetupuCreate a new NavigableString.

        When unpickling a NavigableString, this method is called with
        the string in DEFAULT_OUTPUT_ENCODING. That encoding needs to be
        passed in to the superclass's __new__ or the superclass won't know
        how to handle non-ASCII characters.
        uA copy of a NavigableString has the same contents and class
        as the original, but it is not connected to the parse tree.

        :param recursive: This parameter is ignored; it's only defined
           so that NavigableString.__deepcopy__ implements the same
           signature as Tag.__deepcopy__.
        a__deepcopy__uA copy of a NavigableString can only be a deep copy, because
        only one PageElement can occupy a given place in a parse tree.
        astringu'%s' object has no attribute '%s'a__name__utext.string gives you text. This is for backwards
        compatibility for Navigable*String, but for CData* it lets you
        get the string without the CData wrapper.aformat_stringaPREFIXaSUFFIXuRun the string through the provided formatter.

        :param formatter: A Formatter object, or a string naming one of the standard formatters.
        uA NavigableString cannot be given a name.uPrevent NavigableString.name from ever being set.uYield all strings of certain classes, possibly stripping them.

        This makes it easy for NavigableString to implement methods
        like get_text() as conveniences, creating a consistent
        text-extraction API across all PageElements.

        :param strip: If True, all strings will be stripped before being
            yielded.

        :param types: A tuple of NavigableString subclasses. If this
            NavigableString isn't one of those subclasses, the
            sequence will be empty. By default, the subclasses
            considered are NavigableString and CData objects. That
            means no comments, processing instructions, etc.

        :yield: A sequence that either contains this string, or is empty.

        atypesadefaultaDEFAULT_INTERESTING_STRING_TYPESastripuNavigableString._all_stringsuMake this string ready for output by adding any subclass-specific
            prefix or suffix.

        :param formatter: A Formatter object, or a string naming one
            of the standard formatters. The string will be passed into the
            Formatter, but only to trigger any side effects: the return
            value is ignored.

        :return: The string, with any subclass-specific prefix and
           suffix added on.
        u PUBLIC "%s"u "%s"u SYSTEM "%s"aDoctypeuGenerate an appropriate document type declaration for a given
        public ID and system ID.

        :param name: The name of the document's root element, e.g. 'html'.
        :param pub_id: The Formal Public Identifier for this document type,
            e.g. '-//W3C//DTD XHTML 1.1//EN'
        :param system_id: The system identifier for this document type,
            e.g. 'http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd'

        :return: A Doctype.
        aparser_classuNo value provided for new tag's name.a_namespacesastore_line_numbersasourcelineasourceposabuilderacdata_list_attributesa_replace_cdata_list_attribute_valuesaattrsahiddenacan_be_empty_elementapreserve_whitespace_tagsainteresting_string_typesaset_up_substitutionsastring_containersuBasic constructor.

        :param parser: A BeautifulSoup object.
        :param builder: A TreeBuilder.
        :param name: The name of the tag.
        :param namespace: The URI of this Tag's XML namespace, if any.
        :param prefix: The prefix for this Tag's XML namespace, if any.
        :param attrs: A dictionary of this Tag's attribute values.
        :param parent: The PageElement to use as this Tag's parent.
        :param previous: The PageElement that was parsed immediately before
            this tag.
        :param is_xml: If True, this is an XML tag. Otherwise, this is an
            HTML tag.
        :param sourceline: The line number where this tag was found in its
            source document.
        :param sourcepos: The character position within `sourceline` where this
            tag was found.
        :param can_be_empty_element: If True, this tag should be
            represented as <tag/>. If False, this tag should be represented
            as <tag></tag>.
        :param cdata_list_attributes: A list of attributes whose values should
            be treated as CDATA if they ever show up on this tag.
        :param preserve_whitespace_tags: A list of tag names whose contents
            should have their whitespace preserved.
        :param interesting_string_types: This is a NavigableString
            subclass or a tuple of them. When iterating over this
            Tag's strings in methods like Tag.strings or Tag.get_text,
            these are the types of strings that are interesting enough
            to be considered. The default is to consider
            NavigableString and CData the only interesting string
            subtypes.
        :param namespaces: A dictionary mapping currently active
            namespace prefixes to URIs. This can be used later to
            construct CSS selectors.
        a_clonea_event_streamadescendantsaEND_ELEMENT_EVENTatag_stackamemoDarecursiveFaSTART_ELEMENT_EVENTuA deepcopy of a Tag is a new Tag, unconnected to the parse tree.
        Its contents are a copy of the old Tag's contents.
        uA copy of a Tag must always be a deep copy, because a Tag's
        children can only have one parent at a time.
        Tais_xmlasourcelineasourceposacan_be_empty_elementacdata_list_attributesapreserve_whitespace_tagsainteresting_string_typesTacan_be_empty_elementahiddenuCreate a new Tag just like this one, but with no
        contents and unattached to any parse tree.

        This is the first step in the deepcopy process.
        uIs this tag an empty-element tag? (aka a self-closing tag)

        A tag that has contents is never an empty-element tag.

        A tag that has no contents may or may not be an empty-element
        tag. It depends on the builder used to create the tag. If the
        builder has a designated list of empty-element tags, then only
        a tag whose name shows up in that list is considered an
        empty-element tag.

        If the builder has no designated list of empty-element tags,
        then any tag with no contents is an empty-element tag.
        uConvenience property to get the single string within this
        PageElement.

        TODO It might make sense to have NavigableString.string return
        itself.

        :return: If this element has a single string child, return
         value is that string. If this element has one child tag,
         return value is the 'string' attribute of the child tag,
         recursively. If this element is itself a string, has no
         children, or has more than one child, return value is None.
        aclearuReplace this PageElement's contents with `string`.uYield all strings of certain classes, possibly stripping them.

        :param strip: If True, all strings will be stripped before being
            yielded.

        :param types: A tuple of NavigableString subclasses. Any strings of
            a subclass not found in this list will be ignored. By
            default, the subclasses considered are the ones found in
            self.interesting_string_types. If that's not specified,
            only NavigableString and CData objects will be
            considered. That means no comments, processing
            instructions, etc.

        :yield: A sequence of strings.

        adescendantuTag._all_stringsuRecursively destroys this PageElement and its children.

        This element will be removed from the tree and wiped out; so
        will everything beneath it.

        The behavior of a decomposed PageElement is undefined and you
        should never use one for anything, but if you need to _check_
        whether an element has been decomposed, you can use the
        `decomposed` property.
        adecomposeuWipe out all children of this PageElement by calling extract()
           on them.

        :param decompose: If this is True, decompose() (a more
            destructive method) will be called instead of extract().
        asmoothwaaPreformattedStringamarkeduSmooth out this element's children by consolidating consecutive
        strings.

        This makes pretty-printed output look more natural following a
        lot of operations that modified the tree.
        uTag.index: element not in taguFind the index of a child by identity, not value.

        Avoids issues with tag.contents.index(element) getting the
        index of equal elements.

        :param element: Look for this PageElement in `self.contents`.
        agetuReturns the value of the 'key' attribute for the tag, or
        the value given for 'default' if it doesn't have that
        attribute.uThe same as get(), but always returns a list.

        :param key: The attribute to look for.
        :param default: Use this value if the attribute is not present
            on this PageElement.
        :return: A list of values, probably containing only a single
            value.
        uDoes this PageElement have an attribute with the given name?a__hash__utag[key] returns the value of the 'key' attribute for the Tag,
        and throws an exception if it's not there.uIterating over a Tag iterates over its contents.uThe length of a Tag is the length of its list of contents.uSetting tag[key] sets the value of the 'key' attribute for the
        tag.uDeleting tag[key] deletes all 'key' attributes for the tag.afind_alluCalling a Tag like a function is the same as calling its
        find_all() method. Eg. tag('a') returns a list of all the A tags
        found within this tag.aendswithTaTag:nlnu.%(name)sTag is deprecated, use .find("%(name)s") instead. If you really were looking for a tag called %(name)sTag, use .find("%(name)sTag")DastacklevellafindastartswithTa__uCalling tag.subtag is the same as calling tag.find(name="subtag")aotheruReturns true iff this Tag has the same name, the same attributes,
        and the same contents (recursively) as `other`.uReturns true iff this Tag is not identical to `other`,
        as defined in __eq__.adecodeuRenders this PageElement as a string.

        :param encoding: The encoding to use (Python 2 only).
            TODO: This is now ignored and a warning should be issued
            if a value is provided.
        :return: A (Unicode) string.
        uRenders this PageElement as a Unicode string.aencodeuRender a bytestring representation of this PageElement and its
        contents.

        :param encoding: The destination encoding.
        :param indent_level: Each line of the rendering will be
           indented this many levels. (The formatter decides what a
           'level' means in terms of spaces or other characters
           output.) Used internally in recursive calls while
           pretty-printing.
        :param formatter: A Formatter object, or a string naming one of
            the standard formatters.
        :param errors: An error handling strategy such as
            'xmlcharrefreplace'. This value is passed along into
            encode() and its value should be one of the constants
            defined by Python.
        :return: A bytestring.

        aEMPTY_ELEMENT_EVENTa_format_tagaeventual_encodingaformatterDaopeningtDaopeningFaindent_levelaoutput_readyastring_literal_taga_should_pretty_printa_indent_stringapiecesuYield a sequence of events that can be used to reconstruct the DOM
        for this element.

        This lets us recreate the nested structure of this element
        (e.g. when formatting it as a string) without using recursive
        method calls.

        This is similar in concept to the SAX API, but it's a simpler
        interface designed for internal use. The events are different
        from SAX and the arguments associated with the events are Tags
        and other Beautiful Soup objects.

        :param iterator: An alternate iterator to use when traversing
         the tree.
        aiteratoraself_and_descendantswcais_empty_elementaSTRING_ELEMENT_EVENTuTag._event_streamaindentw
uAdd indentation whitespace before and/or after a string.

        :param s: The string to amend with whitespace.
        :param indent_level: The indentation level; affects how much
           whitespace goes before the string.
        :param indent_before: Whether or not to add whitespace
           before the string.
        :param indent_after: Whether or not to add whitespace
           (a newline) after the string.
        w/aattributesw aAttributeValueWithCharsetSubstitutionaattribute_valuew=aquoted_attribute_valueavoid_element_close_prefixw<w>uShould this tag be pretty-printed?

        Most of them should, but some (such as <pre> in HTML
        documents) should not.
        TaformatteruPretty-print this PageElement as a string.

        :param encoding: The eventual encoding of the string. If this is None,
            a Unicode string will be returned.
        :param formatter: A Formatter object, or a string naming one of
            the standard formatters.
        :return: A Unicode string (if encoding==None) or a bytestring
            (otherwise).
        TaiteratoruRenders the contents of this tag as a Unicode string.

        :param indent_level: Each line of the rendering will be
           indented this many levels. (The formatter decides what a
           'level' means in terms of spaces or other characters
           output.) Used internally in recursive calls while
           pretty-printing.

        :param eventual_encoding: The tag is destined to be
           encoded into this encoding. decode_contents() is _not_
           responsible for performing that encoding. This information
           is passed in so that it can be substituted in if the
           document contains a <META> tag that mentions the document's
           encoding.

        :param formatter: A Formatter object, or a string naming one of
            the standard Formatters.

        adecode_contentsuRenders the contents of this PageElement as a bytestring.

        :param indent_level: Each line of the rendering will be
           indented this many levels. (The formatter decides what a
           'level' means in terms of spaces or other characters
           output.) Used internally in recursive calls while
           pretty-printing.

        :param eventual_encoding: The bytestring will be in this encoding.

        :param formatter: A Formatter object, or a string naming one of
            the standard Formatters.

        :return: A bytestring.
        aencode_contentsTaindent_levelaencodinguDeprecated method for BS3 compatibility.uLook in the children of this PageElement and find the first
        PageElement that matches the given criteria.

        All find_* methods take a common set of arguments. See the online
        documentation for detailed explanations.

        :param name: A filter on tag name.
        :param attrs: A dictionary of filters on attribute values.
        :param recursive: If this is True, find() will perform a
            recursive search of this PageElement's children. Otherwise,
            only the direct children will be considered.
        :param limit: Stop looking after finding this many results.
        :kwargs: A dictionary of filters on attribute values.
        :return: A PageElement.
        :rtype: bs4.element.Tag | bs4.element.NavigableString
        achildrenuLook in the children of this PageElement and find all
        PageElements that match the given criteria.

        All find_* methods take a common set of arguments. See the online
        documentation for detailed explanations.

        :param name: A filter on tag name.
        :param attrs: A dictionary of filters on attribute values.
        :param recursive: If this is True, find_all() will perform a
            recursive search of this PageElement's children. Otherwise,
            only the direct children will be considered.
        :param limit: Stop looking after finding this many results.
        :kwargs: A dictionary of filters on attribute values.
        :return: A ResultSet of PageElements.
        :rtype: bs4.element.ResultSet
        uIterate over all direct children of this PageElement.

        :yield: A sequence of PageElements.
        uIterate over this PageElement and its children in a
        breadth-first sequence.

        :yield: A sequence of PageElements.
        uTag.self_and_descendantsuIterate over all children of this PageElement in a
        breadth-first sequence.

        :yield: A sequence of PageElements.
        acurrentuTag.descendantsacssaselect_oneuPerform a CSS selection operation on the current element.

        :param selector: A CSS selector.

        :param namespaces: A dictionary mapping namespace prefixes
           used in the CSS selector to namespace URIs. By default,
           Beautiful Soup will use the prefixes it encountered while
           parsing the document.

        :param kwargs: Keyword arguments to be passed into Soup Sieve's
           soupsieve.select() method.

        :return: A Tag.
        :rtype: bs4.element.Tag
        aselectuPerform a CSS selection operation on the current element.

        This uses the SoupSieve library.

        :param selector: A string containing a CSS selector.

        :param namespaces: A dictionary mapping namespace prefixes
           used in the CSS selector to namespace URIs. By default,
           Beautiful Soup will use the prefixes it encountered while
           parsing the document.

        :param limit: After finding this number of results, stop looking.

        :param kwargs: Keyword arguments to be passed into SoupSieve's
           soupsieve.select() method.

        :return: A ResultSet of Tags.
        :rtype: bs4.element.ResultSet
        aCSSuReturn an interface to the CSS selector API.uDeprecated generator.uhas_key is deprecated. Use has_attr(key) instead.ahas_attruDeprecated method. This was kind of misleading because has_key()
        (attributes) was different from __in__ (contents).

        has_key() is gone in Python 3, anyway.
        uThe 'text' argument to the SoupStrainer constructor is deprecated. Use 'string' instead.a_normalize_search_valueakwargsaclassaclass_acopyaupdateaitemsanormalized_attrsuConstructor.

        The SoupStrainer constructor takes the same arguments passed
        into the find_* methods. See the online documentation for
        detailed explanations.

        :param name: A filter on tag name.
        :param attrs: A dictionary of filters on attribute values.
        :param string: A filter for a NavigableString with specific text.
        :kwargs: A dictionary of filters on attribute values.
        amatchTautf8a__iter__anew_valueu%s|%suA human-readable representation of this SoupStrainer.amarkupamarkup_namea_matchesamarkup_attr_mapamarkup_attrsafounduCheck whether a Tag with the given name and attributes would
        match this SoupStrainer.

        Used prospectively to decide whether to even bother creating a Tag
        object.

        :param markup_name: A tag name as found in some markup.
        :param markup_attrs: A dictionary of attributes as found in some markup.

        :return: True if the prospective tag would match this SoupStrainer;
            False otherwise.
        asearch_taguI don't know how to match against a %suFind all items in `markup` that match this SoupStrainer.

        Used by the core _find_all() method, which is ultimately
        called by all find_* methods.

        :param markup: A PageElement or a list of them.
        amatch_againstaalready_triedaaddaoriginal_markupa__init__asourceuConstructor.

        :param source: A SoupStrainer.
        :param result: A list of PageElements.
        uResultSet object has no attribute '%s'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?uRaise a helpful exception to explain a common code fix.a__doc__a__file__a__spec__aoriginahas_locationa__cached__aMITa__license__ucollections.abcTaCallableweacollectionsareasysubs4.cssTaCSSubs4.formatterTaFormatteraHTMLFormatteraXMLFormatteruutf-8acompileTu\S+anonwhitespace_reTu\s+awhitespace_rea_aliasSapalmosaunicode_escapeastring_escapeuraw-unicode-escapeaidnaustring-escapeambcsapunycodeaoemuunicode-escapearaw_unicode_escapeaundefinedTOstra__prepare__aNamespacedAttributea__getitem__u%s.__prepare__() must return a mapping, not %su<metaclass>ubs4.elementa__module__uA namespaced string (e.g. 'xml:lang') that remembers the namespace
    ('xml') and the name ('lang') that were used to create it.
    a__qualname__TnnuNamespacedAttribute.__new__a__orig_bases__uA stand-in object for a character encoding specified in HTML.aCharsetMetaAttributeValueuA generic stand-in for the value of a meta tag's 'charset' attribute.

    When Beautiful Soup parses the markup '<meta charset="utf8">', the
    value of the 'charset' attribute will be one of these objects.
    uCharsetMetaAttributeValue.__new__uCharsetMetaAttributeValue.encodeaContentMetaAttributeValueuA generic stand-in for the value of a meta tag's 'content' attribute.

    When Beautiful Soup parses the markup:
     <meta http-equiv="content-type" content="text/html; charset=utf8">

    The value of the 'content' attribute will be one of these objects.
    u((^|;)\s*charset=)([^;]*)wMuContentMetaAttributeValue.__new__uContentMetaAttributeValue.encodeTOobjectuContains the navigational information for some part of the page:
    that is, its current location in the parse tree.

    NavigableString, Tag, etc. are all subclasses of PageElement.
    TnnnnnuPageElement.setupuPageElement.format_stringuPageElement.formatter_for_nameapropertyuPageElement._is_xmlTanext_siblinganextSiblingTaprevious_siblingapreviousSiblingaobjectuPageElement._all_stringsaget_textuPageElement.get_textagetTextuPageElement.replace_withareplaceWithaunwrapuPageElement.unwrapareplace_with_childrenareplaceWithChildrenawrapuPageElement.wrapTnuPageElement.extractTtpuPageElement._last_descendanta_lastRecursiveChilduPageElement.insertuPageElement.appendaextenduPageElement.extendainsert_beforeuPageElement.insert_beforeainsert_afteruPageElement.insert_afterafind_nextuPageElement.find_nextafindNextuPageElement.find_all_nextafindAllNextafind_next_siblinguPageElement.find_next_siblingafindNextSiblinguPageElement.find_next_siblingsafindNextSiblingsafetchNextSiblingsafind_previousuPageElement.find_previousafindPreviousuPageElement.find_all_previousafindAllPreviousafetchPreviousafind_previous_siblinguPageElement.find_previous_siblingafindPreviousSiblinguPageElement.find_previous_siblingsafindPreviousSiblingsafetchPreviousSiblingsafind_parentuPageElement.find_parentafindParentuPageElement.find_parentsafindParentsafetchParentsanextuPageElement.nextapreviousuPageElement.previousuPageElement._find_oneuPageElement._find_alladecomposeduPageElement.decomposedanextGeneratoruPageElement.nextGeneratoranextSiblingGeneratoruPageElement.nextSiblingGeneratorapreviousGeneratoruPageElement.previousGeneratorapreviousSiblingGeneratoruPageElement.previousSiblingGeneratoraparentGeneratoruPageElement.parentGeneratoruA Python Unicode string that is part of a parse tree.

    When Beautiful Soup parses the markup <b>penguin</b>, it will
    create a NavigableString for the string "penguin".
    uNavigableString.__new__uNavigableString.__deepcopy__a__copy__uNavigableString.__copy__a__getnewargs__uNavigableString.__getnewargs__a__getattr__uNavigableString.__getattr__TaminimaluNavigableString.output_readyuSince a NavigableString is not a Tag, it has no .name.

        This property is implemented so that code like this doesn't crash
        when run on a mixture of Tag and NavigableString objects:
            [x.name for x in tag.children]
        uNavigableString.nameastringsuA NavigableString not subject to the normal formatting rules.

    This is an abstract class used for special kinds of strings such
    as comments (the Comment class) and CDATA blocks (the CData
    class).
    uPreformattedString.output_readyaCDatauA CDATA block.u<![CDATA[u]]>aProcessingInstructionuA SGML processing instruction.u<?aXMLProcessingInstructionuAn XML processing instruction.u?>aCommentuAn HTML or XML comment.u<!--u-->aDeclarationuAn XML declaration.uA document type declaration.aclassmethodafor_name_and_idsuDoctype.for_name_and_idsu<!DOCTYPE u>
aStylesheetuA NavigableString representing an stylesheet (probably
    CSS).

    Used to distinguish embedded stylesheets from textual content.
    aScriptuA NavigableString representing an executable script (probably
    Javascript).

    Used to distinguish executable code from textual content.
    aTemplateStringuA NavigableString representing a string found inside an HTML
    template embedded in a larger document.

    Used to distinguish such strings from the main body of the document.
    aRubyTextStringuA NavigableString representing the contents of the <rt> HTML
    element.

    https://dev.w3.org/html5/spec-LC/text-level-semantics.html#the-rt-element

    Can be used to distinguish such strings from the strings they're
    annotating.
    aRubyParenthesisStringuA NavigableString representing the contents of the <rp> HTML
    element.

    https://dev.w3.org/html5/spec-LC/text-level-semantics.html#the-rp-element
    uRepresents an HTML or XML tag that is part of a parse tree, along
    with its attributes and contents.

    When Beautiful Soup parses the markup <b>penguin</b>, it will
    create a Tag object representing the <b> tag.
    TnnnnnnnnnnnnnnnnuTag.__init__Taparser_classaparserClassuTag.__deepcopy__uTag.__copy__uTag._cloneuTag.is_empty_elementaisSelfClosinguTag.stringuTag.decomposeuTag.clearuTag.smoothuTag.indexuTag.getaget_attribute_listuTag.get_attribute_listuTag.has_attruTag.__hash__uTag.__getitem__uTag.__iter__a__len__uTag.__len__a__contains__uTag.__contains__uA tag is non-None even if it has no contents.a__bool__uTag.__bool__a__setitem__uTag.__setitem__a__delitem__uTag.__delitem__a__call__uTag.__call__uTag.__getattr__a__eq__uTag.__eq__a__ne__uTag.__ne__Tuunicode-escapea__repr__uTag.__repr__a__unicode__uTag.__unicode__a__str__aminimalaxmlcharrefreplaceuTag.encodeuTag.decodeuTag._indent_stringuTag._format_taguTag._should_pretty_printTnaminimalaprettifyuTag.prettifyuTag.decode_contentsuTag.encode_contentsarenderContentsuTag.renderContentsuTag.findafindChilduTag.find_allafindAllafindChildrenuTag.childrenuTag.select_oneuTag.selectuTag.cssachildGeneratoruTag.childGeneratorarecursiveChildGeneratoruTag.recursiveChildGeneratorahas_keyuTag.has_keyuEncapsulates a number of ways of matching a markup element (tag or
    string).

    This is primarily used to underpin the find_* methods, but you can
    create one yourself and pass it in as `parse_only` to the
    `BeautifulSoup` constructor, to parse a subset of a large
    document.
    uSoupStrainer.__init__uSoupStrainer._normalize_search_valueuSoupStrainer.__str__uSoupStrainer.search_tagasearchTaguSoupStrainer.searchuSoupStrainer._matchesTOlistuA ResultSet is just a list that keeps track of the SoupStrainer
    that created it.TTuResultSet.__init__uResultSet.__getattr__ubs4\element.pyTa.0aelementTa.0aelementanamealocal_nameaprefixTa.0wxaselfu<module bs4.element>Ta__class__TaselfTaselfaargsakwargsTaselfwxTaselfamemoarecursiveTaselfamemoarecursiveacloneatag_stackaeventaelementadescendant_cloneTaselfakeyTaselfaotherwiamy_childTaselfaattrTaselfatagatag_nameTaselfanameaattrsastringakwargsanormalized_attrsakeyavalueTaselfaparserabuilderanameanamespaceaprefixaattrsaparentapreviousais_xmlasourcelineasourceposacan_be_empty_elementacdata_list_attributesapreserve_whitespace_tagsainteresting_string_typesanamespacesTaselfasourcearesulta__class__TaselfaotherTaclsaoriginal_valueamatchaobjTaclsaoriginal_valueaobjTaclsaprefixanameanamespaceaobjTaclsavaluewuTaselfaencodingTaselfakeyavalueTaattraaliasTaselfastripatypesTaselfastripatypesadescendantadescendant_typeTaselfastripatypesamy_typeavalueTaselfacloneaattrTaselfaiteratoratag_stackwcanow_closed_tagTaselfanameaattrsastringalimitageneratorakwargsa_stacklevelastraineraresultaprefixalocal_namearesultswiafoundTaselfamethodanameaattrsastringakwargswrwlTaselfaeventual_encodingaformatteraopeningaclosing_slashaprefixaattribute_stringaattributesaattrsakeyavaladecodedatextavoid_element_closing_slashTaselfwsaindent_levelaformatteraindent_beforeaindent_afteraspace_beforeaspace_afterTaselfais_initializedaaccept_selfalast_childTaselfamarkupamatch_againstaalready_triedaresultaitemaoriginal_markupakeyamatchTaselfavalueanew_valuewvTaselfaindent_levelTaattrTaselfatagTaselfadecomposeaelementTaselfaindent_levelaeventual_encodingaformatteraiteratorapiecesastring_literal_tagaeventaelementapieceaindent_beforeaindent_afterTaselfaindent_levelaeventual_encodingaformatterTaselfwiwnTaselfastopNodeacurrentTaselfaencodingaindent_levelaformatteraerrorswuTaselfaencodingarewriteTaselfaindent_levelaencodingaformatteracontentsTaselfatagsatagTaselfa_self_indexalast_childanext_elementTaselfanameaattrsarecursiveastringakwargswrwlTaselfanameaattrsarecursiveastringalimitakwargsageneratora_stacklevelTaselfanameaattrsastringalimitakwargsa_stacklevelTaselfanameaattrsastringakwargsTaselfanameaattrsakwargswrwlTaselfanameaattrsalimitakwargsa_stacklevelTaclsanameapub_idasystem_idavalueTaselfwsaformatteraoutputTaselfaformatterwcTaselfakeyadefaultTaselfakeyadefaultavalueTaselfaseparatorastripatypesTaselfaelementwiachildTaselfapositionanew_childaBeautifulSoupasubchildacurrent_indexaprevious_childanew_childs_last_elementaparentaparents_next_siblinganext_childTaselfaargsaparentaoffsetasuccessoraindexTaselfaargsaparentapredecessoraindexTaselfanameTaselfwiTaselfaformatteraignoreTaselfaformatteraoutputTaselfaencodingaformatterTaselfaencodingaprettyPrintaindentLevelTaselfaargsaold_parentamy_indexaidxareplace_withTamatchaencodingTaencodingTaselfamarkupafoundaelementT
aselfamarkup_nameamarkup_attrsafoundamarkupacall_function_with_tag_dataamatchamarkup_attr_mapaattramatch_againstwkwvaattr_valueTaselfaselectoranamespacesalimitakwargsTaselfaselectoranamespacesakwargsTaselfaparentaprevious_elementanext_elementaprevious_siblinganext_siblingTaselfamarkedwiwawbwnTaselfachildTaselfastringTaselfamy_parentamy_indexachildTaselfawrap_insideame.bs4.formatter`aXMLaHTML_DEFAULTSalanguageaentity_substitutionavoid_element_close_prefixa_defaultacdata_containing_tagsaempty_attributes_are_booleanslw aindentuConstructor.

        :param language: This should be Formatter.XML if you are formatting
           XML markup and Formatter.HTML if you are formatting HTML markup.

        :param entity_substitution: A function to call to replace special
           characters with XML/HTML entities. For examples, see
           bs4.dammit.EntitySubstitution.substitute_html and substitute_xml.
        :param void_element_close_prefix: By default, void elements
           are represented as <tag/> (XML rules) rather than <tag>
           (HTML rules). To get <tag>, pass in the empty string.
        :param cdata_containing_tags: The list of tags that are defined
           as containing CDATA in this dialect. For example, in HTML,
           <script> and <style> tags are defined as containing CDATA,
           and their contents should not be formatted.
        :param blank_attributes_are_booleans: Render attributes whose value
            is the empty string as HTML-style boolean attributes.
            (Attributes whose value is None are always rendered this way.)

        :param indent: If indent is a non-negative integer or string,
            then the contents of elements will be indented
            appropriately when pretty-printing. An indent level of 0,
            negative, or "" will only insert newlines. Using a
            positive integer indent indents that many spaces per
            level. If indent is a string (such as ""), that string
            is used to indent each level. The default behavior to
            indent one space per level.
        aelementTaNavigableStringlaNavigableStringaparentanameansuProcess a string that needs to undergo entity substitution.
        This may be a string encountered in an attribute value or as
        text.

        :param ns: A string.
        :return: A string with certain characters replaced by named
           or numeric entities.
        asubstituteuProcess the value of an attribute.

        :param ns: A string.
        :return: A string with certain characters replaced by named
           or numeric entities.
        aattrsasortedaitemsuReorder a tag's attributes however you want.

        By default, attributes are sorted alphabetically. This makes
        behavior consistent between Python 2 and Python 3, and preserves
        backwards compatibility with older versions of Beautiful Soup.

        If `empty_boolean_attributes` is True, then attributes whose
        values are set to the empty string will be treated as boolean
        attributes.
        utoo many values to unpack (expected 2)aselfuu<genexpr>uFormatter.attributes.<locals>.<genexpr>aHTMLFormattera__init__aHTMLaXMLFormattera__doc__a__file__a__spec__aoriginahas_locationa__cached__ubs4.dammitTaEntitySubstitutionaEntitySubstitutiona__prepare__aFormattera__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>ubs4.formattera__module__uDescribes a strategy to use when outputting a parse tree to a string.

    Some parts of this strategy come from the distinction between
    HTML4, HTML5, and XML. Others are configurable by the user.

    Formatters are passed in as the `formatter` argument to methods
    like `PageElement.encode`. Most people won't need to think about
    formatters, and most people who need to think about them can pass
    in one of these predefined strings as `formatter` rather than
    making a new Formatter object:

    For HTML documents:
     * 'html' - HTML entity substitution for generic HTML documents. (default)
     * 'html5' - HTML entity substitution for HTML5 documents, as
                 well as some optimizations in the way tags are rendered.
     * 'minimal' - Only make the substitutions necessary to guarantee
                   valid HTML.
     * None - Do not perform any substitution. This will be faster
              but may result in invalid markup.

    For XML documents:
     * 'html' - Entity substitution for XHTML documents.
     * 'minimal' - Only make the substitutions necessary to guarantee
                   valid XML. (default)
     * None - Do not perform any substitution. This will be faster
              but may result in invalid markup.
    a__qualname__aXML_FORMATTERSaHTML_FORMATTERSahtmlaxmladictasetascriptastyleSascriptastyleTacdata_containing_tagsuFormatter._defaultTnnw/nFluFormatter.__init__uFormatter.substituteaattribute_valueuFormatter.attribute_valueaattributesuFormatter.attributesa__orig_bases__uA generic Formatter for HTML.aREGISTRYuHTMLFormatter.__init__uA generic Formatter for XML.uXMLFormatter.__init__asubstitute_htmlTaentity_substitutionTaentity_substitutionavoid_element_close_prefixaempty_attributes_are_booleansahtml5asubstitute_xmlaminimalTnDaentity_substitutionnubs4\formatter.pyTa.0wkwvaselfu<module bs4.formatter>Ta__class__Taselfaargsakwargsa__class__Taselfalanguageaentity_substitutionavoid_element_close_prefixacdata_containing_tagsaempty_attributes_are_booleansaindentTaselfalanguageavalueakwargTaselfavalueTaselfatagTaselfansaNavigableString.certifi-a__doc__a__file__apathadirnameaenvironagetTaNUITKA_PACKAGE_certifiu\not_existinga__path__a__spec__aoriginahas_locationasubmodule_search_locationsa__cached__acoreTacontentsawherelacontentslawherea__all__u2023.11.17a__version__ucertifi\__init__.pyu<module certifi>u.certifi.core!(awherea__enter__a__exit__areadTnnnadirnamea__file__ajoinucacert.pemaread_textTacertifiucacert.pemaasciiTaencodingu
certifi.py
~~~~~~~~~~

This module returns the installation location of cacert.pem or its contents.
a__doc__a__spec__aoriginahas_locationa__cached__asysaosatypesaUnionlaModuleTypeaPackageTOstruos.PathLikeaResourceTuutf-8astrictapackagearesourceaencodingaerrorsareturnDareturnOstracontentsucertifi\core.pyu<module certifi.core>TapackagearesourceaencodingaerrorsadataTwfu.charset_normalizer.apiTObytearrayObytesuExpected object of type bytes or bytearray, got: {0}aloggeralevelaaddHandleraexplain_handlerasetLevelaTRACEadebugTuEncoding detection on empty bytes, assuming utf_8 intention.aremoveHandleraprevious_logger_levelaloggingaWARNINGaCharsetMatchesaCharsetMatchautf_8Zualogucp_isolation is set. use this flag for debugging purpose. limited list of encoding allowed : %s.u, aiana_nameucp_exclusion is set. use this flag for debugging purpose. limited list of encoding excluded : %s.uoverride steps (%i) and chunk_size (%i) as content does not fit (%i byte(s) given) parameters.lastepsalengthachunk_sizeaTOO_SMALL_SEQUENCEaTOO_BIG_SEQUENCEuTrying to detect encoding from a tiny portion of ({}) byte(s).uUsing lazy str decoding because the payload is quite large, ({}) byte(s).aany_specified_encodinguDetected declarative mark in sequence. Priority +1 given for %s.aidentify_sig_or_bomutoo many values to unpack (expected 2)aprioritized_encodingsuDetected a SIG or BOM mark on first %i byte(s). Priority +1 given for %s.aappendTaasciiTautf_8aIANA_SUPPORTEDacp_isolationatestedaaddasig_encodingashould_strip_sig_or_bomSautf_16autf_32uEncoding %s won't be tested as-is because it require a BOM. Will try some sub-encoder LE/BE.Sautf_7uEncoding %s won't be tested as-is because detection is unreliable without BOM/SIG.ais_multi_byte_encodingTEModuleNotFoundErrorEImportErroruEncoding %s does not provide an IncrementalDecoderasequences:nl nasig_payloadl TEUnicodeDecodeErrorELookupErroruCode page %s does not fit given bytes sequence at ALL. %satested_but_hard_failureatested_but_soft_failureais_cp_similaraencoding_ianau%s is deemed too similar to code page %s and was consider unsuited already. Continuing!aencoding_soft_failedluCode page %s is a multi byte encoding table and it appear that at least one character was encoded using n-bytes.lamaxlacut_sequence_chunksadecoded_payloadamd_chunksamd_ratiosamess_ratioathresholdlaearly_stop_countuLazyStr Loading: After MD chunk decode, code page %s does not fit given bytes sequence at ALL. %s:lPnnadecodeDaerrorsastrictuLazyStr Loading: After final lookup, code page %s does not fit given bytes sequence at ALL. %samax_chunk_gave_upu%s was excluded because of initial chaos probing. Gave up %i time(s). Computed mean chaos is %f %%.aroundldDandigitslaasciiaspecified_encodingu%s passed initial chaos probing. Mean measured chaos is %f %%aencoding_languagesamb_encoding_languagesu{} should target any language(s) of {}acoherence_ratioalanguage_thresholdw,acd_ratiosamerge_coherence_ratiosuWe detected language {} using {}aresultsf?uEncoding detection: %s is most likely the one.uEncoding detection: %s is most likely the one as we detected a BOM or SIG within the beginning of the sequence.afallback_u8afallback_asciiafallback_specifieduNothing got out of the detection process. Using ASCII/UTF-8/Specified fallback.uEncoding detection: %s will be used as a fallback matchaencodingafingerprintTuEncoding detection: utf_8 will be used as a fallback matchTuEncoding detection: ascii will be used as a fallback matchuEncoding detection: Found %s as plausible (best-candidate) for content. With %i alternatives.abestTuEncoding detection: Unable to determine any suitable charset.u
    Given a raw bytes sequence, return the best possibles charset usable to render str objects.
    If there is no results, it is a strong indicator that the source is binary/not text.
    By default, the process will extract 5 blocks of 512o each to assess the mess and coherence of a given sequence.
    And will give up a particular code page after 20% of measured mess. Those criteria are customizable at will.

    The preemptive behavior DOES NOT replace the traditional detection workflow, it prioritize a particular code page
    but never take it for granted. Can improve the performance.

    You may want to focus your attention to some code page or/and not others, use cp_isolation and cp_exclusion for that
    purpose.

    This function will strip the SIG in the payload/sequence every time except on UTF-16, UTF-32.
    By default the library does not setup any handler other than the NullHandler, if you choose to set the 'explain'
    toggle to True it will alter the logger configuration to add a StreamHandler that is suitable for debugging.
    Custom logging format and handler can be set manually.
    afrom_bytesareadu
    Same thing than the function from_bytes but using a file pointer that is already ready.
    Will not close the file pointer.
    arba__enter__a__exit__afrom_fpTnnnu
    Same thing than the function from_bytes but with one extra step. Opening and reading given file path in binary mode.
    Can raise IOError.
    aPathLikeafrom_pathTastepsachunk_sizeathresholdacp_isolationacp_exclusionapreemptive_behaviouraexplainalanguage_thresholdaenable_fallbackTObytesObytearrayu
    Detect if the given input (file, bytes, or path) points to a binary file. aka. not a string.
    Based on the same main heuristic algorithms and default kwargs at the sole exception that fallbacks match
    are disabled to be stricter around ASCII-compatible but unlikely to be a string.
    a__doc__a__file__a__spec__aoriginahas_locationa__cached__aBinaryIOaListaOptionalaSetaUnionacdTacoherence_ratioaencoding_languagesamb_encoding_languagesamerge_coherence_ratiosaconstantTaIANA_SUPPORTEDaTOO_BIG_SEQUENCEaTOO_SMALL_SEQUENCEaTRACEamdTamess_ratioamodelsTaCharsetMatchaCharsetMatchesautilsTaany_specified_encodingacut_sequence_chunksaiana_nameaidentify_sig_or_bomais_cp_similarais_multi_byte_encodingashould_strip_sig_or_bomagetLoggerTacharset_normalizeraStreamHandlerasetFormatteraFormatterTu%(asctime)s | %(levelname)s | %(message)sTllf?nntFf?tacp_exclusionapreemptive_behaviouraexplainaenable_fallbackareturnafpapathTllf?nntFf?Fafp_or_path_or_payloadais_binaryucharset_normalizer\api.pyu<module charset_normalizer.api>T/asequencesastepsachunk_sizeathresholdacp_isolationacp_exclusionapreemptive_behaviouraexplainalanguage_thresholdaenable_fallbackaprevious_logger_levelalengthais_too_small_sequenceais_too_large_sequenceaprioritized_encodingsaspecified_encodingatestedatested_but_hard_failureatested_but_soft_failureafallback_asciiafallback_u8afallback_specifiedaresultsadecoded_payloadabom_or_sig_availableastrip_sig_or_bomais_multi_byte_decoderasimilar_soft_failure_testamulti_byte_bonusamax_chunk_gave_upaearly_stop_countamd_chunksamean_mess_ratioatarget_languagesasig_encodingasig_payloadaencoding_ianaweaencoding_soft_failedar_alazy_str_hard_failureamd_ratiosachunkafallback_entryacd_ratiosachunk_languagesacd_ratios_mergedT
afpastepsachunk_sizeathresholdacp_isolationacp_exclusionapreemptive_behaviouraexplainalanguage_thresholdaenable_fallbackTapathastepsachunk_sizeathresholdacp_isolationacp_exclusionapreemptive_behaviouraexplainalanguage_thresholdaenable_fallbackafpTafp_or_path_or_payloadastepsachunk_sizeathresholdacp_isolationacp_exclusionapreemptive_behaviouraexplainalanguage_thresholdaenable_fallbackaguesses.charset_normalizer.cd4ais_multi_byte_encodinguFunction not supported on multi-byte code pageuencodings.{}aimport_moduleaIncrementalDecoderTaignoreTaerrorsl;l@llwpadecodeaunicode_rangeais_unicode_range_secondaryaseen_rangeslacharacter_countasortedf333333?u
    Return associated unicode ranges in a single byte code page.
    aFREQUENCIESaitemsutoo many values to unpack (expected 2)alanguagesaappendu
    Return inferred languages used with a unicode range.
    aencoding_unicode_rangeaLatinuLatin Basedaunicode_range_languagesu
    Single-byte encoding language association. Some code page are heavily linked to particular language(s).
    This function does the correspondence.
    astartswithTashift_Taiso2022_jpTaeuc_jacp932aJapaneseTagbaZH_NAMESaChineseTaiso2022_kraKO_NAMESaKoreanu
    Multi-byte encoding language association. Some code page are heavily linked to particular language(s).
    This function does the correspondence.
    atarget_have_accentsais_accentuatedatarget_pure_latinais_latinacharacteru
    Determine main aspects from a supported language if it contains accents and if is pure Latin.
    aget_target_featuresu<lambda>ualphabet_languages.<locals>.<lambda>Takeyareverseu
    Return associated languages associated to given characters.
    u<genexpr>ualphabet_languages.<locals>.<genexpr>u{} not availableaindexatarget_language_characters_countaordered_characters_countlacharacter_rank_projectionacharacter_rank_in_languagelacharacter_approved_countu
    Determine if a ordered characters list (by occurrence from most appearance to rarest) match a particular language.
    The result is a ratio between 0. (absolutely no correspondence) and 1. (near perfect fit).
    Beware that is function is not strict on the match in order to ease the detection. (Meaning close match is 1.)
    aisalphaalayersais_suspiciously_successive_rangeacharacter_rangealoweravaluesu
    Given a decoded text sequence, return a list of str. Unicode range / alphabet separation.
    Ex. a text containing English/Latin with a bit a Hebrew will return two items in the resulting list;
    One containing the latin letters and the other hebrew.
    aper_language_ratiosaroundumerge_coherence_ratios.<locals>.<lambda>u
    This function merge results previously given by the function coherence_ratio.
    The return type is the same as coherence_ratio.
    areplaceTu—uaindex_resultsafiltered_resultsamaxu
    We shall NOT return "English—" in CoherenceMatches because it is an alternative
    of "English". This function only keeps the best match and remove the em-dash in it.
    ufilter_alt_coherence_matches.<locals>.<genexpr>asplitTw,aremoveTuLatin Basedaalpha_unicode_splitaCounteramost_commonaTOO_SMALL_SEQUENCEalg_inclusion_listaalphabet_languagesaignore_non_latinacharacters_popularity_compareapopular_character_orderedf?asufficient_match_countaresultsafilter_alt_coherence_matchesucoherence_ratio.<locals>.<lambda>u
    Detect ANY language that can be identified in given sequence. The sequence will be analysed by layers.
    A layer = Character extraction by alphabets/ranges.
    ucoherence_ratio.<locals>.<genexpr>a__doc__a__file__a__spec__aoriginahas_locationa__cached__aimportlibacodecsTaIncrementalDecoderacollectionsTaCounteralru_cacheaTypeCounteraDictaListaOptionalaTupleaconstantTaFREQUENCIESaKO_NAMESaLANGUAGE_SUPPORTED_COUNTaTOO_SMALL_SEQUENCEaZH_NAMESaLANGUAGE_SUPPORTED_COUNTamdTais_suspiciously_successive_rangeamodelsTaCoherenceMatchesaCoherenceMatchesautilsTais_accentuatedais_latinais_multi_byte_encodingais_unicode_range_secondaryaunicode_rangeaiana_nameareturnaprimary_rangeaencoding_languagesamb_encoding_languagesTamaxsizealanguageTOboolpTFacharactersaordered_charactersadecoded_sequenceamerge_coherence_ratiosTlTf?nathresholdalg_inclusionacoherence_ratioucharset_normalizer\cd.pyTa.0wcwoTa.0acharacterTa.0weaindex_resultsTwxu<module charset_normalizer.cd>Tadecoded_sequencealayersacharacter_rangealayer_target_rangeacharacteradiscovered_rangeTacharactersaignore_non_latinalanguagesacharacter_countacharacter_match_countaratioasource_have_accentsalanguagealanguage_charactersatarget_have_accentsatarget_pure_latinTalanguageaordered_charactersacharacter_approved_countaordered_characters_countatarget_language_characters_countalarge_alphabetacharacter_rank_in_languageaexpected_projection_ratioacharacter_rank_projectionacharacters_before_sourceacharacters_after_sourceacharacters_beforeacharacters_afterabefore_match_countaafter_match_countaFREQUENCIES_language_setacharacteracharacter_rankTadecoded_sequenceathresholdalg_inclusionaresultsaignore_non_latinasufficient_match_countasequence_frequenciesacharacter_countapopular_character_orderedaratioalg_inclusion_listalayeramost_commonalanguageTaiana_nameaunicode_rangesaprimary_rangeaspecified_rangeTaiana_namewpaseen_rangesacharacter_countachunkacharacter_rangeadecoderwiTaresultsaindex_resultsano_em_nameafiltered_resultsaresultalanguagearatioTalanguageatarget_have_accentsatarget_pure_latinacharacterTaiana_nameTaresultsaper_language_ratiosaresultasub_resultalanguagearatioamergeTaprimary_rangealanguagesalanguageacharactersacharacteru.charset_normalizerw.u
Charset-Normalizer
~~~~~~~~~~~~~~
The Real First Universal Charset Detector.
A library that helps you read text from an unknown charset encoding.
Motivated by chardet, This package is trying to resolve the issue by taking a new approach.
All IANA character set names for which the Python core library provides codecs are supported.

Basic usage:
   >>> from charset_normalizer import from_bytes
   >>> results = from_bytes('Bсеки човек има право на образование. Oбразованието!'.encode('utf_8'))
   >>> best_guess = results.best()
   >>> str(best_guess)
   'Bсеки човек има право на образование. Oбразованието!'

Others methods and usages are available - see the full documentation
at <https://github.com/Ousret/charset_normalizer>.
:copyright: (c) 2021 by Ahmed TAHRI
:license: MIT, see LICENSE for more details.
a__doc__a__file__apathadirnameaenvironagetTaNUITKA_PACKAGE_charset_normalizeru\not_existinga__path__a__spec__aoriginahas_locationasubmodule_search_locationsa__cached__alogginglaapiTafrom_bytesafrom_fpafrom_pathais_binarylafrom_bytesafrom_fpafrom_pathais_binaryalegacyTadetectadetectamodelsTaCharsetMatchaCharsetMatchesaCharsetMatchaCharsetMatchesautilsTaset_logging_handleraset_logging_handleraversionTaVERSIONa__version__aVERSIONa__version__T
afrom_fpafrom_pathafrom_bytesais_binaryadetectaCharsetMatchaCharsetMatchesa__version__aVERSIONaset_logging_handlera__all__agetLoggerTacharset_normalizeraaddHandleraNullHandlerucharset_normalizer\__init__.pyu<module charset_normalizer>u.charset_normalizer.constantNNaendswithTa_codecSatactisambcsarot_13a__doc__a__file__a__spec__aoriginahas_locationa__cached__a__annotations__acodecsTaBOM_UTF8aBOM_UTF16_BEaBOM_UTF16_LEaBOM_UTF32_BEaBOM_UTF32_LElaBOM_UTF8aBOM_UTF16_BEaBOM_UTF16_LEaBOM_UTF32_BEaBOM_UTF32_LEuencodings.aliasesTaaliasesaaliasesareTaIGNORECASEacompileaIGNORECASEacompileare_compileaDictaListaSetaUnionautf_8autf_7Lc+/v8c+/v9c+/v+c+/v/c+/v8-agb18030c13autf_32autf_16aENCODING_MARKSl aTOO_SMALL_SEQUENCElaTOO_BIG_SEQUENCElaUTF8_MAXIMAL_ALLOCATIONDuControl characteruBasic LatinuLatin-1 SupplementuLatin Extended-AuLatin Extended-BuIPA ExtensionsuSpacing Modifier LettersuCombining Diacritical MarksuGreek and CopticaCyrillicuCyrillic SupplementaArmenianaHebrewaArabicaSyriacuArabic SupplementaThaanaaNKoaSamaritanaMandaicuSyriac SupplementuArabic Extended-BuArabic Extended-AaDevanagariaBengaliaGurmukhiaGujaratiaOriyaaTamilaTeluguaKannadaaMalayalamaSinhalaaThaiaLaoaTibetanaMyanmaraGeorgianuHangul JamoaEthiopicuEthiopic SupplementaCherokeeuUnified Canadian Aboriginal SyllabicsaOghamaRunicaTagalogaHanunooaBuhidaTagbanwaaKhmeraMongolianuUnified Canadian Aboriginal Syllabics ExtendedaLimbuuTai LeuNew Tai LueuKhmer SymbolsaBugineseuTai ThamuCombining Diacritical Marks ExtendedaBalineseaSundaneseaBatakaLepchauOl ChikiuCyrillic Extended-CuGeorgian ExtendeduSundanese SupplementuVedic ExtensionsuPhonetic ExtensionsuPhonetic Extensions SupplementuCombining Diacritical Marks SupplementuLatin Extended AdditionaluGreek ExtendeduGeneral PunctuationuSuperscripts and SubscriptsuCurrency SymbolsuCombining Diacritical Marks for SymbolsuLetterlike SymbolsuNumber FormsaArrowsuMathematical OperatorsuMiscellaneous TechnicaluControl PicturesuOptical Character RecognitionuEnclosed AlphanumericsuBox DrawinguBlock ElementsuGeometric ShapesuMiscellaneous SymbolsaDingbatsuMiscellaneous Mathematical Symbols-AuSupplemental Arrows-AuBraille PatternsuSupplemental Arrows-BuMiscellaneous Mathematical Symbols-BuSupplemental Mathematical OperatorsuMiscellaneous Symbols and ArrowsaGlagoliticuLatin Extended-CaCopticuGeorgian SupplementaTifinaghuEthiopic ExtendeduCyrillic Extended-AuSupplemental PunctuationuCJK Radicals SupplementuKangxi RadicalsuIdeographic Description CharactersuCJK Symbols and PunctuationaHiraganaaKatakanaaBopomofouHangul Compatibility JamoaKanbunuBopomofo ExtendeduCJK StrokesuKatakana Phonetic ExtensionsuEnclosed CJK Letters and MonthsuCJK CompatibilityuCJK Unified Ideographs Extension AuYijing Hexagram SymbolsuCJK Unified IdeographsuYi SyllablesuYi RadicalsaLisuaVaiuCyrillic Extended-BaBamumuModifier Tone LettersuLatin Extended-DuSyloti NagriuCommon Indic Number FormsuPhags-paaSaurashtrauDevanagari ExtendeduKayah LiaRejanguHangul Jamo Extended-AaJavaneseuMyanmar Extended-BaChamuMyanmar Extended-AuTai VietuMeetei Mayek ExtensionsuEthiopic Extended-AuLatin Extended-EuCherokee SupplementuMeetei MayekuHangul SyllablesuHangul Jamo Extended-BuHigh SurrogatesuHigh Private Use SurrogatesuLow SurrogatesuPrivate Use AreauCJK Compatibility IdeographsuAlphabetic Presentation FormsuArabic Presentation Forms-AuVariation SelectorsuVertical FormsuCombining Half MarksuCJK Compatibility FormsuSmall Form VariantsuArabic Presentation Forms-BuHalfwidth and Fullwidth FormsaSpecialsuLinear B SyllabaryuLinear B IdeogramsuAegean NumbersuAncient Greek NumbersuAncient SymbolsuPhaistos DiscaLycianaCarianuCoptic Epact NumbersuOld ItalicaGothicuOld PermicaUgariticuOld PersianaDeseretaShavianaOsmanyaaOsageaElbasanuCaucasian AlbanianaVithkuqiuLinear AuLatin Extended-FuCypriot SyllabaryuImperial AramaicaPalmyreneaNabataeanaHatranaPhoenicianaLydianuMeroitic HieroglyphsuMeroitic CursiveaKharoshthiuOld South ArabianuOld North ArabianaManichaeanaAvestanuInscriptional ParthianuInscriptional PahlaviuPsalter PahlaviuOld TurkicuOld HungarianuHanifi RohingyauRumi Numeral SymbolsaYezidiuArabic Extended-CuOld SogdianaSogdianuOld UyghuraChorasmianaElymaicaBrahmiaKaithiuSora SompengaChakmaaMahajaniaSharadauSinhala Archaic NumbersaKhojkiaMultaniaKhudawadiaGranthaaNewaaTirhutaaSiddhamaModiuMongolian SupplementaTakriaAhomaDograuWarang CitiuDives AkuruaNandinagariuZanabazar SquareaSoyombouUnified Canadian Aboriginal Syllabics Extended-AuPau Cin HauuDevanagari Extended-AaBhaiksukiaMarchenuMasaram GondiuGunjala GondiaMakasaraKawiuLisu SupplementuTamil SupplementaCuneiformuCuneiform Numbers and PunctuationuEarly Dynastic CuneiformuCypro-MinoanuEgyptian HieroglyphsuEgyptian Hieroglyph Format ControlsuAnatolian HieroglyphsuBamum SupplementaMroaTangsauBassa VahuPahawh HmongaMedefaidrinaMiaouIdeographic Symbols and PunctuationaTangutuTangut ComponentsuKhitan Small ScriptuTangut SupplementuKana Extended-BuKana SupplementuKana Extended-AuSmall Kana ExtensionaNushuaDuployanuShorthand Format ControlsuZnamenny Musical NotationuByzantine Musical SymbolsuMusical SymbolsuAncient Greek Musical NotationuKaktovik NumeralsuMayan NumeralsuTai Xuan Jing SymbolsuCounting Rod NumeralsuMathematical Alphanumeric SymbolsuSutton SignWritinguLatin Extended-GuGlagolitic SupplementuCyrillic Extended-DuNyiakeng Puachue HmongaTotoaWanchouNag MundariuEthiopic Extended-BuMende KikakuiaAdlamuIndic Siyaq NumbersuOttoman Siyaq NumbersuArabic Mathematical Alphabetic SymbolsuMahjong TilesuDomino TilesuPlaying CardsuEnclosed Alphanumeric SupplementuEnclosed Ideographic SupplementuMiscellaneous Symbols and PictographsuEmoticons range(Emoji)uOrnamental DingbatsuTransport and Map SymbolsuAlchemical SymbolsuGeometric Shapes ExtendeduSupplemental Arrows-CuSupplemental Symbols and PictographsuChess SymbolsuSymbols and Pictographs Extended-AuSymbols for Legacy ComputinguCJK Unified Ideographs Extension BuCJK Unified Ideographs Extension CuCJK Unified Ideographs Extension DuCJK Unified Ideographs Extension EuCJK Unified Ideographs Extension FuCJK Compatibility Ideographs SupplementuCJK Unified Ideographs Extension GuCJK Unified Ideographs Extension HaTagsuVariation Selectors SupplementuSupplementary Private Use Area-AuSupplementary Private Use Area-B;ll l;l ll;lll;lll;llPl;lPll;lll;llpl;lpll;lll;ll0l;l0ll;lll;lll;llPl;lPll;lll;lll;ll@l;l@l`l;l`lpl;lpll;lll;lll;ll
l;l
l
l;l
ll;lll;lll;lll;ll
l;l
l
l;l
ll;lll;lll;lll;lll;lll;lll;lll;lll;lll;lll;lll;lll;ll l;l l@l;l@l`l;l`ll;lll;lll;lll;llPl;lPll;lll;lll;ll l;l ll;lll;lll;lll;lll;llPl;lPll;lll;lll;lll;lll;lll;lll;lll;lll;ll l;l lp l;lp l l;l l l;l l!l;l!lP!l;lP!l!l;l!l"l;l"l#l;l#l$l;l$l@$l;l@$l`$l;l`$l%l;l%l%l;l%l%l;l%l&l;l&l'l;l'l'l;l'l'l;l'l(l;l(l)l;l)l)l;l)l*l;l*l+l;l+l,l;l,l`,l;l`,l,l;l,l-l;l-l0-l;l0-l-l;l-l-l;l-l.l;l.l.l;l.l/l;l/l/l;l/l0l;l0l@0l;l@0l0l;l0l1l;l1l01l;l01l1l;l1l1l;l1l1l;l1l1l;l1l2l;l2l3l;l3l4l;l4lMl;lMlNl;lNll;lll;llФl;lФll;ll@l;l@ll;lll;ll l;l ll;ll0l;l0l@l;l@ll;lll;lll;ll0l;l0l`l;l`ll;lll;lll;ll`l;l`ll;lll;lll;ll0l;l0lpl;lpll;lll;lll;lll;lll;lll;lll;lll;lll;llPl;lPll;lll;ll l;l l0l;l0lPl;lPlpl;lpll;lll;lll;lll;lll;ll@l;l@ll;lll;lll;lll;lll;lll;ll0l;l0lPl;lPll;lll;lll;llPl;lPll;lll;lll;ll0l;l0lpl;lpll;lll;lll;ll@l;l@l`l;l`ll;lll;lll;ll l;l l@l;lll;ll
l;l
l`
l;l`
l
l;l
l
l;l
ll;ll@l;l@l`l;l`ll;lll;llPl;ll
l;l
l@
l;l`ll;lll;lll;ll0l;l0lpl;lpll;lll;lll;lll;lll;lll;llPl;lPll;lll;lll;llPl;lll;lll;lll;lll;lll;lll;ll`l;l`ll;lll;llPl;llPl;lll;ll`l;lll;llPl;lPll;lll;lll;ll`l;llpl;lpll;ll`l;l`ll;lll;ll`l;lll;ll l;l l$l;l$l$l;l$lP%l;l/l0l;l0l04l;l04l`4l;lDlFl;lhl@jl;l@jlpjl;lpjljl;ljlkl;lklkl;l@nlnl;lolol;lolpl;lpll;lll;lll;lll;lll;lll;ll0l;l0lpl;lpll;lll;lll;lll;lll;lll;llPl;lll;lll;ll`l;l`ll;lll;lll;lll;ll0l;l0ll;llPl;lll;lll;lll;lll;lll;ll`l;lpll;llPl;lll;ll0l;l0ll;lll;lll;lll;lll;llPl;lPll;lll;lll;lll;lll;lll;llpl;lpll;lll;lll;ll@l;l@l l;l ll;lll;ll l;llPl;lPl#l;lll;lll;lll;lllaUNICODE_RANGES_COMBINEDTOstrOrangeLaSupplementaExtendedaExtensionsaModifieraMarksaPunctuationaSymbolsaFormsaOperatorsaMiscellaneousaDrawingaBlockaShapesaSupplementalaTagsaUNICODE_SECONDARY_RANGE_KEYWORDu(?:(?:encoding)|(?:charset)|(?:coding))(?:[\:= ]{1,10})(?:[\"\']?)([a-zA-Z0-9\-_]+)(?:[\"\']?)aRE_POSSIBLE_ENCODING_INDICATIONLacp720acp737acp856acp874acp875acp1006akoi8_rakoi8_takoi8_uaIANA_NO_ALIASESasortedu<lambda>avaluesaIANA_SUPPORTEDaIANA_SUPPORTED_COUNTD(acp037acp1026acp1125acp1140acp1250acp1251acp1252acp1253acp1254acp1257acp273acp437acp500acp850acp857acp858acp860acp861acp862acp863acp865acp866aiso8859_10aiso8859_11aiso8859_13aiso8859_14aiso8859_15aiso8859_16aiso8859_2aiso8859_3aiso8859_4aiso8859_7aiso8859_9akz1048alatin_1amac_icelandamac_romanamac_turkishaptcp154atis_620Lacp1026acp1140acp273acp500Lacp037acp1140acp273acp500Lacp866Lacp037acp1026acp273acp500Laiso8859_2Lakz1048aptcp154Laiso8859_15aiso8859_9alatin_1Laiso8859_7Laiso8859_15aiso8859_9alatin_1Laiso8859_13Lacp037acp1026acp1140acp500Lacp850acp858acp860acp861acp862acp863acp865Lacp037acp1026acp1140acp273Lacp437acp857acp858acp865Lacp850acp858acp865Lacp437acp850acp857acp865Lacp437acp861acp862acp863acp865Lacp437acp860acp862acp863acp865Lacp437acp860acp861acp863acp865Lacp437acp860acp861acp862acp865Lacp437acp850acp857acp858acp860acp861acp862acp863Lacp1125Laiso8859_14aiso8859_15aiso8859_4aiso8859_9alatin_1Latis_620Lacp1257Laiso8859_10aiso8859_15aiso8859_16aiso8859_3aiso8859_9alatin_1Lacp1252acp1254aiso8859_10aiso8859_14aiso8859_16aiso8859_3aiso8859_9alatin_1Laiso8859_14aiso8859_15aiso8859_2aiso8859_3aiso8859_9alatin_1Lacp1250aiso8859_16aiso8859_4Laiso8859_14aiso8859_15aiso8859_16aiso8859_9alatin_1Laiso8859_10aiso8859_2aiso8859_9alatin_1Lacp1253L
acp1252acp1254acp1258aiso8859_10aiso8859_14aiso8859_15aiso8859_16aiso8859_3aiso8859_4alatin_1Lacp1251aptcp154L
acp1252acp1254acp1258aiso8859_10aiso8859_14aiso8859_15aiso8859_16aiso8859_3aiso8859_4aiso8859_9Lamac_romanamac_turkishLamac_icelandamac_turkishLamac_icelandamac_romanLacp1251akz1048Laiso8859_11aIANA_SUPPORTED_SIMILARD aiso2022_kraiso2022_jpaeuc_kratis_620autf_32aeuc_jpakoi8_raiso8859_1aiso8859_2aiso8859_5aiso8859_6aiso8859_7aiso8859_8autf_16acp855amac_cyrillicagb2312agb18030acp932acp866autf_8autf_8_sigashift_jisabig5acp1250acp1251acp1252acp1253acp1255acp1256acp1254acp949uISO-2022-KRuISO-2022-JPuEUC-KRuTIS-620uUTF-32uEUC-JPuKOI8-RuISO-8859-1uISO-8859-2uISO-8859-5uISO-8859-6uISO-8859-7uISO-8859-8uUTF-16aIBM855aMacCyrillicaGB2312aGB18030aCP932aIBM866uutf-8uUTF-8-SIGaSHIFT_JISaBig5uwindows-1250uwindows-1251uWindows-1252uwindows-1253uwindows-1255uwindows-1256uWindows-1254aCP949aCHARDET_CORRESPONDENCETOstrpSw[w=w-w{w/w"w]w;w&w}w<w|w,w>w:aCOMMON_SAFE_ASCII_CHARACTERSSajohabaeuc_kracp949aKO_NAMESSacp950ahzabig5abig5hkscsaZH_NAMESlaTRACED)aEnglishuEnglish—aGermanaFrenchaDutchaItalianaPolishaSpanishaRussianaJapaneseuJapanese—uJapanese——aPortugueseaSwedishaChineseaUkrainianaNorwegianaFinnishaVietnameseaCzechaHungarianaKoreanaIndonesianaTurkishaRomanianaFarsiaArabicaDanishaSerbianaLithuanianaSloveneaSlovakaHebrewaBulgarianaCroatianaHindiaEstonianaThaiaGreekaTamilaKazakhLwewawtwiwownwswrwhwlwdwcwuwmwfwpwgwwwywbwvwkwxwjwzwqLwewawtwiwownwswrwhwlwdwcwmwuwfwpwgwwwbwywvwkwjwxwzwqLwewnwiwrwswtwawdwhwuwlwgwowcwmwbwfwkwwwzwpwvuüuäuöwjLwewawswnwiwtwrwlwuwowdwcwpwmuéwvwgwfwbwhwquàwxuèwywjLwewnwawiwrwtwowdwswlwgwhwvwmwuwkwcwpwbwwwjwzwfwywxuëLwewiwawownwlwtwrwswcwdwuwpwmwgwvwfwbwzwhwquèuàwkwyuòLwawiwowewnwrwzwwwswcwtwkwywdwpwmwuwlwjułwgwbwhuąuęuóLwewawownwswrwiwlwdwtwcwuwmwpwbwgwvwfwyuówhwquíwjwzuáLuоuаuеuиuнuсuтuрuвuлuкuмuдuпuуuгuяuыuзuбuйuьuчuхuжuцLdu人u一u大u亅u丁u丨u竹u笑u口u日u今u二u彳u行u十u土u丶u寸u寺u時u乙u丿u乂u气u気u冂u巾u亠u市u目u儿u見u八u小u凵u県u月u彐u門u間u木u東u山u出u本u中u刀u分u耳u又u取u最u言u田u心u思u刂u前u京u尹u事u生u厶u云u会u未u来u白u冫u楽u灬u馬u尸u尺u駅u明u耂u者u了u阝u都u高u卜u占u厂u广u店u子u申u奄u亻u俺u上u方u冖u学u衣u艮u食u自L`uーuンuスu・uルuトuリuイuアuラuッuクuドuシuレuジuタuフuロuカuテuマuィuグuバuムuプuオuコuデuニuウuメuサuビuナuブuャuエuュuチuキuズuダuパuミuェuョuハuセuベuガuモuツuネuボuソuノuァuヴuワuポuペuピuケuゴuギuザuホuゲuォuヤuヒuユuヨuヘuゼuヌuゥuゾuヶuヂuヲuヅuヵuヱuヰuヮuヽu゠uヾuヷuヿuヸuヹuヺL]uのuにuるuたuとuはuしuいuをuでuてuがuなuれuかuらuさuっuりuすuあuもuこuまuうuくuよuきuんuめuおuけuそuつuだuやuえuどuわuちuみuせuじuばuへuびuずuろuほuげuむuべuひuょuゆuぶuごuゃuねuふuぐuぎuぼuゅuづuざuぞuぬuぜuぱuぽuぷuぴuぃuぁuぇuぺuゞuぢuぉuぅuゐuゝuゑu゛u゜uゎuゔu゚uゟu゙uゕuゖLwawewowswiwrwdwnwtwmwuwcwlwpwgwvwbwfwhuãwquéuçuáwzuíLwewawnwrwtwswiwlwdwowmwkwgwvwhwfwuwpuäwcwbuöuåwywjwxLdu的u一u是u不u了u在u人u有u我u他u这u个u们u中u来u上u大u为u和u国u地u到u以u说u时u要u就u出u会u可u也u你u对u生u能u而u子u那u得u于u着u下u自u之u年u过u发u后u作u里u用u道u行u所u然u家u种u事u成u方u多u经u么u去u法u学u如u都u同u现u当u没u动u面u起u看u定u天u分u还u进u好u小u部u其u些u主u样u理u心u她u本u前u开u但u因u只u从u想u实LuоuаuнuіuиuрuвuтuеuсuкuлuуuдuмuпuзuяuьuбuгuйuчuхuцuїLwewrwnwtwawswiwowlwdwgwkwmwvwfwpwuwbwhuåwywjuøwcuæwwLwawiwnwtwewswlwowuwkuäwmwrwvwjwhwpwywduöwgwcwbwfwwwzLwnwhwtwiwcwgwawowuwmwlwruàuđwswewvwpwbwyuưwduáwkuộuếLwowewawnwtwswiwlwvwrwkwdwuwmwpuíwcwhwzuáwywjwbuěuéuřLwewawtwlwswnwkwrwiwowzuáuéwgwmwbwywvwdwhwuwpwjuöwfwcLu이u다u에u의u는u로u하u을u가u고u지u서u한u은u기u으u년u대u사u시u를u리u도u인u스u일LwawnwewiwrwtwuwswdwkwmwlwgwpwbwowhwywjwcwwwfwvwzwxwqLwawewiwnwrwluıwkwdwtwswmwywuwowbuüuşwvwgwzwhwcwpuçuğLwewiwawrwnwtwuwlwowcwswdwpwmuăwfwvuîwgwbușuțwzwhuâwjLuاuیuرuدuنuهuوuمuتuبuسuلuکuشuزuفuگuعuخuقuجuآuپuحuطuصLuاuلuيuمuوuنuرuتuبuةuعuدuسuفuهuكuقuأuحuجuشuطuصuىuخuإLwewrwnwtwawiwswdwlwowgwmwkwfwvwuwbwhwpuåwyuøuæwcwjwwLuаuиuоuеuнuрuсuуuтuкuјuвuдuмuпuлuгuзuбwawiwewownuцuшLwiwawswowrwewtwnwuwkwmwlwpwvwdwjwguėwbwyuųušužwcuąuįLwewawiwownwrwswlwtwjwvwkwdwpwmwuwzwbwgwhučwcušužwfwyLwowawewnwiwrwvwtwswlwkwdwmwpwuwcwhwjwbwzuáwyuýuíučuéLuיuוuהuלuרuבuתuמuאuשuנuעuםuדuקuחuפuסuכuגuטuצuןuזuךLuаuиuоuеuнuтuрuсuвuлuкuдuпuмuзuгuяuъuуuбuчuцuйuжuщuхLwawiwowewnwrwjwswtwuwkwlwvwdwmwpwgwzwbwcučwhušužućwfLuकuरuसuनuतuमuहuपuयuलuवuजuदuगuबuशuटuअuएuथuभuडuचuधuषuइLwawiwewswtwlwuwnwowkwrwdwmwvwgwpwjwhuäwbuõuüwfwcuöwyLuาuนuรuอuกuเuงuมuยuลuวuดuทuสuตuะuปuบuคuหuแuจuพuชuขuใLuαuτuοuιuεuνuρuσuκuηuπuςuυuμuλuίuόuάuγuέuδuήuωuχuθuύLuகuதuபuடuரuமuலuனuவuறuயuளuசuநuஇuணuஅuஆuழuஙuஎuஉuஒuஸLuаuыuеuнuтuрuлuіuдuсuмuқuкuоuбuиuуuғuжuңuзuшuйuпuгuөaFREQUENCIESaLANGUAGE_SUPPORTED_COUNTucharset_normalizer\constant.pyTwxu<module charset_normalizer.constant>u.charset_normalizer.legacy0awarnucharset-normalizer disregard arguments 'w,u' in legacy function detect()uTObytearrayObytesuExpected object of type bytes or bytearray, got: {0}afrom_bytesabestaencodingalanguageaUnknownf?achaosautf_8aboma_sigaCHARDET_CORRESPONDENCEaconfidenceu
    chardet legacy method
    Detect the encoding of the given byte string. It should be mostly backward-compatible.
    Encoding name will match Chardet own writing whenever possible. (Not on encoding name unsupported by it)
    This function is deprecated and should be used to migrate your project easily, consult the documentation for
    further information. Not planned for removal.

    :param byte_str:     The byte sequence to examine.
    :param should_rename_legacy:  Should we rename legacy encodings
                                  to their more modern equivalents?
    a__doc__a__file__a__spec__aoriginahas_locationa__cached__aAnylaDictaOptionalaUnionawarningsTawarnaapiTafrom_byteslaconstantTaCHARDET_CORRESPONDENCETFabyte_strashould_rename_legacyakwargsareturnTOstrOfloatadetectucharset_normalizer\legacy.pyu<module charset_normalizer.legacy>Tabyte_strashould_rename_legacyakwargswraencodingalanguageaconfidence.charset_normalizer.modelsWa_payloada_encodinga_mean_mess_ratioa_languagesa_has_sig_or_boma_unicode_rangesa_leavesZa_mean_coherence_ratioa_output_payloada_output_encodinga_stringaCharsetMatchu__eq__ cannot be invoked on {} and {}.aencodingafingerprintachaosacoherencef{Gz?f{Gz?aTOO_BIG_SEQUENCEamulti_byte_usageu
        Implemented to make sorted available upon CharsetMatches items.
        arawastrictu<CharsetMatch '{}' bytes({})>uUnable to add instance <{}> as a submatch of a CharsetMatchaappendaaliasesaitemsutoo many values to unpack (expected 2)aselfaalso_known_asu
        Encoding name are known by many name, using this could help when searching for IBM855 when it's listed as CP855.
        lu
        Return the complete list of possible languages found in decoded sequence.
        Usually not really useful. Returned list may be empty even if 'language' property return something != 'Unknown'.
        aasciiacould_be_from_charsetaEnglishucharset_normalizer.cdTaencoding_languagesamb_encoding_languagesaencoding_languagesamb_encoding_languagesais_multi_byte_encodinguLatin BasedaUnknownu
        Most probable language found in decoded sequence. If none were detected or inferred, the property will return
        "Unknown".
        laroundldDandigitslu
        Original untouched bytes.
        aunicode_rangeasortedu
        The complete list of encoding that output the exact SAME str result and therefore could be the originating
        encoding.
        This list does include the encoding available in property 'encoding'.
        aencodeareplaceu
        Method to get re-encoded bytes payload using given target encoding. Default to UTF-8.
        Any errors will be simply ignored by the encoder NOT replaced.
        asha256aoutputahexdigestu
        Retrieve the unique SHA256 computed using the transformed (re-encoded) payload. Not the original one.
        a_resultsa__iter__uCharsetMatches.__iter__aiana_nameu
        Retrieve a single item either by its position or encoding name (alias may be used here).
        Raise KeyError upon invalid index or encoding not present in results.
        uCannot append instance '{}' to CharsetMatchesaitemaadd_submatchu
        Insert a single match. Will be inserted accordingly to preserve sort.
        Can be inserted as a submatch.
        u
        Simply return the first match. Strict equivalent to matches[0].
        abestu
        Redundant method, call the method best(). Kept for BC reasons.
        apathaunicode_pathaencoding_aliasesaalternative_encodingsalanguageaalphabetsahas_sig_or_bomais_preferredadumpsDaensure_asciiaindenttla__doc__a__file__a__spec__aoriginahas_locationa__cached__uencodings.aliasesTaaliasesahashlibTasha256ajsonTadumpsaAnyaDictaIteratoraListaOptionalaTupleaUnionaconstantTaTOO_BIG_SEQUENCEautilsTaiana_nameais_multi_byte_encodingaunicode_rangeucharset_normalizer.modelsa__module__a__qualname__Tnapayloadaguessed_encodingamean_mess_ratioalanguagesaCoherenceMatchesadecoded_payloada__init__uCharsetMatch.__init__DaotherareturnOobjectOboola__eq__uCharsetMatch.__eq__a__lt__uCharsetMatch.__lt__DareturnOfloatuCharsetMatch.multi_byte_usageDareturnOstra__str__uCharsetMatch.__str__a__repr__uCharsetMatch.__repr__DaotherareturnaCharsetMatchnuCharsetMatch.add_submatchuCharsetMatch.encodingareturnuCharsetMatch.encoding_aliasesDareturnOboolabomuCharsetMatch.bomabyte_order_markuCharsetMatch.byte_order_markuCharsetMatch.languagesuCharsetMatch.languageuCharsetMatch.chaosuCharsetMatch.coherenceapercent_chaosuCharsetMatch.percent_chaosapercent_coherenceuCharsetMatch.percent_coherenceDareturnObytesuCharsetMatch.rawasubmatchuCharsetMatch.submatchahas_submatchuCharsetMatch.has_submatchuCharsetMatch.alphabetsuCharsetMatch.could_be_from_charsetTautf_8DaencodingareturnOstrObytesuCharsetMatch.outputuCharsetMatch.fingerprintTu
    Container with every CharsetMatch items ordered by default from most probable to the less one.
    Act like a list(iterable) but does not implements all related methods.
    aCharsetMatchesaresultsuCharsetMatches.__init__TOintOstra__getitem__uCharsetMatches.__getitem__DareturnOinta__len__uCharsetMatches.__len__a__bool__uCharsetMatches.__bool__uCharsetMatches.appenduCharsetMatches.bestafirstuCharsetMatches.firstTOstrOfloataCoherenceMatchaCliDetectionResultuCliDetectionResult.__init__a__dict__uCliDetectionResult.__dict__ato_jsonuCliDetectionResult.to_jsonucharset_normalizer\models.pyu<module charset_normalizer.models>Ta__class__TaselfTaselfaotherTaselfaitemaresultTaselfapathaencodingaencoding_aliasesaalternative_encodingsalanguageaalphabetsahas_sig_or_bomachaosacoherenceaunicode_pathais_preferredTaselfapayloadaguessed_encodingamean_mess_ratioahas_sig_or_bomalanguagesadecoded_payloadTaselfaresultsTaselfaotherachaos_differenceacoherence_differenceTaselfadetected_rangesTaselfaitemamatchTaselfaalso_known_aswuwpTaselfaencoding_languagesamb_encoding_languagesalanguagesTaselfaencodingu.charset_normalizer.utilseaunicodedataanameuWITH GRAVEuWITH ACUTEuWITH CEDILLAuWITH DIAERESISuWITH CIRCUMFLEXuWITH TILDEuWITH MACRONuWITH RING ABOVEadecompositionasplitTw llaUNICODE_RANGES_COMBINEDaitemsutoo many values to unpack (expected 2)u
    Retrieve the Unicode range official name from a single character.
    aLATINacategorywPaunicode_rangeaPunctuationwSwNaFormsaLoaEmoticonsaPictographsaisspaceSu｜w+w>w<wZSaPdaPoaPcaisloweraisupperaCJKaHIRAGANAaKATAKANAaHANGULaTHAIaARABICuISOLATED FORMaUNICODE_SECONDARY_RANGE_KEYWORDarange_nameu<genexpr>uis_unicode_range_secondary.<locals>.<genexpr>aisprintablewu﻿afindallaRE_POSSIBLE_ENCODING_INDICATIONaminadecodeTaasciiaignoreTaerrorsalowerareplaceTw-w_aaliasesu
    Extract using ASCII-only decoder any specified encoding in the first n-bytes.
    Sautf_16_leautf_8_sigautf_8autf_7autf_16autf_16_beautf_32_beautf_32_leautf_32uencodings.{}aimport_moduleaIncrementalDecoderaMultibyteIncrementalDecoderu
    Verify is a specific encoding is a multi byte one based on it IANA name
    aENCODING_MARKSasequenceastartswithTncu
    Identify and extract SIG/BOM in given sequence.
    Sautf_16autf_32uUnable to retrieve IANA for '{}'arangesaaddais_multi_byte_encodingZaiana_name_bTaignore;lllaid_aaid_bacharacter_match_countllaIANA_SUPPORTED_SIMILARu
    Determine if two code page are at least 80% similar. IANA_SUPPORTED_SIMILAR dict was generated using
    the function cp_similarity.
    aloggingagetLoggerasetLevelaStreamHandlerasetFormatteraFormatteraaddHandleradecoded_payloadais_multi_byte_decoderaoffsetsachunk_sizeasequencesabom_or_sig_availableastrip_sig_or_bomasig_payloadaencoding_ianaaignoreastrictllDaerrorsaignoreachunkacut_sequence_chunksa__doc__a__file__a__spec__aoriginahas_locationa__cached__aimportlibacodecsTaIncrementalDecoderuencodings.aliasesTaaliasesalru_cacheareTafindallaGeneratoraListaOptionalaSetaTupleaUniona_multibytecodecTaMultibyteIncrementalDecoderaconstantTaENCODING_MARKSaIANA_SUPPORTED_SIMILARaRE_POSSIBLE_ENCODING_INDICATIONaUNICODE_RANGES_COMBINEDaUNICODE_SECONDARY_RANGE_KEYWORDaUTF8_MAXIMAL_ALLOCATIONaUTF8_MAXIMAL_ALLOCATIONTamaxsizeDacharacterareturnOstrOboolais_accentuatedDacharacterareturnOstrparemove_accentacharacterareturnais_latinais_punctuationais_symbolais_emoticonais_separatorais_case_variableais_cjkais_hiraganaais_katakanaais_hangulais_thaiais_arabicais_arabic_isolated_formDarange_nameareturnOstrOboolais_unicode_range_secondaryais_unprintableTl asearch_zoneaany_specified_encodingTlDanameareturnOstrOboolaidentify_sig_or_bomDaiana_encodingareturnOstrOboolashould_strip_sig_or_bomTtDacp_nameastrictareturnOstrOboolOstraiana_nameadecoded_sequencearange_scanDaiana_name_aaiana_name_bareturnOstrpOfloatacp_similarityDaiana_name_aaiana_name_bareturnOstrpOboolais_cp_similaracharset_normalizeraINFOu%(asctime)s | %(levelname)s | %(message)sDanamealevelaformat_stringareturnOstrOintOstrnaset_logging_handlerTnTOstrnnucharset_normalizer\utils.pyTa.0akeywordarange_nameu<module charset_normalizer.utils>Tasequenceasearch_zoneaseq_lenaresultsaencoding_aliasaencoding_ianaaspecified_encodingTaiana_name_aaiana_name_baid_aaid_bacharacter_match_countato_be_decodedadecoder_aadecoder_bwiTasequencesaencoding_ianaaoffsetsachunk_sizeabom_or_sig_availableastrip_sig_or_bomasig_payloadais_multi_byte_decoderadecoded_payloadachunk_partial_size_chkwiachunkachunk_endacut_sequencewjTacp_nameastrictaencoding_aliasaencoding_ianaTasequenceamarksaiana_encodingamarkTacharacteradescriptionTacharacteracharacter_nameTacharacterTaiana_name_aaiana_name_bTacharacteracharacter_rangeTanameTacharacteracharacter_categoryacharacter_rangeTacharacteracharacter_categoryTarange_nameTadecoded_sequencearangesacharacter_rangeacharacterTacharacteradecomposedacodesTanamealevelaformat_stringaloggerahandlerTaiana_encodingTacharacteracharacter_ordarange_nameaord_rangeu.charset_normalizer.versionu
Expose version
a__doc__a__file__a__spec__aoriginahas_locationa__cached__u3.3.2a__version__w.aVERSIONucharset_normalizer\version.pyu<module charset_normalizer.version>u.colorama.ansiaCSIwmaOSCu2;aBELwJwKastartswithTw_acode_to_charswAwBwCwDw;wHu
This module generates ANSI character codes to printing colors to terminals.
See: http://en.wikipedia.org/wiki/ANSI_escape_code
a__doc__a__file__a__spec__aoriginahas_locationa__cached__u[u]waset_titleTlaclear_screenaclear_lineTOobjectla__prepare__aAnsiCodesa__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>ucolorama.ansia__module__a__qualname__a__init__uAnsiCodes.__init__a__orig_bases__aAnsiCursorTlaUPuAnsiCursor.UPaDOWNuAnsiCursor.DOWNaFORWARDuAnsiCursor.FORWARDaBACKuAnsiCursor.BACKTlpaPOSuAnsiCursor.POSaAnsiForelaBLACKlaREDl aGREENl!aYELLOWl"aBLUEl#aMAGENTAl$aCYANl%aWHITEl'aRESETlZaLIGHTBLACK_EXl[aLIGHTRED_EXl\aLIGHTGREEN_EXl]aLIGHTYELLOW_EXl^aLIGHTBLUE_EXl_aLIGHTMAGENTA_EXl`aLIGHTCYAN_EXlaaLIGHTWHITE_EXaAnsiBackl(l)l*l+l,l-l.l/l1ldlelflglhliljlkaAnsiStylelaBRIGHTlaDIMlaNORMALaRESET_ALLaForeaBackaStyleaCursorucolorama\ansi.pyu<module colorama.ansi>Ta__class__TaselfwnTaselfwxwyTaselfanameavalueTamodeTacodeTatitleu.colorama.ansitowin32a_StreamWrapper__wrappeda_StreamWrapper__convertora__enter__a__exit__awriteaPYCHARM_HOSTEDaosaenvironasysa__stdout__a__stderr__aisattyaclosedTEAttributeErrorEValueErrorawrappedaautoresetaStreamWrapperastreamawinapi_testafilenolaenable_vt_processingastripaconvertaget_win32_callsawin32_callsaon_stderru
        True if this class is actually needed. If false, then the output
        stream will not be affected, nor will win32 calls be issued, so
        wrapping stdout is not actually required. This will generally be
        False on non-Windows platforms, unless optional functionality like
        autoreset has been requested using kwargs to init()
        awintermaAnsiStyleaRESET_ALLareset_allaBRIGHTastyleaWinStyleaDIMaNORMALaAnsiForeaBLACKaforeaWinColoraREDaGREENaYELLOWaBLUEaMAGENTAaCYANaWHITEaGREYaRESETaLIGHTBLACK_EXaLIGHTRED_EXaLIGHTGREEN_EXaLIGHTYELLOW_EXaLIGHTBLUE_EXaLIGHTMAGENTA_EXaLIGHTCYAN_EXaLIGHTWHITE_EXaAnsiBackabackawrite_and_convertaflushacall_win32TwmTlaStylelaconvert_oscaANSI_CSI_REafinditeraspanutoo many values to unpack (expected 2)aselfawrite_plain_textatextacursoraconvert_ansiagroupsu
        Write the given text to our wrapped stream, stripping any ANSI
        sequences from the text, and optionally converting them into win32
        calls.
        aextract_paramsaHfasplitTw;aparamsTlaJKmTlaABCDlu<genexpr>uAnsiToWin32.extract_params.<locals>.<genexpr>wm:lnnwJaerase_screenTaon_stderrwKaerase_lineaset_cursor_positionwAwBwCwDacursor_adjustaANSI_OSC_REaBELacountu02aset_titlea__doc__a__file__a__spec__aoriginahas_locationa__cached__areaansiTaAnsiForeaAnsiBackaAnsiStyleaStyleaBELTaenable_vt_processingaWinTermaWinColoraWinStyleaWinTermawin32Tawindllawinapi_testawindllTOobjecta__prepare__a__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>ucolorama.ansitowin32a__module__u
    Wraps a stream (such as stdout), acting as a transparent proxy for all
    attribute access apart from method 'write()', which is delegated to our
    Converter instance.
    a__qualname__a__init__uStreamWrapper.__init__a__getattr__uStreamWrapper.__getattr__uStreamWrapper.__enter__uStreamWrapper.__exit__a__setstate__uStreamWrapper.__setstate__a__getstate__uStreamWrapper.__getstate__uStreamWrapper.writeuStreamWrapper.isattyapropertyuStreamWrapper.closeda__orig_bases__aAnsiToWin32u
    Implements a 'write()' method which, on Windows, will strip ANSI character
    sequences from the text, and if outputting to a tty, will convert them into
    win32 function calls.
    acompileTu?\[((?:\d|;)*)([a-zA-Z])?Tu?\]([^]*)()?TnnFuAnsiToWin32.__init__ashould_wrapuAnsiToWin32.should_wrapuAnsiToWin32.get_win32_callsuAnsiToWin32.writeuAnsiToWin32.reset_alluAnsiToWin32.write_and_convertuAnsiToWin32.write_plain_textuAnsiToWin32.convert_ansiuAnsiToWin32.extract_paramsuAnsiToWin32.call_win32uAnsiToWin32.convert_oscuAnsiToWin32.flushucolorama\ansitowin32.pyTa.0wpu<module colorama.ansitowin32>Ta__class__TaselfaargsakwargsTaselfanameTaselfTaselfawrappedaconvertastripaautoresetaon_windowsaconversion_supportedafdasystem_has_native_ansiahave_ttyaneed_conversionTaselfawrappedaconverterTaselfastateTaselfacommandaparamsaparamafunc_argsafuncaargsakwargswnwxwyTaselfastreamTaselfaparamstringacommandaparamsTaselfatextamatchastartaendaparamstringacommandaparamsTaselfacommandaparamstringaparamsTaselfastreamastream_isattyTaselfatextTaselfatextacursoramatchastartaendTaselfatextastartaendu.colorama$a__doc__a__file__apathadirnameaenvironagetTaNUITKA_PACKAGE_coloramau\not_existinga__path__a__spec__aoriginahas_locationasubmodule_search_locationsa__cached__ainitialiseTainitadeinitareinitacolorama_textajust_fix_windows_consolelainitladeinitareinitacolorama_textajust_fix_windows_consoleaansiTaForeaBackaStyleaCursoraForeaBackaStyleaCursoraansitowin32TaAnsiToWin32aAnsiToWin32u0.4.6a__version__ucolorama\__init__.pyu<module colorama>u.colorama.initialise4aorig_stdoutaorig_stderrawrapped_stdoutawrapped_stderraatexit_doneafixed_windows_consoleaatexitaunregisterareset_allaAnsiToWin32uwrap=False conflicts with any other arg=Trueawrap_streamasysastdoutastderraconvertastripaautoresetawraparegisterDaconvertastripaautoresetnnFainitaargsakwargsadeinitacolorama_textTaconvertastripaautoresetashould_wrapastreama__doc__a__file__a__spec__aoriginahas_locationa__cached__lacontextlibaansitowin32TaAnsiToWin32la_wipe_internal_state_for_testsTFnntajust_fix_windows_consoleacontextmanagerareinitucolorama\initialise.pyu<module colorama.initialise>TaargsakwargsTaautoresetaconvertastripawrapTanew_stdoutanew_stderrTastreamaconvertastripaautoresetawrapawrapperu.colorama.win32"lu(%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d)adwSizewYwXadwCursorPositionawAttributesasrWindowaTopaLeftaBottomaRightadwMaximumWindowSizeaCONSOLE_SCREEN_BUFFER_INFOa_GetConsoleScreenBufferInfoabyrefa_GetStdHandleaSTDOUTaSTDERRa_winapi_testu<genexpr>uwinapi_test.<locals>.<genexpr>a_SetConsoleTextAttributeaCOORDllaGetConsoleScreenBufferInfoa_SetConsoleCursorPositionaadjusted_positionac_charaencodeawintypesaDWORDTla_FillConsoleOutputCharacterAavalueaWORDa_FillConsoleOutputAttributeu FillConsoleOutputAttribute( hConsole, csbi.wAttributes, dwConSize, coordScreen, &cCharsWritten )a_SetConsoleTitleWa_GetConsoleModeactypesaWinErrora_SetConsoleModea__doc__a__file__a__spec__aoriginahas_locationa__cached__lllaENABLE_VIRTUAL_TERMINAL_PROCESSINGaLibraryLoaderaWinDLLawindllTEAttributeErrorEImportErroru<lambda>aSetConsoleTextAttributeawinapi_testaStructureaPOINTERa_COORDa__prepare__a__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>ucolorama.win32a__module__ustruct in wincon.h.a__qualname__aSMALL_RECTa_fields_a__str__uCONSOLE_SCREEN_BUFFER_INFO.__str__a__orig_bases__akernel32aGetStdHandleaargtypesaHANDLEarestypeaBOOLaSetConsoleCursorPositionaFillConsoleOutputCharacterAaFillConsoleOutputAttributeaSetConsoleTitleWaLPCWSTRaGetConsoleModeaSetConsoleModeTtaFillConsoleOutputCharacteraSetConsoleTitleucolorama\win32.pyTa.0whTw_u<module colorama.win32>Ta__class__Tastream_idaattralengthastartahandleaattributeanum_writtenTastream_idacharalengthastartahandleanum_writtenasuccessTahandleamodeasuccessTastream_idahandleacsbiasuccessTastream_idapositionaadjustaadjusted_positionasrahandleTastream_idaattrsahandleTatitleTaselfTahandleacsbiasuccessu.colorama.wintermuThis isn't windows!awin32aGetConsoleScreenBufferInfoaSTDOUTawAttributesa_defaultaset_attrsa_forea_default_forea_backa_default_backa_stylea_default_stylela_lightlllaWinStyleaBRIGHTaBRIGHT_BACKGROUNDaset_consoleTaattrsaselfTaon_stderraget_attrsaSTDERRaSetConsoleTextAttributeadwCursorPositionwXlwYaSetConsoleCursorPositionaget_positionDaadjustFadwSizeaCOORDTlplaFillConsoleOutputCharacterw aFillConsoleOutputAttributeTlpaSetConsoleTitleawindllawinapi_testaget_osfhandleaGetConsoleModeaSetConsoleModeaENABLE_VIRTUAL_TERMINAL_PROCESSINGTEOSErrorETypeErrora__doc__a__file__a__spec__aoriginahas_locationa__cached__amsvcrtTaget_osfhandleuTawin32TOobjecta__prepare__aWinColora__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>ucolorama.winterma__module__a__qualname__aBLACKaBLUEaGREENlaCYANaREDlaMAGENTAlaYELLOWaGREYa__orig_bases__aNORMALllaWinTerma__init__uWinTerm.__init__uWinTerm.get_attrsuWinTerm.set_attrsTnareset_alluWinTerm.reset_allTnFpaforeuWinTerm.foreabackuWinTerm.backTnFastyleuWinTerm.styleuWinTerm.set_consoleuWinTerm.get_positionaset_cursor_positionuWinTerm.set_cursor_positionTFacursor_adjustuWinTerm.cursor_adjustTlFaerase_screenuWinTerm.erase_screenaerase_lineuWinTerm.erase_lineaset_titleuWinTerm.set_titleaenable_vt_processingucolorama\winterm.pyu<module colorama.winterm>Ta__class__TaselfTaselfabackalightaon_stderrTaselfwxwyaon_stderrahandleapositionaadjusted_positionTafdahandleamodeTaselfamodeaon_stderrahandleacsbiafrom_coordacells_to_eraseTaselfamodeaon_stderrahandleacsbiacells_in_screenacells_before_cursorafrom_coordacells_to_eraseTaselfaforealightaon_stderrTw_TaselfahandleapositionTaselfaon_stderrTaselfavalueTaselfaattrsaon_stderrahandleTaselfapositionaon_stderrahandleTaselfatitleTaselfastyleaon_stderr.idnas.a__doc__a__file__apathadirnameaenvironagetTaNUITKA_PACKAGE_idnau\not_existinga__path__a__spec__aoriginahas_locationasubmodule_search_locationsa__cached__apackage_dataTa__version__la__version__lacoreTaIDNABidiErroraIDNAErroraInvalidCodepointaInvalidCodepointContextaalabelacheck_bidiacheck_hyphen_okacheck_initial_combineracheck_labelacheck_nfcadecodeaencodeaulabelauts46_remapavalid_contextjavalid_contextoavalid_label_lengthavalid_string_lengthaIDNABidiErroraIDNAErroraInvalidCodepointaInvalidCodepointContextaalabelacheck_bidiacheck_hyphen_okacheck_initial_combineracheck_labelacheck_nfcadecodeaencodeaulabelauts46_remapavalid_contextjavalid_contextoavalid_label_lengthavalid_string_lengthaintrangesTaintranges_containaintranges_containLaIDNABidiErroraIDNAErroraInvalidCodepointaInvalidCodepointContextaalabelacheck_bidiacheck_hyphen_okacheck_initial_combineracheck_labelacheck_nfcadecodeaencodeaintranges_containaulabelauts46_remapavalid_contextjavalid_contextoavalid_label_lengthavalid_string_lengtha__all__uidna\__init__.pyu<module idna>u.idna.corepaunicodedataacombininglanameuUnknown character in unicodedataaintranges_containaidnadataascriptsaencodeTapunycodeuU+{:04X}lllutoo many values to unpack (expected 2)abidirectionaluaIDNABidiErroruUnknown directionality in label {} at position {}wRaALaANabidi_labelwLuFirst codepoint in label {} must be directionality L, R or ALL
wRaALaANaENaESaCSaETaONaBNaNSMuInvalid direction for codepoint at position {} in a right-to-left labelLwRaALaENaANaNSMaENanumber_typeTuCan not mix numeral types in a right-to-left labelLwLaENaESaCSaETaONaBNaNSMuInvalid direction for codepoint at position {} in a left-to-right labelavalid_endingTuLabel ends with illegal codepoint directionalityacategorywMaIDNAErrorTuLabel begins with an illegal combining character:llnu--TuLabel has disallowed hyphens in 3rd and 4th positionw-lTuLabel must not start or end with a hyphenanormalizeaNFCTuLabel must be in Normalization Form Cl a_combining_classa_virama_combining_classaposajoining_typesagetlTlLlDlRl
 lalabellllua_is_scriptaGreekllaHebrewl0u・aHiraganaaKatakanaaHanl`lillTObytesObytearrayadecodeTuutf-8TuEmpty Labelacheck_nfcacheck_hyphen_okacheck_initial_combineracodepoint_classesaPVALIDaCONTEXTJavalid_contextjaInvalidCodepointContextuJoiner {} not allowed at position {} in {}a_unotuUnknown codepoint adjacent to joiner {} at position {} in {}aCONTEXTOavalid_contextouCodepoint {} not allowed at position {} in {}aInvalidCodepointuCodepoint {} at position {} of {} not allowedacheck_bidiTaasciiaulabelavalid_label_lengthTuLabel too longTuNo Inputacheck_labela_punycodea_alabel_prefixalowerastartswithTuMalformed A-label, no Punycode eligible content foundTuA-label must not end with a hyphenTuInvalid A-labelauts46dataTauts46datalabisectabisect_leftwZlwVwDw3aoutputwIacode_pointuRe-map the characters in the string according to UTS46 processing.aasciiTushould pass a unicode string to the function rather than a byte string.auts46_remapasplitTw.a_unicode_dots_reTuEmpty domainaalabelaresultaappendTuEmpty labelTcd.avalid_string_lengthTuDomain too longTuInvalid ASCII in A-labelTuw.a__doc__a__file__a__spec__aoriginahas_locationa__cached__TaidnadataareaUnionaOptionalaintrangesTaintranges_containlcxn--acompileTu[.。．｡]TEUnicodeErrora__prepare__a__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>uidna.corea__module__u Base exception for all IDNA-encoding related problems a__qualname__a__orig_bases__u Exception when bidirectional requirements are not satisfied u Exception when a disallowed or unallocated codepoint is used u Exception when the codepoint is not valid in the context it is used DacpareturnOintpDacpascriptareturnOstrpOboolDwsareturnOstrObytesDwsareturnOintOstrTObytesOstrareturnatrailing_dotTFDalabelacheck_ltrareturnOstrOboolpDalabelareturnOstrOboolDalabelareturnOstrnDalabelaposareturnOstrOintOboolDalabelaposaexceptionareturnOstrOintOboolpTOstrObytesObytearrayDalabelareturnOstrObytesTtFDadomainastd3_rulesatransitionalareturnOstrOboolpOstrTFpppwsastrictauts46astd3_rulesatransitionalTFppuidna\core.pyu<module idna.core>TacpwvTacpascriptTwsTalabelalabel_bytesTalabelacheck_ltrabidi_labelaidxacpadirectionartlavalid_endinganumber_typeTalabelTalabelaposacpacp_valueTwsastrictauts46astd3_rulesatrailing_dotaresultalabelsalabelTwsastrictauts46astd3_rulesatransitionalatrailing_dotaresultalabelsalabelTadomainastd3_rulesatransitionalauts46dataaoutputaposacharacode_pointauts46rowastatusareplacementTalabelaposacp_valueaokwiajoining_typeTalabelaposaexceptionacp_valueacpTalabelatrailing_dot.idna.idnadataDa__doc__a__file__a__spec__aoriginahas_locationa__cached__u15.1.0a__version__DaGreekaHanaHebrewaHiraganaaKatakanaT$qtpqxuq~zqqqqqqqqq+&qb]qkfqqqqF qNHqXPqZYq\[q^]q~_qqqqqqqq'!&!qfeq@qqFTq..q..q//q00q00q*0!0q<080qM4qNqnqpqooqooqq:q@q qq^qqKq#PTqqqq7q=8q?>qB@qECqPFTq0A0q00q q32qSPqTq00q10q21q22qX33qpfqqqqqqq# qVUqhdascriptsDlllllllll l!l"l#l$l%l&l'l(l)l*l+l,l-l.l/l0l1l2l3l4l5l6l7l8l9l:l;l<l=l>l?l@lAlBlClDlElFlGlHlIlJlnlolqlrlsltlulvlwlxlylzl{l|l}l~llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll l!l"l#l$l%l&l'l(l)l*l+l,l-l.l/lMlNlOlPlQlRlSlTlUlVlWlXlYlZl[l\l]l^l_l`lalblcldlelflglhliljlklllmlnlolplqlrlsltlulvlwlxlylzl{l|l}l~llllllllllllllllllllllllllllllllllll@lAlBlClDlElFlGlHlIlJlKlLlMlNlOlPlQlRlSlTlUlVlWlXl`lalblcldlelflglhliljlplqlrlsltlulvlwlxlylzl{l|l}l~lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll
ll l!l"l#l$l%l&l'l(l)l*l+l,l-l.l/l0l1l2l3l4l5l6l7l8l9l:l;l<l=l>l?l@lAlBlClDlElFlGlHlIlJlKlLlMlNlOlPlQlRlSlTlUlVlWlXlYlZl[l\l]l^l_l`lalblcldlelflglhliljlklllmlnlolplqlrlsltlulvlwlxlllllllllllllllllllllllllllllllllllllllllll l
 l/ lf lg lh li l@lAlBlClDlElFlGlHlIlJlKlLlMlNlOlPlQlRlSlTlUlVlWlXlYlZl[l\l]l^l_l`lalblcldlelflglhliljlklllmlnlolplqlrlsl
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
llllllllllllllllllllllllll
l
l
l
l
l
l
l
l
l
l

l
l
l

l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l!
l"
l#
l0l1l2l3l4l5l6l7l8l9l:l;l<l=l>l?l@lAlBlClDlElQlRlSlTlplqlrlsltlulvlwlxlylzl{l|l}l~llllllllllllllllllllllllllllllllllllllllllll
lll
lllllllllllllllllll l!l"l#l$l%l&l'l(l)l*l+l,l-l.l/l0l1l2l3l4l5l6l7l8l9l:l;l<l=l>l?l@lAlBlClKlUppppppplDlUlRppplDlRlDlRlDpppplRppplDpppppppppppplClDpppppplRlDppplRpplUlRpplDppppppppppppppplRppppppppppppppppplDppppppppppppppppppppppppppppppppppppplRlDplRpppppppplDlRlDlRlDplRpplUlRplDppplTlRlDpplRpppplDppplRlDpppppppplRlDlRlDlRlDplRplDpppppppppplRpplDpppppppppppppplRplDppplRlDlRplDpplRplDpppppppppppppppppppppppppppppppppppppplClRlDpppplRplDlRlDppppppppplRlDlRpplDlUlDppplUlRlDlRpppppppppppppppppppplCpplDlUplDpppplRlUplDppppppppplRpplUlRlDplRplDppppplRlDpppppppppppppplUplDlClUlDpppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppplUpppplTplDpppppppppppppppppppppppppppppppppplUlClUpppplDppppppppppppppppppppppppppppppppppppppppppppppppplLlUlDpppplRlUlRlUlRplUplLlRpppplDppplLlDpppplRlDpplRlUplRlDppplRlDlRlDlRpplDpplRlDplRlDlRplDlRpppplDplUlLlDpppppppppppppppppppppppppppppppplRlDppplRlDpppppppppppppppplUlDpplRlDppplRplDpppppppppppplUlDplRpplUlDlRplDplRlDplUlDlRplDlUppplRlDlLlUplDppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppplTajoining_typesDaPVALIDaCONTEXTJaCONTEXTOTq.-q:0q{aqqqqqqq
qq
qqqqqqqqq q"!q$#q&%q('q*)q,+q.-q0/q21q65q97q;:q=<q?>qCBqEDqGFqIHqLKqNMqPOqRQqTSqVUqXWqZYq\[q^]q`_qbaqdcqfeqhgqjiqlkqnmqpoqrqqtsqvuqxwq{zq}|q~qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq
qq
qqqqqqqqq q"!q$#q&%q('q*)q,+q.-q0/q21q:3q=<qA?qCBqHGqJIqLKqNMqOqqqqq@qCBqOFqpPqrqqtsqxwq~{qqqqqqqqqqqqqqqqqqq`0qbaqdcqfeqhgqjiqlkqnmqpoqrqqtsqvuqxwqzyq|{q~}qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq
qq
qqqqqqqqq q"!q$#q&%q('q*)q,+q.-q0/qZYq`qqqqqqqqqq@ q`AqunqyqqqqqKqMqqq.q\@qk`qpqqqXqd`qpfqqqqqqqqqqqqqqqqq

q

q

q)

q1
*
q3
2
q6
5
q:
8
q=
<
qC
>
qI
G
qN
K
qR
Q
q]
\
qv
f
q

q

q

q

q

q

q

q

q

q

q

q

q

q
qq
qq)q1*q42q:5qE<qIGqNKqXUqd_qpfqrqqqqqqqqqqqqqqqqqq
qq)q:*qE<qIFqNJqWUq[Xq^]qd`qpfqqqqqqqqqqqqqqq


q

qE

qI
F
qO
J
qX
T
qd
_
qp
f
q
z
q

q

q

q

q

q

q

q

q

q

q

q

q3q;4qO@qZPqqqqqqqqqqqqqqqq* q65q87q:9qC>qHDqMIqRNqWSq\Xqi]qmjqsqqutqzqqqqqqqqqqqJqPqqqIqNJqWPqYXq^Zq`qqqqqqqqqq[q`]qqqmqoqqqqq5qT@qm`qqnqtrqqqqqqqy qqqq, q<0qnFqupqqqqq_ q}`qqqqqqMqZPqtkqq8qJ@q~Mqqq,q0/q<;qONqxkqyqqqqqq
qq
qqqqqqqqq q"!q$#q&%q('q*)q,+q.-q0/q21q43q65q87q:9q<;q>=q@?qBAqDCqFEqHGqJIqLKqNMqPOqRQqTSqVUqXWqZYq\[q^]q`_qbaqdcqfeqhgqjiqlkqnmqpoqrqqtsqvuqxwqzyq|{q~}qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq( q80qF@qXPqh`qqpqsrqutqwvqyxq{zq}|qqqqqqqqqO!N!q!!q`,0,qb,a,qg,e,qi,h,qk,j,qm,l,qr,q,qu,s,q|,v,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q,,q&--q(-'-q.---qh-0-q--q--q--q--q--q--q--q--q--q.-q0./.q00q.0*0q=0<0q0A0q00q00q00q00q011q11q21qM4qNqФq
q,qBAqDCqFEqHGqJIqLKqNMqPOqRQqTSqVUqXWqZYq\[q^]q`_qbaqdcqfeqhgqjiqlkqpmq~tqqqqqqqqqqqqqqqqqq q$#q&%q('q*)q,+q.-q2/q43q65q87q:9q<;q>=q@?qBAqDCqFEqHGqJIqLKqNMqPOqRQqTSqVUqXWqZYq\[q^]q`_qbaqdcqfeqhgqjiqlkqnmqpoqyqq{zq}|qqqqqqqqqqqqqqqqqqqqqqqqqqq§qħçqɧȧq˧ʧqҧѧqԧӧq֧էqاקqڧ٧qq(q-,qt@qƨqڨШqqq.qT0qqکϩqq7qN@qZPqw`qêzqު۪qqqqqq' q/(q[0qi`qqqqqqqq q"!q%#q*'qq0 qtsqq'
q;(q><qN?q^Pqqqqqq qA-qJBq{Pqqqq(qqq(qd0qqqqq7qV@qh`qqqq6
q97q=<qV?qw`qqqqq: qqq

q

q

q

q6

q;
8
q@
?
q}
`
q

q

q

q6qV@qs`qqIqq(

q:
0
qqqqq('qQ0qpqqqGqvfqqqqq5q@6qHDqtPqwvqqqqqq8qB>qqqqqqqqq
qq)q1*q42q:5qE;qIGqNKqQPqXWqd]qmfqupqKqZPqb^qqqqqqqAqEDqZPqqqq,q:0qG@q;qqq
qqq6q97qD;qZPqqqqq?qHGqPqqqq7
qA8qZPqrqqqq
q7q;:q><qH?qZPqf`qigqjqqqqqq;qC>qZPqq# qD%$q//q040qV4@4qGFDq9jhq_j@jqjj`jqjpjqjjqjjqjjq7kkqDk@kqZkPkqxkckqk}kqn`nqKooqoOoqooqooqooqooqpq֌qqqqq#q32qSPqVUqhdqpqkq}pqqqq.qG0q7qm;qvuqqqqq+%qqq"q%#q+&qn0qq-q>0qJ@qONqqqqqqqqqqL"qZPqq:q@q qq^qKq#PTq  Tqqvuqqj`qq00acodepoint_classesuidna\idnadata.pyu<module idna.idnadata>u.idna.intrangesk*asortedllalast_writearangesaappenda_encode_rangeluRepresent a list of integers as a sequence of ranges:
    ((start_0, end_0), (start_1, end_1), ...), such that the original
    integers are exactly those x such that start_i <= x < end_i for some i.

    Ranges are encoded as single integers (start << 32 | end), not as tuples.
    l qabisectabisect_lefta_decode_rangeutoo many values to unpack (expected 2)aposuDetermine if `int_` falls into one of the ranges in `ranges`.u
Given a list of integers, made up of (hopefully) a small number of long runs
of consecutive integers, compute a representation of the form
((start1, end1), (start2, end2) ...). Then answer the question "was x present
in the original list?" in time O(log(# runs)).
a__doc__a__file__a__spec__aoriginahas_locationa__cached__aListaTuplealist_areturnTOintQaintranges_from_listDastartaendareturnOintppwrTOintpaint_aintranges_containuidna\intranges.pyu<module idna.intranges>TwrTastartaendTaint_arangesatuple_aposaleftarightw_Talist_asorted_listarangesalast_writewiacurrent_rangeu.idna.package_dataa__doc__a__file__a__spec__aoriginahas_locationa__cached__u3.6a__version__uidna\package_data.pyu<module idna.package_data>u.idna.uts46dataLdTlw3Tlw3Tlw3Tlw3Tlw3Tlw3Tlw3Tlw3Tlw3Tlw3Tl
w3Tlw3Tlw3Tl
w3Tlw3Tlw3Tlw3Tlw3Tlw3Tlw3Tlw3Tlw3Tlw3Tlw3Tlw3Tlw3Tlw3Tlw3Tlw3Tlw3Tlw3Tlw3Tl w3Tl!w3Tl"w3Tl#w3Tl$w3Tl%w3Tl&w3Tl'w3Tl(w3Tl)w3Tl*w3Tl+w3Tl,w3Tl-wVTl.wVTl/w3Tl0wVTl1wVTl2wVTl3wVTl4wVTl5wVTl6wVTl7wVTl8wVTl9wVTl:w3Tl;w3Tl<w3Tl=w3Tl>w3Tl?w3Tl@w3TlAwMwaTlBwMwbTlCwMwcTlDwMwdTlEwMweTlFwMwfTlGwMwgTlHwMwhTlIwMwiTlJwMwjTlKwMwkTlLwMwlTlMwMwmTlNwMwnTlOwMwoTlPwMwpTlQwMwqTlRwMwrTlSwMwsTlTwMwtTlUwMwuTlVwMwvTlWwMwwTlXwMwxTlYwMwyTlZwMwzTl[w3Tl\w3Tl]w3Tl^w3Tl_w3Tl`w3TlawVTlbwVTlcwVLdTldwVTlewVTlfwVTlgwVTlhwVTliwVTljwVTlkwVTllwVTlmwVTlnwVTlowVTlpwVTlqwVTlrwVTlswVTltwVTluwVTlvwVTlwwVTlxwVTlywVTlzwVTl{w3Tl|w3Tl}w3Tl~w3Tlw3TlwXTlwXTlwXTlwXTlwXTlwXTlwXTlwXTlwXTlwXTlwXTlwXTlwXTlwXTlwXTlwXTlwXTlwXTlwXTlwXTlwXTlwXTlwXTlwXTlwXTlwXTlwXTlwXTlwXTlwXTlwXTlwXTlw3w TlwVTlwVTlwVTlwVTlwVTlwVTlwVTlw3u ̈TlwVTlwMwaTlwVTlwVTlwITlwVTlw3u ̄TlwVTlwVTlwMw2TlwMw3Tlw3u ́TlwMuμTlwVTlwVTlw3u ̧TlwMw1TlwMwoTlwVTlwMu1⁄4TlwMu1⁄2TlwMu3⁄4TlwVTlwMuàTlwMuáTlwMuâTlwMuãTlwMuäTlwMuåTlwMuæTlwMuçLdTlwMuèTlwMuéTlwMuêTlwMuëTlwMuìTlwMuíTlwMuîTlwMuïTlwMuðTlwMuñTlwMuòTlwMuóTlwMuôTlwMuõTlwMuöTlwVTlwMuøTlwMuùTlwMuúTlwMuûTlwMuüTlwMuýTlwMuþTlwDassTlwVTlwVTlwVTlwVTlwVTlwVTlwVTlwVTlwVTlwVTlwVTlwVTlwVTlwVTlwVTlwVTlwVTlwVTlwVTlwVTlwVTlwVTlwVTlwVTlwVTlwVTlwVTlwVTlwVTlwVTlwVTlwVTlwMuāTlwVTlwMuăTlwVTlwMuąTlwVTlwMućTlwVTlwMuĉTlwVTl
wMuċTlwVTlwMučTl
wVTlwMuďTlwVTlwMuđTlwVTlwMuēTlwVTlwMuĕTlwVTlwMuėTlwVTlwMuęTlwVTlwMuěTlwVTlwMuĝTlwVTlwMuğTlwVTl wMuġTl!wVTl"wMuģTl#wVTl$wMuĥTl%wVTl&wMuħTl'wVTl(wMuĩTl)wVTl*wMuīTl+wVLdTl,wMuĭTl-wVTl.wMuįTl/wVTl0wMui̇Tl1wVTl2wMaijTl4wMuĵTl5wVTl6wMuķTl7wVTl9wMuĺTl:wVTl;wMuļTl<wVTl=wMuľTl>wVTl?wMul·TlAwMułTlBwVTlCwMuńTlDwVTlEwMuņTlFwVTlGwMuňTlHwVTlIwMuʼnTlJwMuŋTlKwVTlLwMuōTlMwVTlNwMuŏTlOwVTlPwMuőTlQwVTlRwMuœTlSwVTlTwMuŕTlUwVTlVwMuŗTlWwVTlXwMuřTlYwVTlZwMuśTl[wVTl\wMuŝTl]wVTl^wMuşTl_wVTl`wMušTlawVTlbwMuţTlcwVTldwMuťTlewVTlfwMuŧTlgwVTlhwMuũTliwVTljwMuūTlkwVTllwMuŭTlmwVTlnwMuůTlowVTlpwMuűTlqwVTlrwMuųTlswVTltwMuŵTluwVTlvwMuŷTlwwVTlxwMuÿTlywMuźTlzwVTl{wMużTl|wVTl}wMužTl~wVTlwMwsTlwVTlwMuɓTlwMuƃTlwVTlwMuƅTlwVTlwMuɔTlwMuƈTlwVTlwMuɖTlwMuɗTlwMuƌTlwVTlwMuǝTlwMuəTlwMuɛTlwMuƒTlwVTlwMuɠLdTlwMuɣTlwVTlwMuɩTlwMuɨTlwMuƙTlwVTlwMuɯTlwMuɲTlwVTlwMuɵTlwMuơTlwVTlwMuƣTlwVTlwMuƥTlwVTlwMuʀTlwMuƨTlwVTlwMuʃTlwVTlwMuƭTlwVTlwMuʈTlwMuưTlwVTlwMuʊTlwMuʋTlwMuƴTlwVTlwMuƶTlwVTlwMuʒTlwMuƹTlwVTlwMuƽTlwVTlwMudžTlwMaljTlwManjTlwMuǎTlwVTlwMuǐTlwVTlwMuǒTlwVTlwMuǔTlwVTlwMuǖTlwVTlwMuǘTlwVTlwMuǚTlwVTlwMuǜTlwVTlwMuǟTlwVTlwMuǡTlwVTlwMuǣTlwVTlwMuǥTlwVTlwMuǧTlwVTlwMuǩTlwVTlwMuǫTlwVTlwMuǭTlwVTlwMuǯTlwVTlwMadzTlwMuǵTlwVTlwMuƕTlwMuƿTlwMuǹTlwVTlwMuǻTlwVTlwMuǽTlwVTlwMuǿTlwVTlwMuȁTlwVTlwMuȃTlwVTlwMuȅTlwVTlwMuȇTlwVTlwMuȉTlwVTl
wMuȋTlwVTlwMuȍLdTl
wVTlwMuȏTlwVTlwMuȑTlwVTlwMuȓTlwVTlwMuȕTlwVTlwMuȗTlwVTlwMușTlwVTlwMuțTlwVTlwMuȝTlwVTlwMuȟTlwVTl wMuƞTl!wVTl"wMuȣTl#wVTl$wMuȥTl%wVTl&wMuȧTl'wVTl(wMuȩTl)wVTl*wMuȫTl+wVTl,wMuȭTl-wVTl.wMuȯTl/wVTl0wMuȱTl1wVTl2wMuȳTl3wVTl:wMuⱥTl;wMuȼTl<wVTl=wMuƚTl>wMuⱦTl?wVTlAwMuɂTlBwVTlCwMuƀTlDwMuʉTlEwMuʌTlFwMuɇTlGwVTlHwMuɉTlIwVTlJwMuɋTlKwVTlLwMuɍTlMwVTlNwMuɏTlOwVTlwMwhTlwMuɦTlwMwjTlwMwrTlwMuɹTlwMuɻTlwMuʁTlwMwwTlwMwyTlwVTlw3u ̆Tlw3u ̇Tlw3u ̊Tlw3u ̨Tlw3u ̃Tlw3u ̋TlwVTlwMuɣTlwMwlTlwMwsTlwMwxTlwMuʕTlwVTl@wMùTlAwMúTlBwVTlCwMu̓TlDwMǘTlEwMuιTlFwVTlOwITlPwVTlpwMuͱTlqwVTlrwMuͳTlswVTltwMuʹTluwVTlvwMuͷTlwwVLdTlxwXTlzw3u ιTl{wVTl~w3w;TlwMuϳTlwXTlw3u ́Tlw3u ̈́TlwMuάTlwMu·TlwMuέTlwMuήTlwMuίTlwXTlwMuόTlwXTlwMuύTlwMuώTlwVTlwMuαTlwMuβTlwMuγTlwMuδTlwMuεTlwMuζTlwMuηTlwMuθTlwMuιTlwMuκTlwMuλTlwMuμTlwMuνTlwMuξTlwMuοTlwMuπTlwMuρTlwXTlwMuσTlwMuτTlwMuυTlwMuφTlwMuχTlwMuψTlwMuωTlwMuϊTlwMuϋTlwVTlwDuσTlwVTlwMuϗTlwMuβTlwMuθTlwMuυTlwMuύTlwMuϋTlwMuφTlwMuπTlwVTlwMuϙTlwVTlwMuϛTlwVTlwMuϝTlwVTlwMuϟTlwVTlwMuϡTlwVTlwMuϣTlwVTlwMuϥTlwVTlwMuϧTlwVTlwMuϩTlwVTlwMuϫTlwVTlwMuϭTlwVTlwMuϯTlwVTlwMuκTlwMuρTlwMuσTlwVTlwMuθTlwMuεTlwVTlwMuϸTlwVTlwMuσTlwMuϻTlwVTlwMuͻTlwMuͼTlwMuͽTlwMuѐTlwMuёTlwMuђLdTlwMuѓTlwMuєTlwMuѕTlwMuіTlwMuїTlwMuјTlwMuљTl
wMuњTlwMuћTlwMuќTl
wMuѝTlwMuўTlwMuџTlwMuаTlwMuбTlwMuвTlwMuгTlwMuдTlwMuеTlwMuжTlwMuзTlwMuиTlwMuйTlwMuкTlwMuлTlwMuмTlwMuнTlwMuоTlwMuпTl wMuрTl!wMuсTl"wMuтTl#wMuуTl$wMuфTl%wMuхTl&wMuцTl'wMuчTl(wMuшTl)wMuщTl*wMuъTl+wMuыTl,wMuьTl-wMuэTl.wMuюTl/wMuяTl0wVTl`wMuѡTlawVTlbwMuѣTlcwVTldwMuѥTlewVTlfwMuѧTlgwVTlhwMuѩTliwVTljwMuѫTlkwVTllwMuѭTlmwVTlnwMuѯTlowVTlpwMuѱTlqwVTlrwMuѳTlswVTltwMuѵTluwVTlvwMuѷTlwwVTlxwMuѹTlywVTlzwMuѻTl{wVTl|wMuѽTl}wVTl~wMuѿTlwVTlwMuҁTlwVTlwMuҋTlwVTlwMuҍTlwVTlwMuҏTlwVTlwMuґTlwVTlwMuғTlwVTlwMuҕTlwVTlwMuҗTlwVTlwMuҙTlwVTlwMuқTlwVTlwMuҝTlwVLdTlwMuҟTlwVTlwMuҡTlwVTlwMuңTlwVTlwMuҥTlwVTlwMuҧTlwVTlwMuҩTlwVTlwMuҫTlwVTlwMuҭTlwVTlwMuүTlwVTlwMuұTlwVTlwMuҳTlwVTlwMuҵTlwVTlwMuҷTlwVTlwMuҹTlwVTlwMuһTlwVTlwMuҽTlwVTlwMuҿTlwVTlwXTlwMuӂTlwVTlwMuӄTlwVTlwMuӆTlwVTlwMuӈTlwVTlwMuӊTlwVTlwMuӌTlwVTlwMuӎTlwVTlwMuӑTlwVTlwMuӓTlwVTlwMuӕTlwVTlwMuӗTlwVTlwMuәTlwVTlwMuӛTlwVTlwMuӝTlwVTlwMuӟTlwVTlwMuӡTlwVTlwMuӣTlwVTlwMuӥTlwVTlwMuӧTlwVTlwMuөTlwVTlwMuӫTlwVTlwMuӭTlwVTlwMuӯTlwVTlwMuӱTlwVTlwMuӳTlwVTlwMuӵTlwVTlwMuӷTlwVTlwMuӹTlwVTlwMuӻTlwVTlwMuӽTlwVTlwMuӿTlwVTlwMuԁTlwVTlwMuԃLdTlwVTlwMuԅTlwVTlwMuԇTlwVTlwMuԉTlwVTl
wMuԋTlwVTlwMuԍTl
wVTlwMuԏTlwVTlwMuԑTlwVTlwMuԓTlwVTlwMuԕTlwVTlwMuԗTlwVTlwMuԙTlwVTlwMuԛTlwVTlwMuԝTlwVTlwMuԟTlwVTl wMuԡTl!wVTl"wMuԣTl#wVTl$wMuԥTl%wVTl&wMuԧTl'wVTl(wMuԩTl)wVTl*wMuԫTl+wVTl,wMuԭTl-wVTl.wMuԯTl/wVTl0wXTl1wMuաTl2wMuբTl3wMuգTl4wMuդTl5wMuեTl6wMuզTl7wMuէTl8wMuըTl9wMuթTl:wMuժTl;wMuիTl<wMuլTl=wMuխTl>wMuծTl?wMuկTl@wMuհTlAwMuձTlBwMuղTlCwMuճTlDwMuմTlEwMuյTlFwMuնTlGwMuշTlHwMuոTlIwMuչTlJwMuպTlKwMuջTlLwMuռTlMwMuսTlNwMuվTlOwMuտTlPwMuրTlQwMuցTlRwMuւTlSwMuփTlTwMuքTlUwMuօTlVwMuֆTlWwXTlYwVTlwMuեւTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVLdTluwMuاٴTlvwMuوٴTlwwMuۇٴTlxwMuيٴTlywVTlwXTlwVTlwXTlwVTlKwXTlMwVTlwXTlwVTlwXTlwVTl.wXTl0wVTl?wXTl@wVTl\wXTl^wVTl_wXTl`wVTlkwXTlpwVTlwXTlwVTlwXTlwVTlXwMuक़TlYwMuख़TlZwMuग़Tl[wMuज़Tl\wMuड़Tl]wMuढ़Tl^wMuफ़Tl_wMuय़Tl`wVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwMuড়TlwMuঢ়TlwXTlwMuয়TlwVTlwXTlwVTlwXTl
wVTl
wXTl
wVTl
wXTl
wVTl
wXTl
wVTl)
wXTl*
wVTl1
wXTl2
wVTl3
wMuਲ਼Tl4
wXTl5
wVTl6
wMuਸ਼Tl7
wXTl8
wVTl:
wXTl<
wVTl=
wXTl>
wVTlC
wXTlG
wVTlI
wXTlK
wVTlN
wXTlQ
wVTlR
wXTlY
wMuਖ਼TlZ
wMuਗ਼Tl[
wMuਜ਼Tl\
wVTl]
wXLdTl^
wMuਫ਼Tl_
wXTlf
wVTlw
wXTl
wVTl
wXTl
wVTl
wXTl
wVTl
wXTl
wVTl
wXTl
wVTl
wXTl
wVTl
wXTl
wVTl
wXTl
wVTl
wXTl
wVTl
wXTl
wVTl
wXTl
wVTl
wXTl
wVTl
wXTl
wVTl
wXTl
wVTlwXTlwVTlwXTlwVTl
wXTlwVTlwXTlwVTl)wXTl*wVTl1wXTl2wVTl4wXTl5wVTl:wXTl<wVTlEwXTlGwVTlIwXTlKwVTlNwXTlUwVTlXwXTl\wMuଡ଼Tl]wMuଢ଼Tl^wXTl_wVTldwXTlfwVTlxwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTl
wXTlwVTlwXTlwVTl)wXTl*wVLdTl:wXTl<wVTlEwXTlFwVTlIwXTlJwVTlNwXTlUwVTlWwXTlXwVTl[wXTl]wVTl^wXTl`wVTldwXTlfwVTlpwXTlwwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTl
wVTl

wXTl
wVTl
wXTl
wVTlE
wXTlF
wVTlI
wXTlJ
wVTlP
wXTlT
wVTld
wXTlf
wVTl
wXTl
wVTl
wXTl
wVTl
wXTl
wVTl
wXTl
wVTl
wXTl
wVTl
wXTl
wVTl
wXTl
wVTl
wXTl
wVTl
wXTl
wVTl
wXTl
wVTl
wXTl
wVTl
wXTl
wVTl
wXTlwVTl3wMuําTl4wVTl;wXTl?wVTl\wXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwMuໍາTlwVLdTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwMuຫນTlwMuຫມTlwVTlwXTlwVTlwMu་Tl
wVTlCwMuགྷTlDwVTlHwXTlIwVTlMwMuཌྷTlNwVTlRwMuདྷTlSwVTlWwMuབྷTlXwVTl\wMuཛྷTl]wVTliwMuཀྵTljwVTlmwXTlqwVTlswMuཱིTltwVTluwMuཱུTlvwMuྲྀTlwwMuྲཱྀTlxwMuླྀTlywMuླཱྀTlzwVTlwMuཱྀTlwVTlwMuྒྷTlwVTlwXTlwVTlwMuྜྷTlwVTlwMuྡྷTlwVTlwMuྦྷTlwVTlwMuྫྷTlwVTlwMuྐྵTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwMuⴧTlwXTlwMuⴭTlwXTlwVTlwMuნTlwVTl_wXTlawVTlIwXTlJwVTlNwXTlPwVTlWwXTlXwVTlYwXTlZwVTl^wXTl`wVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVLdTlwXTlwVTl[wXTl]wVTl}wXTlwVTlwXTlwVTlwXTlwMuᏰTlwMuᏱTlwMuᏲTlwMuᏳTlwMuᏴTlwMuᏵTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTl7wXTl@wVTlTwXTl`wVTlmwXTlnwVTlqwXTlrwVTltwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwITlwXTlwITlwVTlwXTl wVTlywXTlwVTlwXTlwVTlwXTlwVTlwXTl wVTl,wXTl0wVTl<wXTl@wVTlAwXTlDwVTlnwXTlpwVTluwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTl_wXTl`wVTl}wXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlMwXTlPwVTlwXTlwVTlwXTlwVTl8wXTl;wVTlJwXTlMwVTlwMuвLdTlwMuдTlwMuоTlwMuсTlwMuтTlwMuъTlwMuѣTlwMuꙋTlwXTlwMuაTlwMuბTlwMuგTlwMuდTlwMuეTlwMuვTlwMuზTlwMuთTlwMuიTlwMuკTlwMuლTlwMuმTlwMuნTlwMuოTlwMuპTlwMuჟTlwMuრTlwMuსTlwMuტTlwMuუTlwMuფTlwMuქTlwMuღTlwMuყTlwMuშTlwMuჩTlwMuცTlwMuძTlwMuწTlwMuჭTlwMuხTlwMuჯTlwMuჰTlwMuჱTlwMuჲTlwMuჳTlwMuჴTlwMuჵTlwMuჶTlwMuჷTlwMuჸTlwMuჹTlwMuჺTlwXTlwMuჽTlwMuჾTlwMuჿTlwVTlwXTlwVTlwXTlwVTl,wMwaTl-wMuæTl.wMwbTl/wVTl0wMwdTl1wMweTl2wMuǝTl3wMwgTl4wMwhTl5wMwiTl6wMwjTl7wMwkTl8wMwlTl9wMwmTl:wMwnTl;wVTl<wMwoTl=wMuȣTl>wMwpTl?wMwrTl@wMwtTlAwMwuTlBwMwwTlCwMwaTlDwMuɐTlEwMuɑTlFwMuᴂTlGwMwbTlHwMwdTlIwMweTlJwMuəTlKwMuɛTlLwMuɜTlMwMwgTlNwVTlOwMwkTlPwMwmTlQwMuŋTlRwMwoTlSwMuɔLdTlTwMuᴖTlUwMuᴗTlVwMwpTlWwMwtTlXwMwuTlYwMuᴝTlZwMuɯTl[wMwvTl\wMuᴥTl]wMuβTl^wMuγTl_wMuδTl`wMuφTlawMuχTlbwMwiTlcwMwrTldwMwuTlewMwvTlfwMuβTlgwMuγTlhwMuρTliwMuφTljwMuχTlkwVTlxwMuнTlywVTlwMuɒTlwMwcTlwMuɕTlwMuðTlwMuɜTlwMwfTlwMuɟTlwMuɡTlwMuɥTlwMuɨTlwMuɩTlwMuɪTlwMuᵻTlwMuʝTlwMuɭTlwMuᶅTlwMuʟTlwMuɱTlwMuɰTlwMuɲTlwMuɳTlwMuɴTlwMuɵTlwMuɸTlwMuʂTlwMuʃTlwMuƫTlwMuʉTlwMuʊTlwMuᴜTlwMuʋTlwMuʌTlwMwzTlwMuʐTlwMuʑTlwMuʒTlwMuθTlwVTlwMuḁTlwVTlwMuḃTlwVTlwMuḅTlwVTlwMuḇTlwVTlwMuḉTlwVTl
wMuḋTlwVTlwMuḍTl
wVTlwMuḏTlwVTlwMuḑTlwVTlwMuḓTlwVTlwMuḕTlwVTlwMuḗTlwVTlwMuḙTlwVTlwMuḛTlwVTlwMuḝTlwVTlwMuḟTlwVTl wMuḡTl!wVTl"wMuḣTl#wVLdTl$wMuḥTl%wVTl&wMuḧTl'wVTl(wMuḩTl)wVTl*wMuḫTl+wVTl,wMuḭTl-wVTl.wMuḯTl/wVTl0wMuḱTl1wVTl2wMuḳTl3wVTl4wMuḵTl5wVTl6wMuḷTl7wVTl8wMuḹTl9wVTl:wMuḻTl;wVTl<wMuḽTl=wVTl>wMuḿTl?wVTl@wMuṁTlAwVTlBwMuṃTlCwVTlDwMuṅTlEwVTlFwMuṇTlGwVTlHwMuṉTlIwVTlJwMuṋTlKwVTlLwMuṍTlMwVTlNwMuṏTlOwVTlPwMuṑTlQwVTlRwMuṓTlSwVTlTwMuṕTlUwVTlVwMuṗTlWwVTlXwMuṙTlYwVTlZwMuṛTl[wVTl\wMuṝTl]wVTl^wMuṟTl_wVTl`wMuṡTlawVTlbwMuṣTlcwVTldwMuṥTlewVTlfwMuṧTlgwVTlhwMuṩTliwVTljwMuṫTlkwVTllwMuṭTlmwVTlnwMuṯTlowVTlpwMuṱTlqwVTlrwMuṳTlswVTltwMuṵTluwVTlvwMuṷTlwwVTlxwMuṹTlywVTlzwMuṻTl{wVTl|wMuṽTl}wVTl~wMuṿTlwVTlwMuẁTlwVTlwMuẃTlwVTlwMuẅTlwVTlwMuẇTlwVLdTlwMuẉTlwVTlwMuẋTlwVTlwMuẍTlwVTlwMuẏTlwVTlwMuẑTlwVTlwMuẓTlwVTlwMuẕTlwVTlwMuaʾTlwMuṡTlwVTlwMußTlwVTlwMuạTlwVTlwMuảTlwVTlwMuấTlwVTlwMuầTlwVTlwMuẩTlwVTlwMuẫTlwVTlwMuậTlwVTlwMuắTlwVTlwMuằTlwVTlwMuẳTlwVTlwMuẵTlwVTlwMuặTlwVTlwMuẹTlwVTlwMuẻTlwVTlwMuẽTlwVTlwMuếTlwVTlwMuềTlwVTlwMuểTlwVTlwMuễTlwVTlwMuệTlwVTlwMuỉTlwVTlwMuịTlwVTlwMuọTlwVTlwMuỏTlwVTlwMuốTlwVTlwMuồTlwVTlwMuổTlwVTlwMuỗTlwVTlwMuộTlwVTlwMuớTlwVTlwMuờTlwVTlwMuởTlwVTlwMuỡTlwVTlwMuợTlwVTlwMuụTlwVTlwMuủTlwVTlwMuứTlwVTlwMuừTlwVTlwMuửTlwVTlwMuữTlwVTlwMuựLdTlwVTlwMuỳTlwVTlwMuỵTlwVTlwMuỷTlwVTlwMuỹTlwVTlwMuỻTlwVTlwMuỽTlwVTlwMuỿTlwVTlwMuἀTlwMuἁTl
wMuἂTlwMuἃTlwMuἄTl
wMuἅTlwMuἆTlwMuἇTlwVTlwXTlwMuἐTlwMuἑTlwMuἒTlwMuἓTlwMuἔTlwMuἕTlwXTl wVTl(wMuἠTl)wMuἡTl*wMuἢTl+wMuἣTl,wMuἤTl-wMuἥTl.wMuἦTl/wMuἧTl0wVTl8wMuἰTl9wMuἱTl:wMuἲTl;wMuἳTl<wMuἴTl=wMuἵTl>wMuἶTl?wMuἷTl@wVTlFwXTlHwMuὀTlIwMuὁTlJwMuὂTlKwMuὃTlLwMuὄTlMwMuὅTlNwXTlPwVTlXwXTlYwMuὑTlZwXTl[wMuὓTl\wXTl]wMuὕTl^wXTl_wMuὗTl`wVTlhwMuὠTliwMuὡTljwMuὢTlkwMuὣTllwMuὤTlmwMuὥTlnwMuὦTlowMuὧTlpwVTlqwMuάTlrwVTlswMuέTltwVTluwMuήTlvwVTlwwMuίTlxwVTlywMuόTlzwVTl{wMuύTl|wVTl}wMuώTl~wXTlwMuἀιTlwMuἁιTlwMuἂιTlwMuἃιTlwMuἄιTlwMuἅιTlwMuἆιTlwMuἇιLdTlwMuἀιTlwMuἁιTlwMuἂιTlwMuἃιTlwMuἄιTlwMuἅιTlwMuἆιTlwMuἇιTlwMuἠιTlwMuἡιTlwMuἢιTlwMuἣιTlwMuἤιTlwMuἥιTlwMuἦιTlwMuἧιTlwMuἠιTlwMuἡιTlwMuἢιTlwMuἣιTlwMuἤιTlwMuἥιTlwMuἦιTlwMuἧιTlwMuὠιTlwMuὡιTlwMuὢιTlwMuὣιTlwMuὤιTlwMuὥιTlwMuὦιTlwMuὧιTlwMuὠιTlwMuὡιTlwMuὢιTlwMuὣιTlwMuὤιTlwMuὥιTlwMuὦιTlwMuὧιTlwVTlwMuὰιTlwMuαιTlwMuάιTlwXTlwVTlwMuᾶιTlwMuᾰTlwMuᾱTlwMuὰTlwMuάTlwMuαιTlw3u ̓TlwMuιTlw3u ̓Tlw3u ͂Tlw3u ̈͂TlwMuὴιTlwMuηιTlwMuήιTlwXTlwVTlwMuῆιTlwMuὲTlwMuέTlwMuὴTlwMuήTlwMuηιTlw3u ̓̀Tlw3u ̓́Tlw3u ̓͂TlwVTlwMuΐTlwXTlwVTlwMuῐTlwMuῑTlwMuὶTlwMuίTlwXTlw3u ̔̀Tlw3u ̔́Tlw3u ̔͂TlwVTlwMuΰTlwVTlwMuῠTlwMuῡTlwMuὺTlwMuύTlwMuῥTlw3u ̈̀Tlw3u ̈́Tlw3w`TlwXTlwMuὼιTlwMuωιTlwMuώιTlwXTlwVLdTlwMuῶιTlwMuὸTlwMuόTlwMuὼTlwMuώTlwMuωιTlw3u ́Tlw3u ̔TlwXTl w3w Tl wITl wDuTl wXTl wVTl wMu‐Tl wVTl w3u ̳Tl wVTl$ wXTl' wVTl( wXTl/ w3w Tl0 wVTl3 wMu′′Tl4 wMu′′′Tl5 wVTl6 wMu‵‵Tl7 wMu‵‵‵Tl8 wVTl< w3u!!Tl= wVTl> w3u ̅Tl? wVTlG w3u??TlH w3u?!TlI w3u!?TlJ wVTlW wMu′′′′TlX wVTl_ w3w Tl` wITla wXTld wITle wXTlp wMw0Tlq wMwiTlr wXTlt wMw4Tlu wMw5Tlv wMw6Tlw wMw7Tlx wMw8Tly wMw9Tlz w3w+Tl{ wMu−Tl| w3w=Tl} w3w(Tl~ w3w)Tl wMwnTl wMw0Tl wMw1Tl wMw2Tl wMw3Tl wMw4Tl wMw5Tl wMw6Tl wMw7Tl wMw8Tl wMw9Tl w3w+Tl wMu−Tl w3w=Tl w3w(Tl w3w)Tl wXTl wMwaTl wMweTl wMwoTl wMwxTl wMuəTl wMwhTl wMwkTl wMwlTl wMwmTl wMwnTl wMwpTl wMwsTl wMwtTl wXTl wVTl wMarsTl wVTl wXTl wVTl wXTl!w3ua/cTl!w3ua/sTl!wMwcTl!wMu°cTl!wVLdTl!w3uc/oTl!w3uc/uTl!wMuɛTl!wVTl!wMu°fTl
!wMwgTl!wMwhTl!wMuħTl!wMwiTl!wMwlTl!wVTl!wMwnTl!wManoTl!wVTl!wMwpTl!wMwqTl!wMwrTl!wVTl !wMasmTl!!wMatelTl"!wMatmTl#!wVTl$!wMwzTl%!wVTl&!wMuωTl'!wVTl(!wMwzTl)!wVTl*!wMwkTl+!wMuåTl,!wMwbTl-!wMwcTl.!wVTl/!wMweTl1!wMwfTl2!wXTl3!wMwmTl4!wMwoTl5!wMuאTl6!wMuבTl7!wMuגTl8!wMuדTl9!wMwiTl:!wVTl;!wMafaxTl<!wMuπTl=!wMuγTl?!wMuπTl@!wMu∑TlA!wVTlE!wMwdTlG!wMweTlH!wMwiTlI!wMwjTlJ!wVTlP!wMu1⁄7TlQ!wMu1⁄9TlR!wMu1⁄10TlS!wMu1⁄3TlT!wMu2⁄3TlU!wMu1⁄5TlV!wMu2⁄5TlW!wMu3⁄5TlX!wMu4⁄5TlY!wMu1⁄6TlZ!wMu5⁄6Tl[!wMu1⁄8Tl\!wMu3⁄8Tl]!wMu5⁄8Tl^!wMu7⁄8Tl_!wMu1⁄Tl`!wMwiTla!wMaiiTlb!wMaiiiTlc!wMaivTld!wMwvTle!wMaviTlf!wMaviiTlg!wMaviiiTlh!wMaixTli!wMwxTlj!wMaxiTlk!wMaxiiTll!wMwlTlm!wMwcTln!wMwdTlo!wMwmTlp!wMwiTlq!wMaiiTlr!wMaiiiTls!wMaivTlt!wMwvTlu!wMaviTlv!wMaviiTlw!wMaviiiTlx!wMaixTly!wMwxTlz!wMaxiTl{!wMaxiiTl|!wMwlLdTl}!wMwcTl~!wMwdTl!wMwmTl!wVTl!wXTl!wVTl!wMu0⁄3Tl!wVTl!wXTl!wVTl,"wMu∫∫Tl-"wMu∫∫∫Tl."wVTl/"wMu∮∮Tl0"wMu∮∮∮Tl1"wVTl)#wMu〈Tl*#wMu〉Tl+#wVTl'$wXTl@$wVTlK$wXTl`$wMw1Tla$wMw2Tlb$wMw3Tlc$wMw4Tld$wMw5Tle$wMw6Tlf$wMw7Tlg$wMw8Tlh$wMw9Tli$wMu10Tlj$wMu11Tlk$wMu12Tll$wMu13Tlm$wMu14Tln$wMu15Tlo$wMu16Tlp$wMu17Tlq$wMu18Tlr$wMu19Tls$wMu20Tlt$w3u(1)Tlu$w3u(2)Tlv$w3u(3)Tlw$w3u(4)Tlx$w3u(5)Tly$w3u(6)Tlz$w3u(7)Tl{$w3u(8)Tl|$w3u(9)Tl}$w3u(10)Tl~$w3u(11)Tl$w3u(12)Tl$w3u(13)Tl$w3u(14)Tl$w3u(15)Tl$w3u(16)Tl$w3u(17)Tl$w3u(18)Tl$w3u(19)Tl$w3u(20)Tl$wXTl$w3u(a)Tl$w3u(b)Tl$w3u(c)Tl$w3u(d)Tl$w3u(e)Tl$w3u(f)Tl$w3u(g)Tl$w3u(h)Tl$w3u(i)Tl$w3u(j)Tl$w3u(k)Tl$w3u(l)Tl$w3u(m)Tl$w3u(n)Tl$w3u(o)Tl$w3u(p)Tl$w3u(q)Tl$w3u(r)Tl$w3u(s)Tl$w3u(t)Tl$w3u(u)Tl$w3u(v)Tl$w3u(w)Tl$w3u(x)Tl$w3u(y)Tl$w3u(z)Tl$wMwaTl$wMwbTl$wMwcTl$wMwdTl$wMweTl$wMwfTl$wMwgTl$wMwhTl$wMwiTl$wMwjTl$wMwkLdTl$wMwlTl$wMwmTl$wMwnTl$wMwoTl$wMwpTl$wMwqTl$wMwrTl$wMwsTl$wMwtTl$wMwuTl$wMwvTl$wMwwTl$wMwxTl$wMwyTl$wMwzTl$wMwaTl$wMwbTl$wMwcTl$wMwdTl$wMweTl$wMwfTl$wMwgTl$wMwhTl$wMwiTl$wMwjTl$wMwkTl$wMwlTl$wMwmTl$wMwnTl$wMwoTl$wMwpTl$wMwqTl$wMwrTl$wMwsTl$wMwtTl$wMwuTl$wMwvTl$wMwwTl$wMwxTl$wMwyTl$wMwzTl$wMw0Tl$wVTl*wMu∫∫∫∫Tl
*wVTlt*w3u::=Tlu*w3u==Tlv*w3u===Tlw*wVTl*wMu⫝̸Tl*wVTlt+wXTlv+wVTl+wXTl+wVTl,wMuⰰTl,wMuⰱTl,wMuⰲTl,wMuⰳTl,wMuⰴTl,wMuⰵTl,wMuⰶTl,wMuⰷTl,wMuⰸTl,wMuⰹTl
,wMuⰺTl,wMuⰻTl,wMuⰼTl
,wMuⰽTl,wMuⰾTl,wMuⰿTl,wMuⱀTl,wMuⱁTl,wMuⱂTl,wMuⱃTl,wMuⱄTl,wMuⱅTl,wMuⱆTl,wMuⱇTl,wMuⱈTl,wMuⱉTl,wMuⱊTl,wMuⱋTl,wMuⱌTl,wMuⱍTl,wMuⱎTl,wMuⱏTl ,wMuⱐTl!,wMuⱑTl",wMuⱒTl#,wMuⱓTl$,wMuⱔTl%,wMuⱕTl&,wMuⱖTl',wMuⱗTl(,wMuⱘTl),wMuⱙTl*,wMuⱚTl+,wMuⱛTl,,wMuⱜLdTl-,wMuⱝTl.,wMuⱞTl/,wMuⱟTl0,wVTl`,wMuⱡTla,wVTlb,wMuɫTlc,wMuᵽTld,wMuɽTle,wVTlg,wMuⱨTlh,wVTli,wMuⱪTlj,wVTlk,wMuⱬTll,wVTlm,wMuɑTln,wMuɱTlo,wMuɐTlp,wMuɒTlq,wVTlr,wMuⱳTls,wVTlu,wMuⱶTlv,wVTl|,wMwjTl},wMwvTl~,wMuȿTl,wMuɀTl,wMuⲁTl,wVTl,wMuⲃTl,wVTl,wMuⲅTl,wVTl,wMuⲇTl,wVTl,wMuⲉTl,wVTl,wMuⲋTl,wVTl,wMuⲍTl,wVTl,wMuⲏTl,wVTl,wMuⲑTl,wVTl,wMuⲓTl,wVTl,wMuⲕTl,wVTl,wMuⲗTl,wVTl,wMuⲙTl,wVTl,wMuⲛTl,wVTl,wMuⲝTl,wVTl,wMuⲟTl,wVTl,wMuⲡTl,wVTl,wMuⲣTl,wVTl,wMuⲥTl,wVTl,wMuⲧTl,wVTl,wMuⲩTl,wVTl,wMuⲫTl,wVTl,wMuⲭTl,wVTl,wMuⲯTl,wVTl,wMuⲱTl,wVTl,wMuⲳTl,wVTl,wMuⲵTl,wVTl,wMuⲷTl,wVTl,wMuⲹTl,wVTl,wMuⲻTl,wVTl,wMuⲽTl,wVTl,wMuⲿTl,wVTl,wMuⳁTl,wVTl,wMuⳃTl,wVTl,wMuⳅTl,wVTl,wMuⳇLdTl,wVTl,wMuⳉTl,wVTl,wMuⳋTl,wVTl,wMuⳍTl,wVTl,wMuⳏTl,wVTl,wMuⳑTl,wVTl,wMuⳓTl,wVTl,wMuⳕTl,wVTl,wMuⳗTl,wVTl,wMuⳙTl,wVTl,wMuⳛTl,wVTl,wMuⳝTl,wVTl,wMuⳟTl,wVTl,wMuⳡTl,wVTl,wMuⳣTl,wVTl,wMuⳬTl,wVTl,wMuⳮTl,wVTl,wMuⳳTl,wVTl,wXTl,wVTl&-wXTl'-wVTl(-wXTl--wVTl.-wXTl0-wVTlh-wXTlo-wMuⵡTlp-wVTlq-wXTl-wVTl-wXTl-wVTl-wXTl-wVTl-wXTl-wVTl-wXTl-wVTl-wXTl-wVTl-wXTl-wVTl-wXTl-wVTl-wXTl-wVTl-wXTl-wVTl^.wXTl.wVTl.wXTl.wVTl.wMu母Tl.wVTl.wMu龟Tl.wXTl/wMu一Tl/wMu丨Tl/wMu丶Tl/wMu丿Tl/wMu乙Tl/wMu亅Tl/wMu二Tl/wMu亠Tl/wMu人Tl/wMu儿Tl
/wMu入Tl/wMu八Tl/wMu冂Tl
/wMu冖Tl/wMu冫Tl/wMu几Tl/wMu凵Tl/wMu刀Tl/wMu力Tl/wMu勹Tl/wMu匕Tl/wMu匚Tl/wMu匸Tl/wMu十Tl/wMu卜Tl/wMu卩LdTl/wMu厂Tl/wMu厶Tl/wMu又Tl/wMu口Tl/wMu囗Tl/wMu土Tl /wMu士Tl!/wMu夂Tl"/wMu夊Tl#/wMu夕Tl$/wMu大Tl%/wMu女Tl&/wMu子Tl'/wMu宀Tl(/wMu寸Tl)/wMu小Tl*/wMu尢Tl+/wMu尸Tl,/wMu屮Tl-/wMu山Tl./wMu巛Tl//wMu工Tl0/wMu己Tl1/wMu巾Tl2/wMu干Tl3/wMu幺Tl4/wMu广Tl5/wMu廴Tl6/wMu廾Tl7/wMu弋Tl8/wMu弓Tl9/wMu彐Tl:/wMu彡Tl;/wMu彳Tl</wMu心Tl=/wMu戈Tl>/wMu戶Tl?/wMu手Tl@/wMu支TlA/wMu攴TlB/wMu文TlC/wMu斗TlD/wMu斤TlE/wMu方TlF/wMu无TlG/wMu日TlH/wMu曰TlI/wMu月TlJ/wMu木TlK/wMu欠TlL/wMu止TlM/wMu歹TlN/wMu殳TlO/wMu毋TlP/wMu比TlQ/wMu毛TlR/wMu氏TlS/wMu气TlT/wMu水TlU/wMu火TlV/wMu爪TlW/wMu父TlX/wMu爻TlY/wMu爿TlZ/wMu片Tl[/wMu牙Tl\/wMu牛Tl]/wMu犬Tl^/wMu玄Tl_/wMu玉Tl`/wMu瓜Tla/wMu瓦Tlb/wMu甘Tlc/wMu生Tld/wMu用Tle/wMu田Tlf/wMu疋Tlg/wMu疒Tlh/wMu癶Tli/wMu白Tlj/wMu皮Tlk/wMu皿Tll/wMu目Tlm/wMu矛Tln/wMu矢Tlo/wMu石Tlp/wMu示Tlq/wMu禸Tlr/wMu禾Tls/wMu穴Tlt/wMu立Tlu/wMu竹Tlv/wMu米Tlw/wMu糸Tlx/wMu缶Tly/wMu网Tlz/wMu羊Tl{/wMu羽Tl|/wMu老Tl}/wMu而LdTl~/wMu耒Tl/wMu耳Tl/wMu聿Tl/wMu肉Tl/wMu臣Tl/wMu自Tl/wMu至Tl/wMu臼Tl/wMu舌Tl/wMu舛Tl/wMu舟Tl/wMu艮Tl/wMu色Tl/wMu艸Tl/wMu虍Tl/wMu虫Tl/wMu血Tl/wMu行Tl/wMu衣Tl/wMu襾Tl/wMu見Tl/wMu角Tl/wMu言Tl/wMu谷Tl/wMu豆Tl/wMu豕Tl/wMu豸Tl/wMu貝Tl/wMu赤Tl/wMu走Tl/wMu足Tl/wMu身Tl/wMu車Tl/wMu辛Tl/wMu辰Tl/wMu辵Tl/wMu邑Tl/wMu酉Tl/wMu釆Tl/wMu里Tl/wMu金Tl/wMu長Tl/wMu門Tl/wMu阜Tl/wMu隶Tl/wMu隹Tl/wMu雨Tl/wMu靑Tl/wMu非Tl/wMu面Tl/wMu革Tl/wMu韋Tl/wMu韭Tl/wMu音Tl/wMu頁Tl/wMu風Tl/wMu飛Tl/wMu食Tl/wMu首Tl/wMu香Tl/wMu馬Tl/wMu骨Tl/wMu高Tl/wMu髟Tl/wMu鬥Tl/wMu鬯Tl/wMu鬲Tl/wMu鬼Tl/wMu魚Tl/wMu鳥Tl/wMu鹵Tl/wMu鹿Tl/wMu麥Tl/wMu麻Tl/wMu黃Tl/wMu黍Tl/wMu黑Tl/wMu黹Tl/wMu黽Tl/wMu鼎Tl/wMu鼓Tl/wMu鼠Tl/wMu鼻Tl/wMu齊Tl/wMu齒Tl/wMu龍Tl/wMu龜Tl/wMu龠Tl/wXTl0w3w Tl0wVTl0wMw.Tl0wVTl60wMu〒Tl70wVTl80wMu十Tl90wMu卄Tl:0wMu卅Tl;0wVTl@0wXLdTlA0wVTl0wXTl0wVTl0w3u ゙Tl0w3u ゚Tl0wVTl0wMuよりTl0wVTl0wMuコトTl1wXTl1wVTl01wXTl11wMuᄀTl21wMuᄁTl31wMuᆪTl41wMuᄂTl51wMuᆬTl61wMuᆭTl71wMuᄃTl81wMuᄄTl91wMuᄅTl:1wMuᆰTl;1wMuᆱTl<1wMuᆲTl=1wMuᆳTl>1wMuᆴTl?1wMuᆵTl@1wMuᄚTlA1wMuᄆTlB1wMuᄇTlC1wMuᄈTlD1wMuᄡTlE1wMuᄉTlF1wMuᄊTlG1wMuᄋTlH1wMuᄌTlI1wMuᄍTlJ1wMuᄎTlK1wMuᄏTlL1wMuᄐTlM1wMuᄑTlN1wMuᄒTlO1wMuᅡTlP1wMuᅢTlQ1wMuᅣTlR1wMuᅤTlS1wMuᅥTlT1wMuᅦTlU1wMuᅧTlV1wMuᅨTlW1wMuᅩTlX1wMuᅪTlY1wMuᅫTlZ1wMuᅬTl[1wMuᅭTl\1wMuᅮTl]1wMuᅯTl^1wMuᅰTl_1wMuᅱTl`1wMuᅲTla1wMuᅳTlb1wMuᅴTlc1wMuᅵTld1wXTle1wMuᄔTlf1wMuᄕTlg1wMuᇇTlh1wMuᇈTli1wMuᇌTlj1wMuᇎTlk1wMuᇓTll1wMuᇗTlm1wMuᇙTln1wMuᄜTlo1wMuᇝTlp1wMuᇟTlq1wMuᄝTlr1wMuᄞTls1wMuᄠTlt1wMuᄢTlu1wMuᄣTlv1wMuᄧTlw1wMuᄩTlx1wMuᄫTly1wMuᄬTlz1wMuᄭTl{1wMuᄮTl|1wMuᄯTl}1wMuᄲTl~1wMuᄶTl1wMuᅀTl1wMuᅇTl1wMuᅌTl1wMuᇱTl1wMuᇲTl1wMuᅗTl1wMuᅘTl1wMuᅙTl1wMuᆄTl1wMuᆅLdTl1wMuᆈTl1wMuᆑTl1wMuᆒTl1wMuᆔTl1wMuᆞTl1wMuᆡTl1wXTl1wVTl1wMu一Tl1wMu二Tl1wMu三Tl1wMu四Tl1wMu上Tl1wMu中Tl1wMu下Tl1wMu甲Tl1wMu乙Tl1wMu丙Tl1wMu丁Tl1wMu天Tl1wMu地Tl1wMu人Tl1wVTl1wXTl1wVTl2w3u(ᄀ)Tl2w3u(ᄂ)Tl2w3u(ᄃ)Tl2w3u(ᄅ)Tl2w3u(ᄆ)Tl2w3u(ᄇ)Tl2w3u(ᄉ)Tl2w3u(ᄋ)Tl2w3u(ᄌ)Tl2w3u(ᄎ)Tl
2w3u(ᄏ)Tl2w3u(ᄐ)Tl2w3u(ᄑ)Tl
2w3u(ᄒ)Tl2w3u(가)Tl2w3u(나)Tl2w3u(다)Tl2w3u(라)Tl2w3u(마)Tl2w3u(바)Tl2w3u(사)Tl2w3u(아)Tl2w3u(자)Tl2w3u(차)Tl2w3u(카)Tl2w3u(타)Tl2w3u(파)Tl2w3u(하)Tl2w3u(주)Tl2w3u(오전)Tl2w3u(오후)Tl2wXTl 2w3u(一)Tl!2w3u(二)Tl"2w3u(三)Tl#2w3u(四)Tl$2w3u(五)Tl%2w3u(六)Tl&2w3u(七)Tl'2w3u(八)Tl(2w3u(九)Tl)2w3u(十)Tl*2w3u(月)Tl+2w3u(火)Tl,2w3u(水)Tl-2w3u(木)Tl.2w3u(金)Tl/2w3u(土)Tl02w3u(日)Tl12w3u(株)Tl22w3u(有)Tl32w3u(社)Tl42w3u(名)Tl52w3u(特)Tl62w3u(財)Tl72w3u(祝)Tl82w3u(労)Tl92w3u(代)Tl:2w3u(呼)Tl;2w3u(学)Tl<2w3u(監)Tl=2w3u(企)Tl>2w3u(資)Tl?2w3u(協)Tl@2w3u(祭)TlA2w3u(休)TlB2w3u(自)TlC2w3u(至)TlD2wMu問TlE2wMu幼TlF2wMu文TlG2wMu箏TlH2wVTlP2wMapteTlQ2wMu21LdTlR2wMu22TlS2wMu23TlT2wMu24TlU2wMu25TlV2wMu26TlW2wMu27TlX2wMu28TlY2wMu29TlZ2wMu30Tl[2wMu31Tl\2wMu32Tl]2wMu33Tl^2wMu34Tl_2wMu35Tl`2wMuᄀTla2wMuᄂTlb2wMuᄃTlc2wMuᄅTld2wMuᄆTle2wMuᄇTlf2wMuᄉTlg2wMuᄋTlh2wMuᄌTli2wMuᄎTlj2wMuᄏTlk2wMuᄐTll2wMuᄑTlm2wMuᄒTln2wMu가Tlo2wMu나Tlp2wMu다Tlq2wMu라Tlr2wMu마Tls2wMu바Tlt2wMu사Tlu2wMu아Tlv2wMu자Tlw2wMu차Tlx2wMu카Tly2wMu타Tlz2wMu파Tl{2wMu하Tl|2wMu참고Tl}2wMu주의Tl~2wMu우Tl2wVTl2wMu一Tl2wMu二Tl2wMu三Tl2wMu四Tl2wMu五Tl2wMu六Tl2wMu七Tl2wMu八Tl2wMu九Tl2wMu十Tl2wMu月Tl2wMu火Tl2wMu水Tl2wMu木Tl2wMu金Tl2wMu土Tl2wMu日Tl2wMu株Tl2wMu有Tl2wMu社Tl2wMu名Tl2wMu特Tl2wMu財Tl2wMu祝Tl2wMu労Tl2wMu秘Tl2wMu男Tl2wMu女Tl2wMu適Tl2wMu優Tl2wMu印Tl2wMu注Tl2wMu項Tl2wMu休Tl2wMu写Tl2wMu正Tl2wMu上Tl2wMu中Tl2wMu下Tl2wMu左Tl2wMu右Tl2wMu医Tl2wMu宗Tl2wMu学Tl2wMu監Tl2wMu企Tl2wMu資Tl2wMu協Tl2wMu夜Tl2wMu36Tl2wMu37Tl2wMu38Tl2wMu39Tl2wMu40LdTl2wMu41Tl2wMu42Tl2wMu43Tl2wMu44Tl2wMu45Tl2wMu46Tl2wMu47Tl2wMu48Tl2wMu49Tl2wMu50Tl2wMu1月Tl2wMu2月Tl2wMu3月Tl2wMu4月Tl2wMu5月Tl2wMu6月Tl2wMu7月Tl2wMu8月Tl2wMu9月Tl2wMu10月Tl2wMu11月Tl2wMu12月Tl2wMahgTl2wMaergTl2wMaevTl2wMaltdTl2wMuアTl2wMuイTl2wMuウTl2wMuエTl2wMuオTl2wMuカTl2wMuキTl2wMuクTl2wMuケTl2wMuコTl2wMuサTl2wMuシTl2wMuスTl2wMuセTl2wMuソTl2wMuタTl2wMuチTl2wMuツTl2wMuテTl2wMuトTl2wMuナTl2wMuニTl2wMuヌTl2wMuネTl2wMuノTl2wMuハTl2wMuヒTl2wMuフTl2wMuヘTl2wMuホTl2wMuマTl2wMuミTl2wMuムTl2wMuメTl2wMuモTl2wMuヤTl2wMuユTl2wMuヨTl2wMuラTl2wMuリTl2wMuルTl2wMuレTl2wMuロTl2wMuワTl2wMuヰTl2wMuヱTl2wMuヲTl2wMu令和Tl3wMuアパートTl3wMuアルファTl3wMuアンペアTl3wMuアールTl3wMuイニングTl3wMuインチTl3wMuウォンTl3wMuエスクードTl3wMuエーカーTl3wMuオンスTl
3wMuオームTl3wMuカイリTl3wMuカラットTl
3wMuカロリーTl3wMuガロンTl3wMuガンマTl3wMuギガTl3wMuギニーTl3wMuキュリーTl3wMuギルダーTl3wMuキロTl3wMuキログラムTl3wMuキロメートルTl3wMuキロワットTl3wMuグラムTl3wMuグラムトンLdTl3wMuクルゼイロTl3wMuクローネTl3wMuケースTl3wMuコルナTl3wMuコーポTl3wMuサイクルTl 3wMuサンチームTl!3wMuシリングTl"3wMuセンチTl#3wMuセントTl$3wMuダースTl%3wMuデシTl&3wMuドルTl'3wMuトンTl(3wMuナノTl)3wMuノットTl*3wMuハイツTl+3wMuパーセントTl,3wMuパーツTl-3wMuバーレルTl.3wMuピアストルTl/3wMuピクルTl03wMuピコTl13wMuビルTl23wMuファラッドTl33wMuフィートTl43wMuブッシェルTl53wMuフランTl63wMuヘクタールTl73wMuペソTl83wMuペニヒTl93wMuヘルツTl:3wMuペンスTl;3wMuページTl<3wMuベータTl=3wMuポイントTl>3wMuボルトTl?3wMuホンTl@3wMuポンドTlA3wMuホールTlB3wMuホーンTlC3wMuマイクロTlD3wMuマイルTlE3wMuマッハTlF3wMuマルクTlG3wMuマンションTlH3wMuミクロンTlI3wMuミリTlJ3wMuミリバールTlK3wMuメガTlL3wMuメガトンTlM3wMuメートルTlN3wMuヤードTlO3wMuヤールTlP3wMuユアンTlQ3wMuリットルTlR3wMuリラTlS3wMuルピーTlT3wMuルーブルTlU3wMuレムTlV3wMuレントゲンTlW3wMuワットTlX3wMu0点TlY3wMu1点TlZ3wMu2点Tl[3wMu3点Tl\3wMu4点Tl]3wMu5点Tl^3wMu6点Tl_3wMu7点Tl`3wMu8点Tla3wMu9点Tlb3wMu10点Tlc3wMu11点Tld3wMu12点Tle3wMu13点Tlf3wMu14点Tlg3wMu15点Tlh3wMu16点Tli3wMu17点Tlj3wMu18点Tlk3wMu19点Tll3wMu20点Tlm3wMu21点Tln3wMu22点Tlo3wMu23点Tlp3wMu24点Tlq3wMahpaTlr3wMadaTls3wMaauTlt3wMabarTlu3wMaovTlv3wMapcTlw3wMadmTlx3wMadm2Tly3wMadm3Tlz3wMaiuTl{3wMu平成Tl|3wMu昭和Tl}3wMu大正LdTl~3wMu明治Tl3wMu株式会社Tl3wMapaTl3wManaTl3wMuμaTl3wMamaTl3wMakaTl3wMakbTl3wMambTl3wMagbTl3wMacalTl3wMakcalTl3wMapfTl3wManfTl3wMuμfTl3wMuμgTl3wMamgTl3wMakgTl3wMahzTl3wMakhzTl3wMamhzTl3wMaghzTl3wMathzTl3wMuμlTl3wMamlTl3wMadlTl3wMaklTl3wMafmTl3wManmTl3wMuμmTl3wMammTl3wMacmTl3wMakmTl3wMamm2Tl3wMacm2Tl3wMam2Tl3wMakm2Tl3wMamm3Tl3wMacm3Tl3wMam3Tl3wMakm3Tl3wMum∕sTl3wMum∕s2Tl3wMapaTl3wMakpaTl3wMampaTl3wMagpaTl3wMaradTl3wMurad∕sTl3wMurad∕s2Tl3wMapsTl3wMansTl3wMuμsTl3wMamsTl3wMapvTl3wManvTl3wMuμvTl3wMamvTl3wMakvTl3wMamvTl3wMapwTl3wManwTl3wMuμwTl3wMamwTl3wMakwTl3wMamwTl3wMukωTl3wMumωTl3wXTl3wMabqTl3wMaccTl3wMacdTl3wMuc∕kgTl3wXTl3wMadbTl3wMagyTl3wMahaTl3wMahpTl3wMainTl3wMakkTl3wMakmTl3wMaktTl3wMalmTl3wMalnTl3wMalogTl3wMalxTl3wMambTl3wMamilTl3wMamolTl3wMaphTl3wXTl3wMappmTl3wMaprTl3wMasrTl3wMasvTl3wMawbTl3wMuv∕mTl3wMua∕mTl3wMu1日Tl3wMu2日LdTl3wMu3日Tl3wMu4日Tl3wMu5日Tl3wMu6日Tl3wMu7日Tl3wMu8日Tl3wMu9日Tl3wMu10日Tl3wMu11日Tl3wMu12日Tl3wMu13日Tl3wMu14日Tl3wMu15日Tl3wMu16日Tl3wMu17日Tl3wMu18日Tl3wMu19日Tl3wMu20日Tl3wMu21日Tl3wMu22日Tl3wMu23日Tl3wMu24日Tl3wMu25日Tl3wMu26日Tl3wMu27日Tl3wMu28日Tl3wMu29日Tl3wMu30日Tl3wMu31日Tl3wMagalTl4wVTlwXTlwVTlǤwXTlФwVTl,wXTl@wMuꙁTlAwVTlBwMuꙃTlCwVTlDwMuꙅTlEwVTlFwMuꙇTlGwVTlHwMuꙉTlIwVTlJwMuꙋTlKwVTlLwMuꙍTlMwVTlNwMuꙏTlOwVTlPwMuꙑTlQwVTlRwMuꙓTlSwVTlTwMuꙕTlUwVTlVwMuꙗTlWwVTlXwMuꙙTlYwVTlZwMuꙛTl[wVTl\wMuꙝTl]wVTl^wMuꙟTl_wVTl`wMuꙡTlawVTlbwMuꙣTlcwVTldwMuꙥTlewVTlfwMuꙧTlgwVTlhwMuꙩTliwVTljwMuꙫTlkwVTllwMuꙭTlmwVTlwMuꚁTlwVTlwMuꚃTlwVTlwMuꚅTlwVTlwMuꚇTlwVTlwMuꚉTlwVTlwMuꚋTlwVTlwMuꚍTlwVTlwMuꚏTlwVTlwMuꚑTlwVLdTlwMuꚓTlwVTlwMuꚕTlwVTlwMuꚗTlwVTlwMuꚙTlwVTlwMuꚛTlwVTlwMuъTlwMuьTlwVTlwXTlwVTl"wMuꜣTl#wVTl$wMuꜥTl%wVTl&wMuꜧTl'wVTl(wMuꜩTl)wVTl*wMuꜫTl+wVTl,wMuꜭTl-wVTl.wMuꜯTl/wVTl2wMuꜳTl3wVTl4wMuꜵTl5wVTl6wMuꜷTl7wVTl8wMuꜹTl9wVTl:wMuꜻTl;wVTl<wMuꜽTl=wVTl>wMuꜿTl?wVTl@wMuꝁTlAwVTlBwMuꝃTlCwVTlDwMuꝅTlEwVTlFwMuꝇTlGwVTlHwMuꝉTlIwVTlJwMuꝋTlKwVTlLwMuꝍTlMwVTlNwMuꝏTlOwVTlPwMuꝑTlQwVTlRwMuꝓTlSwVTlTwMuꝕTlUwVTlVwMuꝗTlWwVTlXwMuꝙTlYwVTlZwMuꝛTl[wVTl\wMuꝝTl]wVTl^wMuꝟTl_wVTl`wMuꝡTlawVTlbwMuꝣTlcwVTldwMuꝥTlewVTlfwMuꝧTlgwVTlhwMuꝩTliwVTljwMuꝫTlkwVTllwMuꝭTlmwVTlnwMuꝯTlowVTlpwMuꝯTlqwVTlywMuꝺTlzwVTl{wMuꝼTl|wVTl}wMuᵹTl~wMuꝿTlwVLdTlwMuꞁTlwVTlwMuꞃTlwVTlwMuꞅTlwVTlwMuꞇTlwVTlwMuꞌTlwVTlwMuɥTlwVTlwMuꞑTlwVTlwMuꞓTlwVTlwMuꞗTlwVTlwMuꞙTlwVTlwMuꞛTlwVTlwMuꞝTlwVTlwMuꞟTlwVTlwMuꞡTlwVTlwMuꞣTlwVTlwMuꞥTlwVTlwMuꞧTlwVTlwMuꞩTlwVTlwMuɦTlwMuɜTlwMuɡTlwMuɬTlwMuɪTlwVTlwMuʞTlwMuʇTlwMuʝTlwMuꭓTlwMuꞵTlwVTlwMuꞷTlwVTlwMuꞹTlwVTlwMuꞻTlwVTlwMuꞽTlwVTlwMuꞿTlwVTlwMuꟁTlwVTl§wMuꟃTlçwVTlħwMuꞔTlŧwMuʂTlƧwMuᶎTlǧwMuꟈTlȧwVTlɧwMuꟊTlʧwVTl˧wXTlЧwMuꟑTlѧwVTlҧwXTlӧwVTlԧwXTlէwVTl֧wMuꟗTlקwVTlاwMuꟙTl٧wVTlڧwXTlwMwcTlwMwfTlwMwqTlwMuꟶTlwVTlwMuħTlwMuœTlwVTl-wXTl0wVTl:wXTl@wVTlxwXTlwVTlƨwXTlΨwVTlڨwXTlwVTlTwXLdTl_wVTl}wXTlwVTlΩwXTlϩwVTlکwXTlީwVTlwXTlwVTl7wXTl@wVTlNwXTlPwVTlZwXTl\wVTlêwXTl۪wVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTl wVTl'wXTl(wVTl/wXTl0wVTl\wMuꜧTl]wMuꬷTl^wMuɫTl_wMuꭒTl`wVTliwMuʍTljwVTllwXTlpwMuᎠTlqwMuᎡTlrwMuᎢTlswMuᎣTltwMuᎤTluwMuᎥTlvwMuᎦTlwwMuᎧTlxwMuᎨTlywMuᎩTlzwMuᎪTl{wMuᎫTl|wMuᎬTl}wMuᎭTl~wMuᎮTlwMuᎯTlwMuᎰTlwMuᎱTlwMuᎲTlwMuᎳTlwMuᎴTlwMuᎵTlwMuᎶTlwMuᎷTlwMuᎸTlwMuᎹTlwMuᎺTlwMuᎻTlwMuᎼTlwMuᎽTlwMuᎾTlwMuᎿTlwMuᏀTlwMuᏁTlwMuᏂTlwMuᏃTlwMuᏄTlwMuᏅTlwMuᏆTlwMuᏇTlwMuᏈTlwMuᏉTlwMuᏊTlwMuᏋTlwMuᏌTlwMuᏍTlwMuᏎTlwMuᏏTlwMuᏐTlwMuᏑTlwMuᏒTlwMuᏓTlwMuᏔTlwMuᏕTlwMuᏖTlwMuᏗTlwMuᏘTlwMuᏙTlwMuᏚTlwMuᏛTlwMuᏜTlwMuᏝTlwMuᏞLdTlwMuᏟTlwMuᏠTlwMuᏡTlwMuᏢTlwMuᏣTlwMuᏤTlwMuᏥTlwMuᏦTlwMuᏧTlwMuᏨTlwMuᏩTlwMuᏪTlwMuᏫTlwMuᏬTlwMuᏭTlwMuᏮTlwMuᏯTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwMu豈TlwMu更TlwMu車TlwMu賈TlwMu滑TlwMu串TlwMu句TlwMu龜TlwMu契Tl
wMu金TlwMu喇TlwMu奈Tl
wMu懶TlwMu癩TlwMu羅TlwMu蘿TlwMu螺TlwMu裸TlwMu邏TlwMu樂TlwMu洛TlwMu烙TlwMu珞TlwMu落TlwMu酪TlwMu駱TlwMu亂TlwMu卵TlwMu欄TlwMu爛TlwMu蘭Tl wMu鸞Tl!wMu嵐Tl"wMu濫Tl#wMu藍Tl$wMu襤Tl%wMu拉Tl&wMu臘Tl'wMu蠟Tl(wMu廊Tl)wMu朗Tl*wMu浪Tl+wMu狼Tl,wMu郎Tl-wMu來Tl.wMu冷Tl/wMu勞Tl0wMu擄Tl1wMu櫓Tl2wMu爐Tl3wMu盧Tl4wMu老Tl5wMu蘆Tl6wMu虜Tl7wMu路Tl8wMu露Tl9wMu魯Tl:wMu鷺Tl;wMu碌Tl<wMu祿Tl=wMu綠Tl>wMu菉Tl?wMu錄Tl@wMu鹿TlAwMu論TlBwMu壟TlCwMu弄TlDwMu籠TlEwMu聾TlFwMu牢TlGwMu磊TlHwMu賂TlIwMu雷LdTlJwMu壘TlKwMu屢TlLwMu樓TlMwMu淚TlNwMu漏TlOwMu累TlPwMu縷TlQwMu陋TlRwMu勒TlSwMu肋TlTwMu凜TlUwMu凌TlVwMu稜TlWwMu綾TlXwMu菱TlYwMu陵TlZwMu讀Tl[wMu拏Tl\wMu樂Tl]wMu諾Tl^wMu丹Tl_wMu寧Tl`wMu怒TlawMu率TlbwMu異TlcwMu北TldwMu磻TlewMu便TlfwMu復TlgwMu不TlhwMu泌TliwMu數TljwMu索TlkwMu參TllwMu塞TlmwMu省TlnwMu葉TlowMu說TlpwMu殺TlqwMu辰TlrwMu沈TlswMu拾TltwMu若TluwMu掠TlvwMu略TlwwMu亮TlxwMu兩TlywMu凉TlzwMu梁Tl{wMu糧Tl|wMu良Tl}wMu諒Tl~wMu量TlwMu勵TlwMu呂TlwMu女TlwMu廬TlwMu旅TlwMu濾TlwMu礪TlwMu閭TlwMu驪TlwMu麗TlwMu黎TlwMu力TlwMu曆TlwMu歷TlwMu轢TlwMu年TlwMu憐TlwMu戀TlwMu撚TlwMu漣TlwMu煉TlwMu璉TlwMu秊TlwMu練TlwMu聯TlwMu輦TlwMu蓮TlwMu連TlwMu鍊TlwMu列TlwMu劣TlwMu咽TlwMu烈TlwMu裂TlwMu說TlwMu廉TlwMu念TlwMu捻TlwMu殮TlwMu簾TlwMu獵TlwMu令TlwMu囹TlwMu寧TlwMu嶺TlwMu怜TlwMu玲LdTlwMu瑩TlwMu羚TlwMu聆TlwMu鈴TlwMu零TlwMu靈TlwMu領TlwMu例TlwMu禮TlwMu醴TlwMu隸TlwMu惡TlwMu了TlwMu僚TlwMu寮TlwMu尿TlwMu料TlwMu樂TlwMu燎TlwMu療TlwMu蓼TlwMu遼TlwMu龍TlwMu暈TlwMu阮TlwMu劉TlwMu杻TlwMu柳TlwMu流TlwMu溜TlwMu琉TlwMu留TlwMu硫TlwMu紐TlwMu類TlwMu六TlwMu戮TlwMu陸TlwMu倫TlwMu崙TlwMu淪TlwMu輪TlwMu律TlwMu慄TlwMu栗TlwMu率TlwMu隆TlwMu利TlwMu吏TlwMu履TlwMu易TlwMu李TlwMu梨TlwMu泥TlwMu理TlwMu痢TlwMu罹TlwMu裏TlwMu裡TlwMu里TlwMu離TlwMu匿TlwMu溺TlwMu吝TlwMu燐TlwMu璘TlwMu藺TlwMu隣TlwMu鱗TlwMu麟TlwMu林TlwMu淋TlwMu臨TlwMu立TlwMu笠TlwMu粒TlwMu狀TlwMu炙TlwMu識TlwMu什TlwMu茶TlwMu刺TlwMu切TlwMu度TlwMu拓TlwMu糖TlwMu宅TlwMu洞TlwMu暴TlwMu輻TlwMu行TlwMu降Tl
wMu見TlwMu廓TlwMu兀Tl
wMu嗀TlwVTlwMu塚TlwVTlwMu晴LdTlwVTlwMu凞TlwMu猪TlwMu益TlwMu礼TlwMu神TlwMu祥TlwMu福TlwMu靖TlwMu精TlwMu羽TlwVTl wMu蘒Tl!wVTl"wMu諸Tl#wVTl%wMu逸Tl&wMu都Tl'wVTl*wMu飯Tl+wMu飼Tl,wMu館Tl-wMu鶴Tl.wMu郞Tl/wMu隷Tl0wMu侮Tl1wMu僧Tl2wMu免Tl3wMu勉Tl4wMu勤Tl5wMu卑Tl6wMu喝Tl7wMu嘆Tl8wMu器Tl9wMu塀Tl:wMu墨Tl;wMu層Tl<wMu屮Tl=wMu悔Tl>wMu慨Tl?wMu憎Tl@wMu懲TlAwMu敏TlBwMu既TlCwMu暑TlDwMu梅TlEwMu海TlFwMu渚TlGwMu漢TlHwMu煮TlIwMu爫TlJwMu琢TlKwMu碑TlLwMu社TlMwMu祉TlNwMu祈TlOwMu祐TlPwMu祖TlQwMu祝TlRwMu禍TlSwMu禎TlTwMu穀TlUwMu突TlVwMu節TlWwMu練TlXwMu縉TlYwMu繁TlZwMu署Tl[wMu者Tl\wMu臭Tl]wMu艹Tl_wMu著Tl`wMu褐TlawMu視TlbwMu謁TlcwMu謹TldwMu賓TlewMu贈TlfwMu辶TlgwMu逸TlhwMu難TliwMu響TljwMu頻TlkwMu恵TllwMu𤋮TlmwMu舘TlnwXTlpwMu並TlqwMu况TlrwMu全TlswMu侀TltwMu充TluwMu冀TlvwMu勇TlwwMu勺TlxwMu喝TlywMu啕TlzwMu喙Tl{wMu嗢Tl|wMu塚LdTl}wMu墳Tl~wMu奄TlwMu奔TlwMu婢TlwMu嬨TlwMu廒TlwMu廙TlwMu彩TlwMu徭TlwMu惘TlwMu慎TlwMu愈TlwMu憎TlwMu慠TlwMu懲TlwMu戴TlwMu揄TlwMu搜TlwMu摒TlwMu敖TlwMu晴TlwMu朗TlwMu望TlwMu杖TlwMu歹TlwMu殺TlwMu流TlwMu滛TlwMu滋TlwMu漢TlwMu瀞TlwMu煮TlwMu瞧TlwMu爵TlwMu犯TlwMu猪TlwMu瑱TlwMu甆TlwMu画TlwMu瘝TlwMu瘟TlwMu益TlwMu盛TlwMu直TlwMu睊TlwMu着TlwMu磌TlwMu窱TlwMu節TlwMu类TlwMu絛TlwMu練TlwMu缾TlwMu者TlwMu荒TlwMu華TlwMu蝹TlwMu襁TlwMu覆TlwMu視TlwMu調TlwMu諸TlwMu請TlwMu謁TlwMu諾TlwMu諭TlwMu謹TlwMu變TlwMu贈TlwMu輸TlwMu遲TlwMu醙TlwMu鉶TlwMu陼TlwMu難TlwMu靖TlwMu韛TlwMu響TlwMu頋TlwMu頻TlwMu鬒TlwMu龜TlwMu𢡊TlwMu𢡄TlwMu𣏕TlwMu㮝TlwMu䀘TlwMu䀹TlwMu𥉉TlwMu𥳐TlwMu𧻓TlwMu齃TlwMu龎TlwXTlwMaffTlwMafiTlwMaflTlwMaffiTlwMafflTlwMastLdTlwXTlwMuմնTlwMuմեTlwMuմիTlwMuվնTlwMuմխTlwXTlwMuיִTlwVTlwMuײַTl wMuעTl!wMuאTl"wMuדTl#wMuהTl$wMuכTl%wMuלTl&wMuםTl'wMuרTl(wMuתTl)w3w+Tl*wMuשׁTl+wMuשׂTl,wMuשּׁTl-wMuשּׂTl.wMuאַTl/wMuאָTl0wMuאּTl1wMuבּTl2wMuגּTl3wMuדּTl4wMuהּTl5wMuוּTl6wMuזּTl7wXTl8wMuטּTl9wMuיּTl:wMuךּTl;wMuכּTl<wMuלּTl=wXTl>wMuמּTl?wXTl@wMuנּTlAwMuסּTlBwXTlCwMuףּTlDwMuפּTlEwXTlFwMuצּTlGwMuקּTlHwMuרּTlIwMuשּTlJwMuתּTlKwMuוֹTlLwMuבֿTlMwMuכֿTlNwMuפֿTlOwMuאלTlPwMuٱTlRwMuٻTlVwMuپTlZwMuڀTl^wMuٺTlbwMuٿTlfwMuٹTljwMuڤTlnwMuڦTlrwMuڄTlvwMuڃTlzwMuچTl~wMuڇTlwMuڍTlwMuڌTlwMuڎTlwMuڈTlwMuژTlwMuڑTlwMuکTlwMuگTlwMuڳTlwMuڱTlwMuںTlwMuڻTlwMuۀTlwMuہTlwMuھTlwMuےTlwMuۓTlwVTlwXTlwMuڭTlwMuۇTlwMuۆTlwMuۈTlwMuۇٴTlwMuۋTlwMuۅTlwMuۉTlwMuېTlwMuىLdTlwMuئاTlwMuئەTlwMuئوTlwMuئۇTlwMuئۆTlwMuئۈTlwMuئېTlwMuئىTlwMuیTlwMuئجTlwMuئحTlwMuئمTlwMuئىTlwMuئيTlwMuبجTlwMuبحTlwMuبخTlwMuبمTlwMuبىTl
wMuبيTlwMuتجTlwMuتحTl
wMuتخTlwMuتمTlwMuتىTlwMuتيTlwMuثجTlwMuثمTlwMuثىTlwMuثيTlwMuجحTlwMuجمTlwMuحجTlwMuحمTlwMuخجTlwMuخحTlwMuخمTlwMuسجTlwMuسحTlwMuسخTlwMuسمTl wMuصحTl!wMuصمTl"wMuضجTl#wMuضحTl$wMuضخTl%wMuضمTl&wMuطحTl'wMuطمTl(wMuظمTl)wMuعجTl*wMuعمTl+wMuغجTl,wMuغمTl-wMuفجTl.wMuفحTl/wMuفخTl0wMuفمTl1wMuفىTl2wMuفيTl3wMuقحTl4wMuقمTl5wMuقىTl6wMuقيTl7wMuكاTl8wMuكجTl9wMuكحTl:wMuكخTl;wMuكلTl<wMuكمTl=wMuكىTl>wMuكيTl?wMuلجTl@wMuلحTlAwMuلخTlBwMuلمTlCwMuلىTlDwMuليTlEwMuمجTlFwMuمحTlGwMuمخTlHwMuممTlIwMuمىTlJwMuميTlKwMuنجTlLwMuنحTlMwMuنخTlNwMuنمTlOwMuنىTlPwMuنيTlQwMuهجTlRwMuهمTlSwMuهىTlTwMuهيTlUwMuيجTlVwMuيحTlWwMuيخTlXwMuيمTlYwMuيىTlZwMuييLdTl[wMuذٰTl\wMuرٰTl]wMuىٰTl^w3u ٌّTl_w3u ٍّTl`w3u َّTlaw3u ُّTlbw3u ِّTlcw3u ّٰTldwMuئرTlewMuئزTlfwMuئمTlgwMuئنTlhwMuئىTliwMuئيTljwMuبرTlkwMuبزTllwMuبمTlmwMuبنTlnwMuبىTlowMuبيTlpwMuترTlqwMuتزTlrwMuتمTlswMuتنTltwMuتىTluwMuتيTlvwMuثرTlwwMuثزTlxwMuثمTlywMuثنTlzwMuثىTl{wMuثيTl|wMuفىTl}wMuفيTl~wMuقىTlwMuقيTlwMuكاTlwMuكلTlwMuكمTlwMuكىTlwMuكيTlwMuلمTlwMuلىTlwMuليTlwMuماTlwMuممTlwMuنرTlwMuنزTlwMuنمTlwMuننTlwMuنىTlwMuنيTlwMuىٰTlwMuيرTlwMuيزTlwMuيمTlwMuينTlwMuيىTlwMuييTlwMuئجTlwMuئحTlwMuئخTlwMuئمTlwMuئهTlwMuبجTlwMuبحTlwMuبخTlwMuبمTlwMuبهTlwMuتجTlwMuتحTlwMuتخTlwMuتمTlwMuتهTlwMuثمTlwMuجحTlwMuجمTlwMuحجTlwMuحمTlwMuخجTlwMuخمTlwMuسجTlwMuسحTlwMuسخTlwMuسمTlwMuصحTlwMuصخTlwMuصمTlwMuضجTlwMuضحTlwMuضخTlwMuضمTlwMuطحTlwMuظمTlwMuعجTlwMuعمTlwMuغجTlwMuغمTlwMuفجLdTlwMuفحTlwMuفخTlwMuفمTlwMuقحTlwMuقمTlwMuكجTlwMuكحTlwMuكخTlwMuكلTlwMuكمTlwMuلجTlwMuلحTlwMuلخTlwMuلمTlwMuلهTlwMuمجTlwMuمحTlwMuمخTlwMuممTlwMuنجTlwMuنحTlwMuنخTlwMuنمTlwMuنهTlwMuهجTlwMuهمTlwMuهٰTlwMuيجTlwMuيحTlwMuيخTlwMuيمTlwMuيهTlwMuئمTlwMuئهTlwMuبمTlwMuبهTlwMuتمTlwMuتهTlwMuثمTlwMuثهTlwMuسمTlwMuسهTlwMuشمTlwMuشهTlwMuكلTlwMuكمTlwMuلمTlwMuنمTlwMuنهTlwMuيمTlwMuيهTlwMuـَّTlwMuـُّTlwMuـِّTlwMuطىTlwMuطيTlwMuعىTlwMuعيTlwMuغىTlwMuغيTlwMuسىTlwMuسيTlwMuشىTlwMuشيTlwMuحىTlwMuحيTlwMuجىTlwMuجيTlwMuخىTlwMuخيTlwMuصىTlwMuصيTlwMuضىTlwMuضيTlwMuشجTl
wMuشحTlwMuشخTlwMuشمTl
wMuشرTlwMuسرTlwMuصرTlwMuضرTlwMuطىTlwMuطيTlwMuعىTlwMuعيTlwMuغىTlwMuغيTlwMuسىTlwMuسيTlwMuشىTlwMuشيTlwMuحىTlwMuحيTlwMuجىTlwMuجيTlwMuخىTl wMuخيTl!wMuصىTl"wMuصيLdTl#wMuضىTl$wMuضيTl%wMuشجTl&wMuشحTl'wMuشخTl(wMuشمTl)wMuشرTl*wMuسرTl+wMuصرTl,wMuضرTl-wMuشجTl.wMuشحTl/wMuشخTl0wMuشمTl1wMuسهTl2wMuشهTl3wMuطمTl4wMuسجTl5wMuسحTl6wMuسخTl7wMuشجTl8wMuشحTl9wMuشخTl:wMuطمTl;wMuظمTl<wMuاًTl>wVTlPwMuتجمTlQwMuتحجTlSwMuتحمTlTwMuتخمTlUwMuتمجTlVwMuتمحTlWwMuتمخTlXwMuجمحTlZwMuحميTl[wMuحمىTl\wMuسحجTl]wMuسجحTl^wMuسجىTl_wMuسمحTlawMuسمجTlbwMuسممTldwMuصححTlfwMuصممTlgwMuشحمTliwMuشجيTljwMuشمخTllwMuشممTlnwMuضحىTlowMuضخمTlqwMuطمحTlswMuطممTltwMuطميTluwMuعجمTlvwMuعممTlxwMuعمىTlywMuغممTlzwMuغميTl{wMuغمىTl|wMuفخمTl~wMuقمحTlwMuقممTlwMuلحمTlwMuلحيTlwMuلحىTlwMuلججTlwMuلخمTlwMuلمحTlwMuمحجTlwMuمحمTlwMuمحيTlwMuمجحTlwMuمجمTlwMuمخجTlwMuمخمTlwXTlwMuمجخTlwMuهمجTlwMuهممTlwMuنحمTlwMuنحىTlwMuنجمTlwMuنجىTlwMuنميTlwMuنمىTlwMuيممTlwMuبخيTlwMuتجيTlwMuتجىTlwMuتخيTlwMuتخىTlwMuتميTlwMuتمىTlwMuجميTlwMuجحىTlwMuجمىTlwMuسخىTlwMuصحيTlwMuشحيLdTlwMuضحيTlwMuلجيTlwMuلميTlwMuيحيTlwMuيجيTlwMuيميTlwMuمميTlwMuقميTlwMuنحيTlwMuقمحTlwMuلحمTlwMuعميTlwMuكميTlwMuنجحTlwMuمخيTlwMuلجمTlwMuكممTlwMuلجمTlwMuنجحTlwMuجحيTlwMuحجيTlwMuمجيTlwMuفميTlwMuبحيTlwMuكممTlwMuعجمTlwMuصممTlwMuسخيTlwMuنجيTlwXTlwVTlwXTlwMuصلےTlwMuقلےTlwMuاللهTlwMuاكبرTlwMuمحمدTlwMuصلعمTlwMuرسولTlwMuعليهTlwMuوسلمTlwMuصلىTlw3uصلى الله عليه وسلمTlw3uجل جلالهTlwMuریالTlwVTlwITlw3w,TlwMu、TlwXTlw3w:Tlw3w;Tlw3w!Tlw3w?TlwMu〖TlwMu〗TlwXTl wVTl0wXTl1wMu—Tl2wMu–Tl3w3w_Tl5w3w(Tl6w3w)Tl7w3w{Tl8w3w}Tl9wMu〔Tl:wMu〕Tl;wMu【Tl<wMu】Tl=wMu《Tl>wMu》Tl?wMu〈Tl@wMu〉TlAwMu「TlBwMu」TlCwMu『TlDwMu』TlEwVTlGw3w[TlHw3w]TlIw3u ̅TlMw3w_TlPw3w,TlQwMu、TlRwXTlTw3w;TlUw3w:TlVw3w?TlWw3w!TlXwMu—TlYw3w(TlZw3w)Tl[w3w{Tl\w3w}Tl]wMu〔Tl^wMu〕Tl_w3w#Tl`w3w&Tlaw3w*LdTlbw3w+TlcwMw-Tldw3w<Tlew3w>Tlfw3w=TlgwXTlhw3w\Tliw3w$Tljw3w%Tlkw3w@TllwXTlpw3u ًTlqwMuـًTlrw3u ٌTlswVTltw3u ٍTluwXTlvw3u َTlwwMuـَTlxw3u ُTlywMuـُTlzw3u ِTl{wMuـِTl|w3u ّTl}wMuـّTl~w3u ْTlwMuـْTlwMuءTlwMuآTlwMuأTlwMuؤTlwMuإTlwMuئTlwMuاTlwMuبTlwMuةTlwMuتTlwMuثTlwMuجTlwMuحTlwMuخTlwMuدTlwMuذTlwMuرTlwMuزTlwMuسTlwMuشTlwMuصTlwMuضTlwMuطTlwMuظTlwMuعTlwMuغTlwMuفTlwMuقTlwMuكTlwMuلTlwMuمTlwMuنTlwMuهTlwMuوTlwMuىTlwMuيTlwMuلآTlwMuلأTlwMuلإTlwMuلاTlwXTlwITlwXTlw3w!Tlw3w"Tlw3w#Tlw3w$Tlw3w%Tlw3w&Tlw3w'Tlw3w(Tlw3w)Tl
w3w*Tlw3w+Tlw3w,Tl
wMw-TlwMw.Tlw3w/TlwMw0TlwMw1TlwMw2TlwMw3TlwMw4TlwMw5TlwMw6TlwMw7TlwMw8TlwMw9Tlw3w:Tlw3w;Tlw3w<Tlw3w=Tlw3w>LdTlw3w?Tl w3w@Tl!wMwaTl"wMwbTl#wMwcTl$wMwdTl%wMweTl&wMwfTl'wMwgTl(wMwhTl)wMwiTl*wMwjTl+wMwkTl,wMwlTl-wMwmTl.wMwnTl/wMwoTl0wMwpTl1wMwqTl2wMwrTl3wMwsTl4wMwtTl5wMwuTl6wMwvTl7wMwwTl8wMwxTl9wMwyTl:wMwzTl;w3w[Tl<w3w\Tl=w3w]Tl>w3w^Tl?w3w_Tl@w3w`TlAwMwaTlBwMwbTlCwMwcTlDwMwdTlEwMweTlFwMwfTlGwMwgTlHwMwhTlIwMwiTlJwMwjTlKwMwkTlLwMwlTlMwMwmTlNwMwnTlOwMwoTlPwMwpTlQwMwqTlRwMwrTlSwMwsTlTwMwtTlUwMwuTlVwMwvTlWwMwwTlXwMwxTlYwMwyTlZwMwzTl[w3w{Tl\w3w|Tl]w3w}Tl^w3w~Tl_wMu⦅Tl`wMu⦆TlawMw.TlbwMu「TlcwMu」TldwMu、TlewMu・TlfwMuヲTlgwMuァTlhwMuィTliwMuゥTljwMuェTlkwMuォTllwMuャTlmwMuュTlnwMuョTlowMuッTlpwMuーTlqwMuアTlrwMuイTlswMuウTltwMuエTluwMuオTlvwMuカTlwwMuキTlxwMuクTlywMuケTlzwMuコTl{wMuサTl|wMuシTl}wMuスTl~wMuセTlwMuソTlwMuタTlwMuチTlwMuツLdTlwMuテTlwMuトTlwMuナTlwMuニTlwMuヌTlwMuネTlwMuノTlwMuハTlwMuヒTlwMuフTlwMuヘTlwMuホTlwMuマTlwMuミTlwMuムTlwMuメTlwMuモTlwMuヤTlwMuユTlwMuヨTlwMuラTlwMuリTlwMuルTlwMuレTlwMuロTlwMuワTlwMuンTlwMu゙TlwMu゚TlwXTlwMuᄀTlwMuᄁTlwMuᆪTlwMuᄂTlwMuᆬTlwMuᆭTlwMuᄃTlwMuᄄTlwMuᄅTlwMuᆰTlwMuᆱTlwMuᆲTlwMuᆳTlwMuᆴTlwMuᆵTlwMuᄚTlwMuᄆTlwMuᄇTlwMuᄈTlwMuᄡTlwMuᄉTlwMuᄊTlwMuᄋTlwMuᄌTlwMuᄍTlwMuᄎTlwMuᄏTlwMuᄐTlwMuᄑTlwMuᄒTlwXTlwMuᅡTlwMuᅢTlwMuᅣTlwMuᅤTlwMuᅥTlwMuᅦTlwXTlwMuᅧTlwMuᅨTlwMuᅩTlwMuᅪTlwMuᅫTlwMuᅬTlwXTlwMuᅭTlwMuᅮTlwMuᅯTlwMuᅰTlwMuᅱTlwMuᅲTlwXTlwMuᅳTlwMuᅴTlwMuᅵTlwXTlwMu¢TlwMu£TlwMu¬Tlw3u ̄TlwMu¦TlwMu¥TlwMu₩TlwXTlwMu│TlwMu←TlwMu↑TlwMu→TlwMu↓TlwMu■LdTlwMu○TlwXTlwVTlwXTl
wVTl'wXTl(wVTl;wXTl<wVTl>wXTl?wVTlNwXTlPwVTl^wXTlwVTlwXTlwVTlwXTlwVTl4wXTl7wVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTl$wXTl-wVTlKwXTlPwVTl{wXTlwVTlwXTlwVTlwXTlwVTlwXTlwMu𐐨TlwMu𐐩TlwMu𐐪TlwMu𐐫TlwMu𐐬TlwMu𐐭TlwMu𐐮TlwMu𐐯TlwMu𐐰TlwMu𐐱Tl
wMu𐐲TlwMu𐐳TlwMu𐐴Tl
wMu𐐵TlwMu𐐶TlwMu𐐷TlwMu𐐸TlwMu𐐹TlwMu𐐺TlwMu𐐻TlwMu𐐼TlwMu𐐽TlwMu𐐾TlwMu𐐿TlwMu𐑀TlwMu𐑁TlwMu𐑂TlwMu𐑃TlwMu𐑄TlwMu𐑅TlwMu𐑆TlwMu𐑇Tl wMu𐑈Tl!wMu𐑉Tl"wMu𐑊Tl#wMu𐑋Tl$wMu𐑌Tl%wMu𐑍Tl&wMu𐑎Tl'wMu𐑏Tl(wVTlwXTlwVTlwXTlwMu𐓘TlwMu𐓙TlwMu𐓚TlwMu𐓛TlwMu𐓜TlwMu𐓝TlwMu𐓞TlwMu𐓟TlwMu𐓠TlwMu𐓡LdTlwMu𐓢TlwMu𐓣TlwMu𐓤TlwMu𐓥TlwMu𐓦TlwMu𐓧TlwMu𐓨TlwMu𐓩TlwMu𐓪TlwMu𐓫TlwMu𐓬TlwMu𐓭TlwMu𐓮TlwMu𐓯TlwMu𐓰TlwMu𐓱TlwMu𐓲TlwMu𐓳TlwMu𐓴TlwMu𐓵TlwMu𐓶TlwMu𐓷TlwMu𐓸TlwMu𐓹TlwMu𐓺TlwMu𐓻TlwXTlwVTlwXTlwVTl(wXTl0wVTldwXTlowVTlpwMu𐖗TlqwMu𐖘TlrwMu𐖙TlswMu𐖚TltwMu𐖛TluwMu𐖜TlvwMu𐖝TlwwMu𐖞TlxwMu𐖟TlywMu𐖠TlzwMu𐖡Tl{wXTl|wMu𐖣Tl}wMu𐖤Tl~wMu𐖥TlwMu𐖦TlwMu𐖧TlwMu𐖨TlwMu𐖩TlwMu𐖪TlwMu𐖫TlwMu𐖬TlwMu𐖭TlwMu𐖮TlwMu𐖯TlwMu𐖰TlwMu𐖱TlwXTlwMu𐖳TlwMu𐖴TlwMu𐖵TlwMu𐖶TlwMu𐖷TlwMu𐖸TlwMu𐖹TlwXTlwMu𐖻TlwMu𐖼TlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTl7wXTl@wVTlVwXTl`wVTlhwXTlwVTlwMuːTlwMuˑTlwMuæTlwMuʙTlwMuɓTlwXTlwMuʣTlwMuꭦTlwMuʥTlwMuʤTlwMuɖTlwMuɗLdTlwMuᶑTlwMuɘTlwMuɞTlwMuʩTlwMuɤTlwMuɢTlwMuɠTlwMuʛTlwMuħTlwMuʜTlwMuɧTlwMuʄTlwMuʪTlwMuʫTlwMuɬTlwMu𝼄TlwMuꞎTlwMuɮTlwMu𝼅TlwMuʎTlwMu𝼆TlwMuøTlwMuɶTlwMuɷTlwMwqTlwMuɺTlwMu𝼈TlwMuɽTlwMuɾTlwMuʀTlwMuʨTlwMuʦTlwMuꭧTlwMuʧTlwMuʈTlwMuⱱTlwXTlwMuʏTlwMuʡTlwMuʢTlwMuʘTlwMuǀTlwMuǁTlwMuǂTlwMu𝼊TlwMu𝼞TlwXTlwVTlwXTlwVTlwXTl
wVTl6wXTl7wVTl9wXTl<wVTl=wXTl?wVTlVwXTlWwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTl:wXTl?wVTl@wXTlwVTlwXTlwVTlwXTlwVTl
wXTl
wVTl
wXTl
wVTl
wXTl
wVTl
wXTl
wVTl6
wXTl8
wVTl;
wXTl?
wVTlI
wXTlP
wVTlY
wXTl`
wVTl
wXTl
wVTl
wXTl
wVTl
wXTlwVLdTl6wXTl9wVTlVwXTlXwVTlswXTlxwVTlwXTlwVTlwXTlwVTlwXTlwVTlIwXTlwMu𐳀TlwMu𐳁TlwMu𐳂TlwMu𐳃TlwMu𐳄TlwMu𐳅TlwMu𐳆TlwMu𐳇TlwMu𐳈TlwMu𐳉TlwMu𐳊TlwMu𐳋TlwMu𐳌TlwMu𐳍TlwMu𐳎TlwMu𐳏TlwMu𐳐TlwMu𐳑TlwMu𐳒TlwMu𐳓TlwMu𐳔TlwMu𐳕TlwMu𐳖TlwMu𐳗TlwMu𐳘TlwMu𐳙TlwMu𐳚TlwMu𐳛TlwMu𐳜TlwMu𐳝TlwMu𐳞TlwMu𐳟TlwMu𐳠TlwMu𐳡TlwMu𐳢TlwMu𐳣TlwMu𐳤TlwMu𐳥TlwMu𐳦TlwMu𐳧TlwMu𐳨TlwMu𐳩TlwMu𐳪TlwMu𐳫TlwMu𐳬TlwMu𐳭TlwMu𐳮TlwMu𐳯TlwMu𐳰TlwMu𐳱TlwMu𐳲TlwXTlwVTlwXTlwVTl(
wXTl0
wVTl:
wXTl`wVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTl(wXTl0wVTlZwXTlpwVTlwXTlwVTlwXTlwVTlwXTlwVTlNwXTlRwVTlvwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVLdTlwXTlwVTl5wXTl6wVTlHwXTlPwVTlwwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlBwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTl
wXTlwVTlwXTlwVTl)wXTl*wVTl1wXTl2wVTl4wXTl5wVTl:wXTl;wVTlEwXTlGwVTlIwXTlKwVTlNwXTlPwVTlQwXTlWwVTlXwXTl]wVTldwXTlfwVTlmwXTlpwVTluwXTlwVTl\wXTl]wVTlbwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlEwXTlPwVTlZwXTl`wVTlmwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTl,wXTl0wVTlGwXTlwVTl<wXTlwMu𑣀TlwMu𑣁TlwMu𑣂TlwMu𑣃TlwMu𑣄TlwMu𑣅TlwMu𑣆TlwMu𑣇TlwMu𑣈TlwMu𑣉TlwMu𑣊LdTlwMu𑣋TlwMu𑣌TlwMu𑣍TlwMu𑣎TlwMu𑣏TlwMu𑣐TlwMu𑣑TlwMu𑣒TlwMu𑣓TlwMu𑣔TlwMu𑣕TlwMu𑣖TlwMu𑣗TlwMu𑣘TlwMu𑣙TlwMu𑣚TlwMu𑣛TlwMu𑣜TlwMu𑣝TlwMu𑣞TlwMu𑣟TlwVTlwXTlwVTlwXTlwVTl
wXTlwVTlwXTlwVTlwXTlwVTl6wXTl7wVTl9wXTl;wVTlGwXTlPwVTlZwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlHwXTlPwVTlwXTlwVTlwXTlwVTl
wXTlwVTlwXTl
wVTl7wXTl8wVTlFwXTlPwVTlmwXTlpwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTl
wXTlwVTl7wXTl:wVTl;wXTl<wVTl>wXTl?wVTlHwXTlPwVTlZwXTl`wVTlfwXTlgwVTliwXTljwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTl;wXTl>wVLdTlZwXTlwVTlwXTlwVTlwXTlwVTl#wXTl$wVTlo$wXTlp$wVTlu$wXTl$wVTlD%wXTl/wVTl/wXTl0wVTl04wXTl@4wVTlV4wXTlDwVTlGFwXTlhwVTl9jwXTl@jwVTl_jwXTl`jwVTljjwXTlnjwVTljwXTljwVTljwXTljwVTljwXTljwVTljwXTlkwVTlFkwXTlPkwVTlZkwXTl[kwVTlbkwXTlckwVTlxkwXTl}kwVTlkwXTl@nwMu𖹠TlAnwMu𖹡TlBnwMu𖹢TlCnwMu𖹣TlDnwMu𖹤TlEnwMu𖹥TlFnwMu𖹦TlGnwMu𖹧TlHnwMu𖹨TlInwMu𖹩TlJnwMu𖹪TlKnwMu𖹫TlLnwMu𖹬TlMnwMu𖹭TlNnwMu𖹮TlOnwMu𖹯TlPnwMu𖹰TlQnwMu𖹱TlRnwMu𖹲TlSnwMu𖹳TlTnwMu𖹴TlUnwMu𖹵TlVnwMu𖹶TlWnwMu𖹷TlXnwMu𖹸TlYnwMu𖹹TlZnwMu𖹺Tl[nwMu𖹻Tl\nwMu𖹼Tl]nwMu𖹽Tl^nwMu𖹾Tl_nwMu𖹿Tl`nwVTlnwXTlowVTlKowXTlOowVTlowXTlowVTlowXTlowVTlowXTlowVTlowXTlpwVTlwXTlwVTl֌wXTlwVTlwXTlwVTlwXTlwVTlwXTlwVLdTlwXTlwVTl#wXTl2wVTl3wXTlPwVTlSwXTlUwVTlVwXTldwVTlhwXTlpwVTlwXTlwVTlkwXTlpwVTl}wXTlwVTlwXTlwVTlwXTlwVTlwITlwXTlwVTl.wXTl0wVTlGwXTlPwVTlwXTlwVTlwXTlwVTl'wXTl)wVTl^wMu𝅗𝅥Tl_wMu𝅘𝅥Tl`wMu𝅘𝅥𝅮TlawMu𝅘𝅥𝅯TlbwMu𝅘𝅥𝅰TlcwMu𝅘𝅥𝅱TldwMu𝅘𝅥𝅲TlewVTlswXTl{wVTlwMu𝆹𝅥TlwMu𝆺𝅥TlwMu𝆹𝅥𝅮TlwMu𝆺𝅥𝅮TlwMu𝆹𝅥𝅯TlwMu𝆺𝅥𝅯TlwVTlwXTlwVTlFwXTlwVTlwXTlwVTlwXTlwVTlWwXTl`wVTlywXTlwMwaTlwMwbTlwMwcTlwMwdTlwMweTlwMwfTlwMwgTlwMwhTlwMwiTlwMwjTl
wMwkTlwMwlTlwMwmTl
wMwnTlwMwoTlwMwpTlwMwqTlwMwrTlwMwsTlwMwtTlwMwuTlwMwvTlwMwwTlwMwxTlwMwyTlwMwzTlwMwaTlwMwbTlwMwcTlwMwdTlwMweTlwMwfTl wMwgTl!wMwhTl"wMwiTl#wMwjTl$wMwkLdTl%wMwlTl&wMwmTl'wMwnTl(wMwoTl)wMwpTl*wMwqTl+wMwrTl,wMwsTl-wMwtTl.wMwuTl/wMwvTl0wMwwTl1wMwxTl2wMwyTl3wMwzTl4wMwaTl5wMwbTl6wMwcTl7wMwdTl8wMweTl9wMwfTl:wMwgTl;wMwhTl<wMwiTl=wMwjTl>wMwkTl?wMwlTl@wMwmTlAwMwnTlBwMwoTlCwMwpTlDwMwqTlEwMwrTlFwMwsTlGwMwtTlHwMwuTlIwMwvTlJwMwwTlKwMwxTlLwMwyTlMwMwzTlNwMwaTlOwMwbTlPwMwcTlQwMwdTlRwMweTlSwMwfTlTwMwgTlUwXTlVwMwiTlWwMwjTlXwMwkTlYwMwlTlZwMwmTl[wMwnTl\wMwoTl]wMwpTl^wMwqTl_wMwrTl`wMwsTlawMwtTlbwMwuTlcwMwvTldwMwwTlewMwxTlfwMwyTlgwMwzTlhwMwaTliwMwbTljwMwcTlkwMwdTllwMweTlmwMwfTlnwMwgTlowMwhTlpwMwiTlqwMwjTlrwMwkTlswMwlTltwMwmTluwMwnTlvwMwoTlwwMwpTlxwMwqTlywMwrTlzwMwsTl{wMwtTl|wMwuTl}wMwvTl~wMwwTlwMwxTlwMwyTlwMwzTlwMwaTlwMwbTlwMwcTlwMwdTlwMweTlwMwfTlwMwgLdTlwMwhTlwMwiTlwMwjTlwMwkTlwMwlTlwMwmTlwMwnTlwMwoTlwMwpTlwMwqTlwMwrTlwMwsTlwMwtTlwMwuTlwMwvTlwMwwTlwMwxTlwMwyTlwMwzTlwMwaTlwXTlwMwcTlwMwdTlwXTlwMwgTlwXTlwMwjTlwMwkTlwXTlwMwnTlwMwoTlwMwpTlwMwqTlwXTlwMwsTlwMwtTlwMwuTlwMwvTlwMwwTlwMwxTlwMwyTlwMwzTlwMwaTlwMwbTlwMwcTlwMwdTlwXTlwMwfTlwXTlwMwhTlwMwiTlwMwjTlwMwkTlwMwlTlwMwmTlwMwnTlwXTlwMwpTlwMwqTlwMwrTlwMwsTlwMwtTlwMwuTlwMwvTlwMwwTlwMwxTlwMwyTlwMwzTlwMwaTlwMwbTlwMwcTlwMwdTlwMweTlwMwfTlwMwgTlwMwhTlwMwiTlwMwjTlwMwkTlwMwlTlwMwmTlwMwnTlwMwoTlwMwpTlwMwqTlwMwrTlwMwsTlwMwtTlwMwuTlwMwvTlwMwwTlwMwxTlwMwyTlwMwzTlwMwaTlwMwbTlwMwcTlwMwdTlwMweTlwMwfLdTlwMwgTlwMwhTlwMwiTlwMwjTlwMwkTlwMwlTlwMwmTlwMwnTlwMwoTlwMwpTlwMwqTlwMwrTlwMwsTlwMwtTlwMwuTlwMwvTlwMwwTlwMwxTlwMwyTlwMwzTlwMwaTlwMwbTlwXTlwMwdTlwMweTlwMwfTl
wMwgTlwXTl
wMwjTlwMwkTlwMwlTlwMwmTlwMwnTlwMwoTlwMwpTlwMwqTlwXTlwMwsTlwMwtTlwMwuTlwMwvTlwMwwTlwMwxTlwMwyTlwXTlwMwaTlwMwbTl wMwcTl!wMwdTl"wMweTl#wMwfTl$wMwgTl%wMwhTl&wMwiTl'wMwjTl(wMwkTl)wMwlTl*wMwmTl+wMwnTl,wMwoTl-wMwpTl.wMwqTl/wMwrTl0wMwsTl1wMwtTl2wMwuTl3wMwvTl4wMwwTl5wMwxTl6wMwyTl7wMwzTl8wMwaTl9wMwbTl:wXTl;wMwdTl<wMweTl=wMwfTl>wMwgTl?wXTl@wMwiTlAwMwjTlBwMwkTlCwMwlTlDwMwmTlEwXTlFwMwoTlGwXTlJwMwsTlKwMwtTlLwMwuTlMwMwvTlNwMwwTlOwMwxTlPwMwyTlQwXTlRwMwaTlSwMwbTlTwMwcTlUwMwdTlVwMweLdTlWwMwfTlXwMwgTlYwMwhTlZwMwiTl[wMwjTl\wMwkTl]wMwlTl^wMwmTl_wMwnTl`wMwoTlawMwpTlbwMwqTlcwMwrTldwMwsTlewMwtTlfwMwuTlgwMwvTlhwMwwTliwMwxTljwMwyTlkwMwzTllwMwaTlmwMwbTlnwMwcTlowMwdTlpwMweTlqwMwfTlrwMwgTlswMwhTltwMwiTluwMwjTlvwMwkTlwwMwlTlxwMwmTlywMwnTlzwMwoTl{wMwpTl|wMwqTl}wMwrTl~wMwsTlwMwtTlwMwuTlwMwvTlwMwwTlwMwxTlwMwyTlwMwzTlwMwaTlwMwbTlwMwcTlwMwdTlwMweTlwMwfTlwMwgTlwMwhTlwMwiTlwMwjTlwMwkTlwMwlTlwMwmTlwMwnTlwMwoTlwMwpTlwMwqTlwMwrTlwMwsTlwMwtTlwMwuTlwMwvTlwMwwTlwMwxTlwMwyTlwMwzTlwMwaTlwMwbTlwMwcTlwMwdTlwMweTlwMwfTlwMwgTlwMwhTlwMwiTlwMwjTlwMwkTlwMwlTlwMwmTlwMwnTlwMwoTlwMwpTlwMwqTlwMwrTlwMwsTlwMwtTlwMwuTlwMwvTlwMwwTlwMwxTlwMwyTlwMwzTlwMwaLdTlwMwbTlwMwcTlwMwdTlwMweTlwMwfTlwMwgTlwMwhTlwMwiTlwMwjTlwMwkTlwMwlTlwMwmTlwMwnTlwMwoTlwMwpTlwMwqTlwMwrTlwMwsTlwMwtTlwMwuTlwMwvTlwMwwTlwMwxTlwMwyTlwMwzTlwMwaTlwMwbTlwMwcTlwMwdTlwMweTlwMwfTlwMwgTlwMwhTlwMwiTlwMwjTlwMwkTlwMwlTlwMwmTlwMwnTlwMwoTlwMwpTlwMwqTlwMwrTlwMwsTlwMwtTlwMwuTlwMwvTlwMwwTlwMwxTlwMwyTlwMwzTlwMwaTlwMwbTlwMwcTlwMwdTlwMweTlwMwfTlwMwgTlwMwhTlwMwiTlwMwjTlwMwkTlwMwlTlwMwmTlwMwnTlwMwoTlwMwpTlwMwqTlwMwrTlwMwsTlwMwtTlwMwuTlwMwvTlwMwwTlwMwxTlwMwyTlwMwzTlwMwaTlwMwbTl
wMwcTlwMwdTlwMweTl
wMwfTlwMwgTlwMwhTlwMwiTlwMwjTlwMwkTlwMwlTlwMwmTlwMwnTlwMwoTlwMwpTlwMwqTlwMwrTlwMwsTlwMwtTlwMwuTlwMwvTlwMwwLdTlwMwxTl wMwyTl!wMwzTl"wMwaTl#wMwbTl$wMwcTl%wMwdTl&wMweTl'wMwfTl(wMwgTl)wMwhTl*wMwiTl+wMwjTl,wMwkTl-wMwlTl.wMwmTl/wMwnTl0wMwoTl1wMwpTl2wMwqTl3wMwrTl4wMwsTl5wMwtTl6wMwuTl7wMwvTl8wMwwTl9wMwxTl:wMwyTl;wMwzTl<wMwaTl=wMwbTl>wMwcTl?wMwdTl@wMweTlAwMwfTlBwMwgTlCwMwhTlDwMwiTlEwMwjTlFwMwkTlGwMwlTlHwMwmTlIwMwnTlJwMwoTlKwMwpTlLwMwqTlMwMwrTlNwMwsTlOwMwtTlPwMwuTlQwMwvTlRwMwwTlSwMwxTlTwMwyTlUwMwzTlVwMwaTlWwMwbTlXwMwcTlYwMwdTlZwMweTl[wMwfTl\wMwgTl]wMwhTl^wMwiTl_wMwjTl`wMwkTlawMwlTlbwMwmTlcwMwnTldwMwoTlewMwpTlfwMwqTlgwMwrTlhwMwsTliwMwtTljwMwuTlkwMwvTllwMwwTlmwMwxTlnwMwyTlowMwzTlpwMwaTlqwMwbTlrwMwcTlswMwdTltwMweTluwMwfTlvwMwgTlwwMwhTlxwMwiTlywMwjTlzwMwkTl{wMwlTl|wMwmTl}wMwnTl~wMwoTlwMwpTlwMwqTlwMwrTlwMwsLdTlwMwtTlwMwuTlwMwvTlwMwwTlwMwxTlwMwyTlwMwzTlwMwaTlwMwbTlwMwcTlwMwdTlwMweTlwMwfTlwMwgTlwMwhTlwMwiTlwMwjTlwMwkTlwMwlTlwMwmTlwMwnTlwMwoTlwMwpTlwMwqTlwMwrTlwMwsTlwMwtTlwMwuTlwMwvTlwMwwTlwMwxTlwMwyTlwMwzTlwMuıTlwMuȷTlwXTlwMuαTlwMuβTlwMuγTlwMuδTlwMuεTlwMuζTlwMuηTlwMuθTlwMuιTlwMuκTlwMuλTlwMuμTlwMuνTlwMuξTlwMuοTlwMuπTlwMuρTlwMuθTlwMuσTlwMuτTlwMuυTlwMuφTlwMuχTlwMuψTlwMuωTlwMu∇TlwMuαTlwMuβTlwMuγTlwMuδTlwMuεTlwMuζTlwMuηTlwMuθTlwMuιTlwMuκTlwMuλTlwMuμTlwMuνTlwMuξTlwMuοTlwMuπTlwMuρTlwMuσTlwMuτTlwMuυTlwMuφTlwMuχTlwMuψTlwMuωTlwMu∂TlwMuεTlwMuθTlwMuκTlwMuφTlwMuρTlwMuπTlwMuαTlwMuβTlwMuγTlwMuδTlwMuεTlwMuζTlwMuηLdTlwMuθTlwMuιTlwMuκTlwMuλTlwMuμTlwMuνTlwMuξTlwMuοTlwMuπTlwMuρTlwMuθTlwMuσTlwMuτTlwMuυTlwMuφTlwMuχTlwMuψTlwMuωTlwMu∇TlwMuαTlwMuβTlwMuγTlwMuδTlwMuεTlwMuζTlwMuηTlwMuθTlwMuιTlwMuκTlwMuλTlwMuμTlwMuνTlwMuξTl
wMuοTlwMuπTlwMuρTl
wMuσTlwMuτTlwMuυTlwMuφTlwMuχTlwMuψTlwMuωTlwMu∂TlwMuεTlwMuθTlwMuκTlwMuφTlwMuρTlwMuπTlwMuαTlwMuβTlwMuγTlwMuδTl wMuεTl!wMuζTl"wMuηTl#wMuθTl$wMuιTl%wMuκTl&wMuλTl'wMuμTl(wMuνTl)wMuξTl*wMuοTl+wMuπTl,wMuρTl-wMuθTl.wMuσTl/wMuτTl0wMuυTl1wMuφTl2wMuχTl3wMuψTl4wMuωTl5wMu∇Tl6wMuαTl7wMuβTl8wMuγTl9wMuδTl:wMuεTl;wMuζTl<wMuηTl=wMuθTl>wMuιTl?wMuκTl@wMuλTlAwMuμTlBwMuνTlCwMuξTlDwMuοTlEwMuπTlFwMuρTlGwMuσTlIwMuτTlJwMuυTlKwMuφTlLwMuχTlMwMuψTlNwMuωLdTlOwMu∂TlPwMuεTlQwMuθTlRwMuκTlSwMuφTlTwMuρTlUwMuπTlVwMuαTlWwMuβTlXwMuγTlYwMuδTlZwMuεTl[wMuζTl\wMuηTl]wMuθTl^wMuιTl_wMuκTl`wMuλTlawMuμTlbwMuνTlcwMuξTldwMuοTlewMuπTlfwMuρTlgwMuθTlhwMuσTliwMuτTljwMuυTlkwMuφTllwMuχTlmwMuψTlnwMuωTlowMu∇TlpwMuαTlqwMuβTlrwMuγTlswMuδTltwMuεTluwMuζTlvwMuηTlwwMuθTlxwMuιTlywMuκTlzwMuλTl{wMuμTl|wMuνTl}wMuξTl~wMuοTlwMuπTlwMuρTlwMuσTlwMuτTlwMuυTlwMuφTlwMuχTlwMuψTlwMuωTlwMu∂TlwMuεTlwMuθTlwMuκTlwMuφTlwMuρTlwMuπTlwMuαTlwMuβTlwMuγTlwMuδTlwMuεTlwMuζTlwMuηTlwMuθTlwMuιTlwMuκTlwMuλTlwMuμTlwMuνTlwMuξTlwMuοTlwMuπTlwMuρTlwMuθTlwMuσTlwMuτTlwMuυTlwMuφTlwMuχTlwMuψTlwMuωTlwMu∇TlwMuαTlwMuβTlwMuγTlwMuδTlwMuεTlwMuζTlwMuηTlwMuθTlwMuιTlwMuκLdTlwMuλTlwMuμTlwMuνTlwMuξTlwMuοTlwMuπTlwMuρTlwMuσTlwMuτTlwMuυTlwMuφTlwMuχTlwMuψTlwMuωTlwMu∂TlwMuεTlwMuθTlwMuκTlwMuφTlwMuρTlwMuπTlwMuϝTlwXTlwMw0TlwMw1TlwMw2TlwMw3TlwMw4TlwMw5TlwMw6TlwMw7TlwMw8TlwMw9TlwMw0TlwMw1TlwMw2TlwMw3TlwMw4TlwMw5TlwMw6TlwMw7TlwMw8TlwMw9TlwMw0TlwMw1TlwMw2TlwMw3TlwMw4TlwMw5TlwMw6TlwMw7TlwMw8TlwMw9TlwMw0TlwMw1TlwMw2TlwMw3TlwMw4TlwMw5TlwMw6TlwMw7TlwMw8TlwMw9TlwMw0TlwMw1TlwMw2TlwMw3TlwMw4TlwMw5TlwMw6TlwMw7TlwMw8TlwMw9TlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTl%wVTl+wXTlwVTlwXTlwVTlwXTlwVTl"wXTl#wVTl%wXTl&wVTl+wXTl0wMuаTl1wMuбTl2wMuвTl3wMuгTl4wMuдTl5wMuеTl6wMuжLdTl7wMuзTl8wMuиTl9wMuкTl:wMuлTl;wMuмTl<wMuоTl=wMuпTl>wMuрTl?wMuсTl@wMuтTlAwMuуTlBwMuфTlCwMuхTlDwMuцTlEwMuчTlFwMuшTlGwMuыTlHwMuэTlIwMuюTlJwMuꚉTlKwMuәTlLwMuіTlMwMuјTlNwMuөTlOwMuүTlPwMuӏTlQwMuаTlRwMuбTlSwMuвTlTwMuгTlUwMuдTlVwMuеTlWwMuжTlXwMuзTlYwMuиTlZwMuкTl[wMuлTl\wMuоTl]wMuпTl^wMuсTl_wMuуTl`wMuфTlawMuхTlbwMuцTlcwMuчTldwMuшTlewMuъTlfwMuыTlgwMuґTlhwMuіTliwMuѕTljwMuџTlkwMuҫTllwMuꙑTlmwMuұTlnwXTlwVTlwXTlwVTl-wXTl0wVTl>wXTl@wVTlJwXTlNwVTlPwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwMu𞤢TlwMu𞤣TlwMu𞤤TlwMu𞤥TlwMu𞤦TlwMu𞤧TlwMu𞤨TlwMu𞤩TlwMu𞤪TlwMu𞤫Tl
wMu𞤬TlwMu𞤭TlwMu𞤮Tl
wMu𞤯LdTlwMu𞤰TlwMu𞤱TlwMu𞤲TlwMu𞤳TlwMu𞤴TlwMu𞤵TlwMu𞤶TlwMu𞤷TlwMu𞤸TlwMu𞤹TlwMu𞤺TlwMu𞤻TlwMu𞤼TlwMu𞤽TlwMu𞤾TlwMu𞤿TlwMu𞥀TlwMu𞥁Tl wMu𞥂Tl!wMu𞥃Tl"wVTlLwXTlPwVTlZwXTl^wVTl`wXTlqwVTlwXTlwVTl>wXTlwMuاTlwMuبTlwMuجTlwMuدTlwXTlwMuوTlwMuزTlwMuحTlwMuطTlwMuيTl
wMuكTlwMuلTlwMuمTl
wMuنTlwMuسTlwMuعTlwMuفTlwMuصTlwMuقTlwMuرTlwMuشTlwMuتTlwMuثTlwMuخTlwMuذTlwMuضTlwMuظTlwMuغTlwMuٮTlwMuںTlwMuڡTlwMuٯTl wXTl!wMuبTl"wMuجTl#wXTl$wMuهTl%wXTl'wMuحTl(wXTl)wMuيTl*wMuكTl+wMuلTl,wMuمTl-wMuنTl.wMuسTl/wMuعTl0wMuفTl1wMuصTl2wMuقTl3wXTl4wMuشTl5wMuتTl6wMuثTl7wMuخTl8wXTl9wMuضTl:wXTl;wMuغTl<wXTlBwMuجTlCwXTlGwMuحTlHwXTlIwMuيTlJwXTlKwMuلTlLwXTlMwMuنTlNwMuسLdTlOwMuعTlPwXTlQwMuصTlRwMuقTlSwXTlTwMuشTlUwXTlWwMuخTlXwXTlYwMuضTlZwXTl[wMuغTl\wXTl]wMuںTl^wXTl_wMuٯTl`wXTlawMuبTlbwMuجTlcwXTldwMuهTlewXTlgwMuحTlhwMuطTliwMuيTljwMuكTlkwXTllwMuمTlmwMuنTlnwMuسTlowMuعTlpwMuفTlqwMuصTlrwMuقTlswXTltwMuشTluwMuتTlvwMuثTlwwMuخTlxwXTlywMuضTlzwMuظTl{wMuغTl|wMuٮTl}wXTl~wMuڡTlwXTlwMuاTlwMuبTlwMuجTlwMuدTlwMuهTlwMuوTlwMuزTlwMuحTlwMuطTlwMuيTlwXTlwMuلTlwMuمTlwMuنTlwMuسTlwMuعTlwMuفTlwMuصTlwMuقTlwMuرTlwMuشTlwMuتTlwMuثTlwMuخTlwMuذTlwMuضTlwMuظTlwMuغTlwXTlwMuبTlwMuجTlwMuدTlwXTlwMuوTlwMuزTlwMuحTlwMuطTlwMuيTlwXTlwMuلTlwMuمTlwMuنTlwMuسTlwMuعTlwMuفTlwMuصTlwMuقTlwMuرTlwMuشTlwMuتTlwMuثTlwMuخTlwMuذLdTlwMuضTlwMuظTlwMuغTlwXTlwVTlwXTlwVTl,wXTl0wVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlw3u0,Tlw3u1,Tlw3u2,Tlw3u3,Tlw3u4,Tlw3u5,Tlw3u6,Tlw3u7,Tlw3u8,Tl
w3u9,TlwVTlw3u(a)Tlw3u(b)Tlw3u(c)Tlw3u(d)Tlw3u(e)Tlw3u(f)Tlw3u(g)Tlw3u(h)Tlw3u(i)Tlw3u(j)Tlw3u(k)Tlw3u(l)Tlw3u(m)Tlw3u(n)Tlw3u(o)Tlw3u(p)Tl w3u(q)Tl!w3u(r)Tl"w3u(s)Tl#w3u(t)Tl$w3u(u)Tl%w3u(v)Tl&w3u(w)Tl'w3u(x)Tl(w3u(y)Tl)w3u(z)Tl*wMu〔s〕Tl+wMwcTl,wMwrTl-wMacdTl.wMawzTl/wVTl0wMwaTl1wMwbTl2wMwcTl3wMwdTl4wMweTl5wMwfTl6wMwgTl7wMwhTl8wMwiTl9wMwjTl:wMwkTl;wMwlTl<wMwmTl=wMwnTl>wMwoTl?wMwpTl@wMwqTlAwMwrTlBwMwsTlCwMwtTlDwMwuTlEwMwvTlFwMwwTlGwMwxTlHwMwyTlIwMwzTlJwMahvTlKwMamvTlLwMasdTlMwMassTlNwMappvTlOwMawcTlPwVTljwMamcTlkwMamdTllwMamrTlmwVTlwMadjTlwVLdTlwXTlwVTlwMuほかTlwMuココTlwMuサTlwXTlwMu手TlwMu字TlwMu双TlwMuデTlwMu二TlwMu多TlwMu解TlwMu天TlwMu交TlwMu映TlwMu無TlwMu料TlwMu前TlwMu後TlwMu再TlwMu新Tl wMu初Tl!wMu終Tl"wMu生Tl#wMu販Tl$wMu声Tl%wMu吹Tl&wMu演Tl'wMu投Tl(wMu捕Tl)wMu一Tl*wMu三Tl+wMu遊Tl,wMu左Tl-wMu中Tl.wMu右Tl/wMu指Tl0wMu走Tl1wMu打Tl2wMu禁Tl3wMu空Tl4wMu合Tl5wMu満Tl6wMu有Tl7wMu月Tl8wMu申Tl9wMu割Tl:wMu営Tl;wMu配Tl<wXTl@wMu〔本〕TlAwMu〔三〕TlBwMu〔二〕TlCwMu〔安〕TlDwMu〔点〕TlEwMu〔打〕TlFwMu〔盗〕TlGwMu〔勝〕TlHwMu〔敗〕TlIwXTlPwMu得TlQwMu可TlRwXTl`wVTlfwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwwXTl{wVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlHwXTlPwVTlZwXTl`wVTlwXTlwVTlwXTlwVTlwXTlwVTlTwXTl`wVTlnwXTlpwVTl}wXTlwVTlwXLdTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwVTlwXTlwMw0TlwMw1TlwMw2TlwMw3TlwMw4TlwMw5TlwMw6TlwMw7TlwMw8TlwMw9TlwXTlwVTlwXTlwVTl:wXTl@wVTlwXTl wVTlwXTlwVTlwXTlwVTl^wXTlwMu丽TlwMu丸TlwMu乁TlwMu𠄢TlwMu你TlwMu侮TlwMu侻TlwMu倂TlwMu偺TlwMu備Tl
wMu僧TlwMu像TlwMu㒞Tl
wMu𠘺TlwMu免TlwMu兔TlwMu兤TlwMu具TlwMu𠔜TlwMu㒹TlwMu內TlwMu再TlwMu𠕋TlwMu冗TlwMu冤TlwMu仌TlwMu冬TlwMu况TlwMu𩇟TlwMu凵TlwMu刃TlwMu㓟Tl wMu刻Tl!wMu剆Tl"wMu割Tl#wMu剷Tl$wMu㔕Tl%wMu勇Tl&wMu勉Tl'wMu勤Tl(wMu勺Tl)wMu包Tl*wMu匆Tl+wMu北Tl,wMu卉Tl-wMu卑Tl.wMu博Tl/wMu即Tl0wMu卽Tl1wMu卿Tl4wMu𠨬Tl5wMu灰Tl6wMu及Tl7wMu叟Tl8wMu𠭣Tl9wMu叫Tl:wMu叱Tl;wMu吆Tl<wMu咞Tl=wMu吸Tl>wMu呈Tl?wMu周Tl@wMu咢LdTlAwMu哶TlBwMu唐TlCwMu啓TlDwMu啣TlEwMu善TlGwMu喙TlHwMu喫TlIwMu喳TlJwMu嗂TlKwMu圖TlLwMu嘆TlMwMu圗TlNwMu噑TlOwMu噴TlPwMu切TlQwMu壮TlRwMu城TlSwMu埴TlTwMu堍TlUwMu型TlVwMu堲TlWwMu報TlXwMu墬TlYwMu𡓤TlZwMu売Tl[wMu壷Tl\wMu夆Tl]wMu多Tl^wMu夢Tl_wMu奢Tl`wMu𡚨TlawMu𡛪TlbwMu姬TlcwMu娛TldwMu娧TlewMu姘TlfwMu婦TlgwMu㛮TlhwXTliwMu嬈TljwMu嬾TllwMu𡧈TlmwMu寃TlnwMu寘TlowMu寧TlpwMu寳TlqwMu𡬘TlrwMu寿TlswMu将TltwXTluwMu尢TlvwMu㞁TlwwMu屠TlxwMu屮TlywMu峀TlzwMu岍Tl{wMu𡷤Tl|wMu嵃Tl}wMu𡷦Tl~wMu嵮TlwMu嵫TlwMu嵼TlwMu巡TlwMu巢TlwMu㠯TlwMu巽TlwMu帨TlwMu帽TlwMu幩TlwMu㡢TlwMu𢆃TlwMu㡼TlwMu庰TlwMu庳TlwMu庶TlwMu廊TlwMu𪎒TlwMu廾TlwMu𢌱TlwMu舁TlwMu弢TlwMu㣇TlwMu𣊸TlwMu𦇚TlwMu形TlwMu彫TlwMu㣣TlwMu徚TlwMu忍TlwMu志TlwMu忹TlwMu悁TlwMu㤺TlwMu㤜TlwMu悔TlwMu𢛔TlwMu惇TlwMu慈TlwMu慌TlwMu慎LdTlwMu慌TlwMu慺TlwMu憎TlwMu憲TlwMu憤TlwMu憯TlwMu懞TlwMu懲TlwMu懶TlwMu成TlwMu戛TlwMu扝TlwMu抱TlwMu拔TlwMu捐TlwMu𢬌TlwMu挽TlwMu拼TlwMu捨TlwMu掃TlwMu揤TlwMu𢯱TlwMu搢TlwMu揅TlwMu掩TlwMu㨮TlwMu摩TlwMu摾TlwMu撝TlwMu摷TlwMu㩬TlwMu敏TlwMu敬TlwMu𣀊TlwMu旣TlwMu書TlwMu晉TlwMu㬙TlwMu暑TlwMu㬈TlwMu㫤TlwMu冒TlwMu冕TlwMu最TlwMu暜TlwMu肭TlwMu䏙TlwMu朗TlwMu望TlwMu朡TlwMu杞TlwMu杓TlwMu𣏃TlwMu㭉TlwMu柺TlwMu枅TlwMu桒TlwMu梅TlwMu𣑭TlwMu梎TlwMu栟TlwMu椔TlwMu㮝TlwMu楂TlwMu榣TlwMu槪TlwMu檨TlwMu𣚣TlwMu櫛TlwMu㰘TlwMu次TlwMu𣢧TlwMu歔TlwMu㱎TlwMu歲TlwMu殟TlwMu殺TlwMu殻TlwMu𣪍TlwMu𡴋TlwMu𣫺TlwMu汎TlwMu𣲼TlwMu沿TlwMu泍TlwMu汧TlwMu洖TlwMu派TlwMu海TlwMu流TlwMu浩TlwMu浸TlwMu涅TlwMu𣴞TlwMu洴TlwMu港TlwMu湮Tl
wMu㴳TlwMu滋TlwMu滇LdTl
wMu𣻑TlwMu淹TlwMu潮TlwMu𣽞TlwMu𣾎TlwMu濆TlwMu瀹TlwMu瀞TlwMu瀛TlwMu㶖TlwMu灊TlwMu災TlwMu灷TlwMu炭TlwMu𠔥TlwMu煅TlwMu𤉣TlwMu熜TlwXTl wMu爨Tl!wMu爵Tl"wMu牐Tl#wMu𤘈Tl$wMu犀Tl%wMu犕Tl&wMu𤜵Tl'wMu𤠔Tl(wMu獺Tl)wMu王Tl*wMu㺬Tl+wMu玥Tl,wMu㺸Tl.wMu瑇Tl/wMu瑜Tl0wMu瑱Tl1wMu璅Tl2wMu瓊Tl3wMu㼛Tl4wMu甤Tl5wMu𤰶Tl6wMu甾Tl7wMu𤲒Tl8wMu異Tl9wMu𢆟Tl:wMu瘐Tl;wMu𤾡Tl<wMu𤾸Tl=wMu𥁄Tl>wMu㿼Tl?wMu䀈Tl@wMu直TlAwMu𥃳TlBwMu𥃲TlCwMu𥄙TlDwMu𥄳TlEwMu眞TlFwMu真TlHwMu睊TlIwMu䀹TlJwMu瞋TlKwMu䁆TlLwMu䂖TlMwMu𥐝TlNwMu硎TlOwMu碌TlPwMu磌TlQwMu䃣TlRwMu𥘦TlSwMu祖TlTwMu𥚚TlUwMu𥛅TlVwMu福TlWwMu秫TlXwMu䄯TlYwMu穀TlZwMu穊Tl[wMu穏Tl\wMu𥥼Tl]wMu𥪧Tl_wXTl`wMu䈂TlawMu𥮫TlbwMu篆TlcwMu築TldwMu䈧TlewMu𥲀TlfwMu糒TlgwMu䊠TlhwMu糨TliwMu糣TljwMu紀TlkwMu𥾆TllwMu絣TlmwMu䌁TlnwMu緇TlowMu縂TlpwMu繅TlqwMu䌴TlrwMu𦈨TlswMu𦉇LdTltwMu䍙TluwMu𦋙TlvwMu罺TlwwMu𦌾TlxwMu羕TlywMu翺TlzwMu者Tl{wMu𦓚Tl|wMu𦔣Tl}wMu聠Tl~wMu𦖨TlwMu聰TlwMu𣍟TlwMu䏕TlwMu育TlwMu脃TlwMu䐋TlwMu脾TlwMu媵TlwMu𦞧TlwMu𦞵TlwMu𣎓TlwMu𣎜TlwMu舁TlwMu舄TlwMu辞TlwMu䑫TlwMu芑TlwMu芋TlwMu芝TlwMu劳TlwMu花TlwMu芳TlwMu芽TlwMu苦TlwMu𦬼TlwMu若TlwMu茝TlwMu荣TlwMu莭TlwMu茣TlwMu莽TlwMu菧TlwMu著TlwMu荓TlwMu菊TlwMu菌TlwMu菜TlwMu𦰶TlwMu𦵫TlwMu𦳕TlwMu䔫TlwMu蓱TlwMu蓳TlwMu蔖TlwMu𧏊TlwMu蕤TlwMu𦼬TlwMu䕝TlwMu䕡TlwMu𦾱TlwMu𧃒TlwMu䕫TlwMu虐TlwMu虜TlwMu虧TlwMu虩TlwMu蚩TlwMu蚈TlwMu蜎TlwMu蛢TlwMu蝹TlwMu蜨TlwMu蝫TlwMu螆TlwXTlwMu蟡TlwMu蠁TlwMu䗹TlwMu衠TlwMu衣TlwMu𧙧TlwMu裗TlwMu裞TlwMu䘵TlwMu裺TlwMu㒻TlwMu𧢮TlwMu𧥦TlwMu䚾TlwMu䛇TlwMu誠TlwMu諭TlwMu變TlwMu豕TlwMu𧲨TlwMu貫TlwMu賁TlwMu贛TlwMu起LLTlwMu𧼯TlwMu𠠄TlwMu跋TlwMu趼TlwMu跰TlwMu𠣞TlwMu軔TlwMu輸TlwMu𨗒TlwMu𨗭TlwMu邔TlwMu郱TlwMu鄑TlwMu𨜮TlwMu鄛TlwMu鈸TlwMu鋗TlwMu鋘TlwMu鉼TlwMu鏹TlwMu鐕TlwMu𨯺TlwMu開TlwMu䦕TlwMu閷TlwMu𨵷TlwMu䧦TlwMu雃TlwMu嶲TlwMu霣TlwMu𩅅TlwMu𩈚TlwMu䩮TlwMu䩶TlwMu韠TlwMu𩐊TlwMu䪲TlwMu𩒖TlwMu頋TlwMu頩TlwMu𩖶TlwMu飢TlwMu䬳TlwMu餩TlwMu馧TlwMu駂TlwMu駾TlwMu䯎TlwMu𩬰Tl
wMu鬒TlwMu鱀TlwMu鳽Tl
wMu䳎TlwMu䳭TlwMu鵧TlwMu𪃎TlwMu䳸TlwMu𪄅TlwMu𪈎TlwMu𪊑TlwMu麻TlwMu䵖TlwMu黹TlwMu黾TlwMu鼅TlwMu鼏TlwMu鼖TlwMu鼻TlwMu𪘀TlwXTlwVTlKwXTlPwVTl#wXTlwITlwXa__doc__a__file__a__spec__aoriginahas_locationa__cached__aListlaTupleaUnionu15.1.0a__version__areturnTOintOstrTOintOstrpa_seg_0a_seg_1a_seg_2a_seg_3a_seg_4a_seg_5a_seg_6a_seg_7a_seg_8a_seg_9a_seg_10a_seg_11a_seg_12a_seg_13a_seg_14a_seg_15a_seg_16a_seg_17a_seg_18a_seg_19a_seg_20a_seg_21a_seg_22a_seg_23a_seg_24a_seg_25a_seg_26a_seg_27a_seg_28a_seg_29a_seg_30a_seg_31a_seg_32a_seg_33a_seg_34a_seg_35a_seg_36a_seg_37a_seg_38a_seg_39a_seg_40a_seg_41a_seg_42a_seg_43a_seg_44a_seg_45a_seg_46a_seg_47a_seg_48a_seg_49a_seg_50a_seg_51a_seg_52a_seg_53a_seg_54a_seg_55a_seg_56a_seg_57a_seg_58a_seg_59a_seg_60a_seg_61a_seg_62a_seg_63a_seg_64a_seg_65a_seg_66a_seg_67a_seg_68a_seg_69a_seg_70a_seg_71a_seg_72a_seg_73a_seg_74a_seg_75a_seg_76a_seg_77a_seg_78a_seg_79a_seg_80a_seg_81auts46datauidna\uts46data.pyu<module idna.uts46data>u.multiprocessing-postLoadtaim_selfagetattraim_classaim_funca__name__a__doc__a__file__a__spec__aoriginahas_locationa__cached__umultiprocessing.forkingTaForkingPicklerlaForkingPicklerumultiprocessing.reductionumultiprocessing-postLoada__module__wCa__qualname__wfuC.fTa_reduce_compiled_methodaregisterumultiprocessing-postLoad.pyu<module multiprocessing-postLoad>Twmu.multiprocessing-preLoada__doc__a__file__a__spec__aoriginahas_locationa__cached__asysaoslafrozenaargvlaargv0aendswithTu.exeu.exea__nuitka_binary_exeaexecutablea_base_executableumultiprocessing-preLoad.pyu<module multiprocessing-preLoad>u.requests.__version__a__doc__a__file__a__spec__aoriginahas_locationa__cached__arequestsa__title__uPython HTTP for Humans.a__description__uhttps://requests.readthedocs.ioa__url__u2.31.0a__version__l1a__build__uKenneth Reitza__author__ume@kennethreitz.orga__author_email__uApache 2.0a__license__uCopyright Kenneth Reitza__copyright__u✨ 🍰 ✨a__cake__urequests\__version__.pyu<module requests.__version__>u.requests._internal_utilsX%abuiltin_stradecodeuGiven a string object, regardless of type, returns a representation of
    that string in the native string type, encoding and decoding where
    necessary. This assumes ASCII unless told otherwise.
    aencodeTaasciiuDetermine if unicode string only contains ASCII characters.

    :param str u_string: unicode string to check. Must be unicode
        and not Python 2 `str`.
    :rtype: bool
    u
requests._internal_utils
~~~~~~~~~~~~~~

Provides utility functions that are consumed internally by Requests
which depend on extremely few external helpers (such as compat)
a__doc__a__file__a__spec__aoriginahas_locationa__cached__arelacompatTabuiltin_strlacompileTc^[^:\s][^:\r\n]*$a_VALID_HEADER_NAME_RE_BYTETu^[^:\s][^:\r\n]*$a_VALID_HEADER_NAME_RE_STRTc^\S[^\r\n]*$|^$a_VALID_HEADER_VALUE_RE_BYTETu^\S[^\r\n]*$|^$a_VALID_HEADER_VALUE_RE_STRa_HEADER_VALIDATORS_STRa_HEADER_VALIDATORS_BYTEaHEADER_VALIDATORSato_native_stringaunicode_is_asciiurequests\_internal_utils.pyu<module requests._internal_utils>TastringaencodingaoutTau_stringu.requests.adapters.aInvalidSchemaTuMissing dependencies for SOCKS support.a__class__a__init__uSends PreparedRequest object. Returns Response object.

        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.
        :param stream: (optional) Whether to stream the request content.
        :param timeout: (optional) How long to wait for the server to send
            data before giving up, as a float, or a :ref:`(connect timeout,
            read timeout) <timeouts>` tuple.
        :type timeout: float or tuple
        :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use
        :param cert: (optional) Any user-provided SSL certificate to be trusted.
        :param proxies: (optional) The proxies dictionary to apply to the request.
        uCleans up adapter specific items.aDEFAULT_RETRIESaRetryTlFTareadamax_retriesafrom_intaselfaconfigaproxy_managera_pool_connectionsa_pool_maxsizea_pool_blockainit_poolmanagerTablocka__attrs__aitemsutoo many values to unpack (expected 2)aPoolManageranum_poolsamaxsizeablockapoolmanageruInitializes a urllib3 PoolManager.

        This method should not be called from user code, and is only
        exposed for use when subclassing the
        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

        :param connections: The number of urllib3 connection pools to cache.
        :param maxsize: The maximum number of connections to save in the pool.
        :param block: Block when no free connections are available.
        :param pool_kwargs: Extra keyword arguments used to initialize the Pool Manager.
        alowerastartswithTasocksaget_auth_from_urlaSOCKSProxyManagerausernameapasswordaproxy_headersaproxy_from_urluReturn urllib3 ProxyManager for the given proxy.

        This method should not be called from user code, and is only
        exposed for use when subclassing the
        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

        :param proxy: The proxy to return a urllib3 ProxyManager for.
        :param proxy_kwargs: Extra keyword arguments used to configure the Proxy Manager.
        :returns: ProxyManager
        :rtype: urllib3.ProxyManager
        Tahttpsaextract_zipped_pathsaDEFAULT_CA_BUNDLE_PATHaosapathaexistsuCould not find a suitable TLS CA certificate bundle, invalid path: acert_locuaCERT_REQUIREDacert_reqsaisdiraca_certsaca_cert_diraCERT_NONEabasestringlaconnacert_filelakey_fileuCould not find the TLS certificate file, invalid path: uCould not find the TLS key file, invalid path: uVerify a SSL certificate. This method should not be called from user
        code, and is only exposed for use when subclassing the
        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

        :param conn: The urllib3 connection object associated with the cert.
        :param url: The requested URL.
        :param verify: Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use
        :param cert: The SSL certificate to verify.
        aResponseastatusastatus_codeaCaseInsensitiveDictaheadersaget_encoding_from_headersaencodingarawareasonaurladecodeTuutf-8aextract_cookies_to_jaracookiesarequestaconnectionuBuilds a :class:`Response <requests.Response>` object from a urllib3
        response. This should not be called from user code, and is only exposed
        for use when subclassing the
        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`

        :param req: The :class:`PreparedRequest <PreparedRequest>` used to generate the response.
        :param resp: The urllib3 response object.
        :rtype: requests.Response
        aselect_proxyaprepend_scheme_if_neededahttpaparse_urlahostaInvalidProxyURLTuPlease check proxy URL. It is malformed and could be missing the host.aproxy_manager_foraconnection_from_urlaurlparseageturluReturns a urllib3 connection for the given URL. This should not be
        called from user code, and is only exposed for use when subclassing the
        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

        :param url: The URL to connect to.
        :param proxies: (optional) A Requests-style dictionary of proxies used on this request.
        :rtype: urllib3.ConnectionPool
        aclearavaluesuDisposes of any internal state.

        Currently, this closes the PoolManager and any active ProxyManager,
        which closes any pooled connections.
        aschemeahttpsapath_urlaurldefragauthuObtain the url to use when making the final request.

        If the message is being sent through a HTTP proxy, the full URL has to
        be used. Otherwise, we should only use the path portion of the URL.

        This should not be called from user code, and is only exposed for use
        when subclassing the
        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.
        :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs.
        :rtype: str
        a_basic_auth_struProxy-AuthorizationuReturns a dictionary of the headers to add to any request sent
        through a proxy. This works with urllib3 magic to ensure that they are
        correctly sent to the proxy, rather than in a tunnelled request if
        CONNECT is being used.

        This should not be called from user code, and is only exposed for use
        when subclassing the
        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

        :param proxy: The url of the proxy being used for this request.
        :rtype: dict
        aget_connectionaLocationValueErroraInvalidURLTarequestacert_verifyarequest_urlaadd_headersTastreamatimeoutaverifyacertaproxiesabodyuContent-LengthaTimeoutSauceTaconnectareaduInvalid timeout u. Pass a (connect, read) timeout tuple, or a single float to set both timeouts to the same value.aurlopenamethodatimeoutTamethodaurlabodyaheadersaredirectaassert_same_hostapreload_contentadecode_contentaretriesatimeoutachunkedaProtocolErroraConnectionErroraMaxRetryErroraConnectTimeoutErroraNewConnectionErroraConnectTimeoutaResponseErroraRetryErrora_ProxyErroraProxyErrora_SSLErroraSSLErroraClosedPoolErrora_HTTPErroraReadTimeoutErroraReadTimeouta_InvalidHeaderaInvalidHeaderabuild_responseuSends PreparedRequest object. Returns Response object.

        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.
        :param stream: (optional) Whether to stream the request content.
        :param timeout: (optional) How long to wait for the server to send
            data before giving up, as a float, or a :ref:`(connect timeout,
            read timeout) <timeouts>` tuple.
        :type timeout: float or tuple or urllib3 Timeout object
        :param verify: (optional) Either a boolean, in which case it controls whether
            we verify the server's TLS certificate, or a string, in which case it
            must be a path to a CA bundle to use
        :param cert: (optional) Any user-provided SSL certificate to be trusted.
        :param proxies: (optional) The proxies dictionary to apply to the request.
        :rtype: requests.Response
        u
requests.adapters
~~~~~~~~~~~~~~~~~

This module contains the transport adapters that Requests uses to define
and maintain connections.
a__doc__a__file__a__spec__aoriginahas_locationa__cached__uos.pathasocketuurllib3.exceptionsTaClosedPoolErroraConnectTimeoutErrorTaHTTPErroraHTTPErrorTaInvalidHeaderTaLocationValueErroraMaxRetryErroraNewConnectionErroraProtocolErrorTaProxyErrorTaReadTimeoutErroraResponseErrorTaSSLErroruurllib3.poolmanagerTaPoolManageraproxy_from_urluurllib3.utilTaTimeoutaTimeoutTaparse_urluurllib3.util.retryTaRetryaauthTa_basic_auth_stracompatTabasestringaurlparseTaextract_cookies_to_jaraexceptionsT
aConnectionErroraConnectTimeoutaInvalidHeaderaInvalidProxyURLaInvalidSchemaaInvalidURLaProxyErroraReadTimeoutaRetryErroraSSLErroramodelsTaResponseastructuresTaCaseInsensitiveDictautilsTaDEFAULT_CA_BUNDLE_PATHaextract_zipped_pathsaget_auth_from_urlaget_encoding_from_headersaprepend_scheme_if_neededaselect_proxyaurldefragauthuurllib3.contrib.socksTaSOCKSProxyManageraDEFAULT_POOLBLOCKl
aDEFAULT_POOLSIZEaDEFAULT_POOL_TIMEOUTurequests.adaptersa__module__uThe Base Transport AdapteraBaseAdaptera__qualname__uBaseAdapter.__init__TFntnnasenduBaseAdapter.sendacloseuBaseAdapter.closeTa__prepare__aHTTPAdaptera__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>uThe built-in HTTP Adapter for urllib3.

    Provides a general-case interface for Requests sessions to contact HTTP and
    HTTPS urls by implementing the Transport Adapter interface. This class will
    usually be created by the :class:`Session <Session>` class under the
    covers.

    :param pool_connections: The number of urllib3 connection pools to cache.
    :param pool_maxsize: The maximum number of connections to save in the pool.
    :param max_retries: The maximum number of retries each connection
        should attempt. Note, this applies only to failed DNS lookups, socket
        connections and connection timeouts, never to requests where data has
        made it to the server. By default, Requests does not retry failed
        connections. If you need granular control over the conditions under
        which we retry a request, import urllib3's ``Retry`` class and pass
        that instead.
    :param pool_block: Whether the connection pool should block for connections.

    Usage::

      >>> import requests
      >>> s = requests.Session()
      >>> a = requests.adapters.HTTPAdapter(max_retries=3)
      >>> s.mount('http://', a)
    Lamax_retriesaconfiga_pool_connectionsa_pool_maxsizea_pool_blockuHTTPAdapter.__init__a__getstate__uHTTPAdapter.__getstate__a__setstate__uHTTPAdapter.__setstate__uHTTPAdapter.init_poolmanageruHTTPAdapter.proxy_manager_foruHTTPAdapter.cert_verifyuHTTPAdapter.build_responseTnuHTTPAdapter.get_connectionuHTTPAdapter.closeuHTTPAdapter.request_urluAdd any headers needed by the connection. As of v2.0 this does
        nothing by default, but is left for overriding by users that subclass
        the :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

        This should not be called from user code, and is only exposed for use
        when subclassing the
        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

        :param request: The :class:`PreparedRequest <PreparedRequest>` to add headers to.
        :param kwargs: The keyword arguments from the call to send().
        uHTTPAdapter.add_headersuHTTPAdapter.proxy_headersuHTTPAdapter.senda__orig_bases__urequests\adapters.pyu<module requests.adapters>Ta__class__TaargsakwargsTaselfTaselfa__class__Taselfapool_connectionsapool_maxsizeamax_retriesapool_blocka__class__TaselfastateaattravalueTaselfarequestakwargsTaselfareqaresparesponseTaselfaconnaurlaverifyacertacert_locTaselfaproxyTaselfaurlaproxiesaproxyaproxy_urlaproxy_manageraconnaparsedTaselfaconnectionsamaxsizeablockapool_kwargsTaselfaproxyaheadersausernameapasswordTaselfaproxyaproxy_kwargsamanagerausernameapasswordaproxy_headersTaselfarequestaproxiesaproxyaschemeais_proxied_http_requestausing_socks_proxyaproxy_schemeaurlTaselfarequestastreamatimeoutaverifyacertaproxiesTaselfarequestastreamatimeoutaverifyacertaproxiesaconnweaurlachunkedaconnectareadarespaerr.requests.apid/asessionsaSessiona__enter__a__exit__arequestamethodaurlTnnnuConstructs and sends a :class:`Request <Request>`.

    :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.
    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary, list of tuples or bytes to send
        in the query string for the :class:`Request`.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.
    :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.
    :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.
        ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')``
        or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content-type'`` is a string
        defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers
        to add for the file.
    :param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.
    :param timeout: (optional) How many seconds to wait for the server to send data
        before giving up, as a float, or a :ref:`(connect timeout, read
        timeout) <timeouts>` tuple.
    :type timeout: float or tuple
    :param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``.
    :type allow_redirects: bool
    :param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.
    :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use. Defaults to ``True``.
    :param stream: (optional) if ``False``, the response content will be immediately downloaded.
    :param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response

    Usage::

      >>> import requests
      >>> req = requests.request('GET', 'https://httpbin.org/get')
      >>> req
      <Response [200]>
    agetaparamsuSends a GET request.

    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary, list of tuples or bytes to send
        in the query string for the :class:`Request`.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    aoptionsuSends an OPTIONS request.

    :param url: URL for the new :class:`Request` object.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    aallow_redirectsaheaduSends a HEAD request.

    :param url: URL for the new :class:`Request` object.
    :param \*\*kwargs: Optional arguments that ``request`` takes. If
        `allow_redirects` is not provided, it will be set to `False` (as
        opposed to the default :meth:`request` behavior).
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    apostadataajsonuSends a POST request.

    :param url: URL for the new :class:`Request` object.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    aputuSends a PUT request.

    :param url: URL for the new :class:`Request` object.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    apatchuSends a PATCH request.

    :param url: URL for the new :class:`Request` object.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    adeleteuSends a DELETE request.

    :param url: URL for the new :class:`Request` object.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    u
requests.api
~~~~~~~~~~~~

This module implements the Requests API.

:copyright: (c) 2012 by Kenneth Reitz.
:license: Apache2, see LICENSE for more details.
a__doc__a__file__a__spec__aoriginahas_locationa__cached__uTasessionsllTnTnnurequests\api.pyu<module requests.api>TaurlakwargsTaurlaparamsakwargsTaurladataakwargsTaurladataajsonakwargsTamethodaurlakwargsasession.requests.authabasestringawarningsawarnuNon-string usernames will no longer be supported in Requests 3.0.0. Please convert the object you've passed in ({!r}) to a string or bytes object in the near future to avoid problems.aDeprecationWarningTacategoryastruNon-string passwords will no longer be supported in Requests 3.0.0. Please convert the object you've passed in ({!r}) to a string or bytes object in the near future to avoid problems.aencodeTalatin1uBasic ato_native_stringab64encoded:astripuReturns a Basic Auth string.uAuth hooks must be callable.ausernameapassworda_basic_auth_straheadersaAuthorizationuProxy-Authorizationathreadingalocala_thread_localainitualast_noncelanonce_countachalaposanum_401_callsarealmanonceagetTaqopTaalgorithmTaopaqueaMD5aupperuMD5-SESSamd5_utf8uHTTPDigestAuth.build_digest_header.<locals>.md5_utf8aSHAasha_utf8uHTTPDigestAuth.build_digest_header.<locals>.sha_utf8uSHA-256asha256_utf8uHTTPDigestAuth.build_digest_header.<locals>.sha256_utf8uSHA-512asha512_utf8uHTTPDigestAuth.build_digest_header.<locals>.sha512_utf8u<lambda>uHTTPDigestAuth.build_digest_header.<locals>.<lambda>aurlparseapathw/aqueryw?w:lu08xTuutf-8atimeactimeaosaurandomTlahashlibasha1ahexdigest:nlnaauthasplitTw,u:auth:uusername="u", realm="u", nonce="u", uri="u", response="w"u, opaque="aalgorithmu, algorithm="aqopu, qop="auth", nc=u, cnonce="uDigest u
        :rtype: str
        amd5asha256asha512ahash_utf8ais_redirectuReset num_401_calls counter on redirects.astatus_codellarequestabodyaseekTuwww-authenticateuadigestalowerlareacompileaIGNORECASETudigest Taflagsaparse_dict_headerasubDacountlacontentacloseacopyaextract_cookies_to_jara_cookiesarawaprepare_cookiesabuild_digest_headeramethodaurlaconnectionasendahistoryaappendu
        Takes the given response and tries digest-auth, if needed.

        :rtype: requests.Response
        ainit_per_thread_statewratellaregister_hookaresponseahandle_401ahandle_redirectu
requests.auth
~~~~~~~~~~~~~

This module contains the authentication handlers for Requests.
a__doc__a__file__a__spec__aoriginahas_locationa__cached__abase64Tab64encodea_internal_utilsTato_native_stringacompatTabasestringastraurlparseacookiesTaextract_cookies_to_jarautilsTaparse_dict_headeruapplication/x-www-form-urlencodedaCONTENT_TYPE_FORM_URLENCODEDumultipart/form-dataaCONTENT_TYPE_MULTI_PARTurequests.autha__module__uBase class that all auth implementations derive fromaAuthBasea__qualname__a__call__uAuthBase.__call__Ta__prepare__aHTTPBasicAutha__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>uAttaches HTTP Basic Authentication to the given Request object.a__init__uHTTPBasicAuth.__init__a__eq__uHTTPBasicAuth.__eq__a__ne__uHTTPBasicAuth.__ne__uHTTPBasicAuth.__call__a__orig_bases__aHTTPProxyAuthuAttaches HTTP Proxy Authentication to a given Request object.uHTTPProxyAuth.__call__aHTTPDigestAuthuAttaches HTTP Digest Authentication to the given Request object.uHTTPDigestAuth.__init__uHTTPDigestAuth.init_per_thread_stateuHTTPDigestAuth.build_digest_headeruHTTPDigestAuth.handle_redirectuHTTPDigestAuth.handle_401uHTTPDigestAuth.__call__uHTTPDigestAuth.__eq__uHTTPDigestAuth.__ne__urequests\auth.pyTwswdahash_utf8Tahash_utf8u<module requests.auth>Ta__class__TaselfwrTaselfaotherTaselfausernameapasswordTausernameapasswordaauthstrTaselfamethodaurlarealmanonceaqopaalgorithmaopaqueahash_utf8a_algorithmamd5_utf8asha_utf8asha256_utf8asha512_utf8aKDaentdigap_parsedapathaA1aA2aHA1aHA2ancvaluewsacnoncearespdiganoncebitabaseTaselfwrakwargsas_authapataprepa_rTaselfwrakwargsTaselfTwx.requests.certsu
requests.certs
~~~~~~~~~~~~~~

This module returns the preferred default CA certificate bundle. There is
only one — the one from the certifi package.

If you are packaging Requests, e.g., for a Linux distribution or a managed
environment, you can change the definition of where() to return a separately
packaged CA bundle.
a__doc__a__file__a__spec__aoriginahas_locationa__cached__acertifiTawherelawhereurequests\certs.pyu<module requests.certs>u.requests.compatCu
requests.compat
~~~~~~~~~~~~~~~

This module previously handled import compatibility issues
between Python 2 and Python 3. It remains for backwards
compatibility until the next major version.
a__doc__a__file__a__spec__aoriginahas_locationa__cached__achardetlacharset_normalizerasysa_verlais_py2lais_py3ahas_simplejsonasimplejsonajsonTaJSONDecodeErroraJSONDecodeErroracollectionsTaOrderedDictaOrderedDictucollections.abcTaCallableaMappingaMutableMappingaCallableaMappingaMutableMappingahttpTacookiejaracookiejaracookielibuhttp.cookiesTaMorselaMorselaStringIOuurllib.parseT
aquoteaquote_plusaunquoteaunquote_plusaurldefragaurlencodeaurljoinaurlparseaurlsplitaurlunparseaquoteaquote_plusaunquoteaunquote_plusaurldefragaurlencodeaurljoinaurlparseaurlsplitaurlunparseuurllib.requestTagetproxiesagetproxies_environmentaparse_http_listaproxy_bypassaproxy_bypass_environmentagetproxiesagetproxies_environmentaparse_http_listaproxy_bypassaproxy_bypass_environmentastrabuiltin_strabytesabasestringTOintOfloatanumeric_typesTOintainteger_typesurequests\compat.pyu<module requests.compat>u.requests1wasplitTw.adevaappendTw0utoo many values to unpack (expected 3)ll:nlnTlllTllpTllpTllpuYou need either charset_normalizer or chardet installedlluOld version of cryptography ({}) may cause slowdown.awarningsawarnaRequestsDependencyWarningu
Requests HTTP Library
~~~~~~~~~~~~~~~~~~~~~

Requests is an HTTP library, written in Python, for human beings.
Basic GET usage:

   >>> import requests
   >>> r = requests.get('https://www.python.org')
   >>> r.status_code
   200
   >>> b'Python is a programming language' in r.content
   True

... or POST:

   >>> payload = dict(key1='value1', key2='value2')
   >>> r = requests.post('https://httpbin.org/post', data=payload)
   >>> print(r.text)
   {
     ...
     "form": {
       "key1": "value1",
       "key2": "value2"
     },
     ...
   }

The other HTTP methods are supported - see `requests.api`. Full documentation
is at <https://requests.readthedocs.io>.

:copyright: (c) 2017 by Kenneth Reitz.
:license: Apache 2.0, see LICENSE for more details.
a__doc__a__file__apathadirnameaenvironagetTaNUITKA_PACKAGE_requestsu\not_existinga__path__a__spec__aoriginahas_locationasubmodule_search_locationsa__cached__laurllib3aexceptionsTaRequestsDependencyWarningacharset_normalizerTa__version__a__version__acharset_normalizer_versionachardetachardet_versionacheck_compatibilitya_check_cryptographyTEAssertionErrorEValueErroruurllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported version!asslaHAS_SNIuurllib3.contribTapyopensslapyopensslainject_into_urllib3acryptographyacryptography_versionuurllib3.exceptionsTaDependencyWarningaDependencyWarningasimplefilteraignorealoggingTaNullHandleraNullHandleruTapackagesautilsapackagesautilsT
a__author__a__author_email__a__build__a__cake__a__copyright__a__description__a__license__a__title__a__url__a__version__a__author__a__author_email__a__build__a__cake__a__copyright__a__description__a__license__a__title__a__url__aapiTadeleteagetaheadaoptionsapatchapostaputarequestadeleteaheadaoptionsapatchapostaputarequestT
aConnectionErroraConnectTimeoutaFileModeWarningaHTTPErroraJSONDecodeErroraReadTimeoutaRequestExceptionaTimeoutaTooManyRedirectsaURLRequiredaConnectionErroraConnectTimeoutaFileModeWarningaHTTPErroraJSONDecodeErroraReadTimeoutaRequestExceptionaTimeoutaTooManyRedirectsaURLRequiredamodelsTaPreparedRequestaRequestaResponseaPreparedRequestaRequestaResponseasessionsTaSessionasessionaSessionasessionastatus_codesTacodesacodesagetLoggerTarequestsaaddHandleradefaultDaappendturequests\__init__.pyu<module requests>Tacryptography_versionawarningTaurllib3_versionachardet_versionacharset_normalizer_versionamajoraminorapatch.requests.cookies_+a_ra_new_headersaurlparseaurlaschemeatypeanetlocaget_hostaheadersagetTaHostato_native_stringaHostDaencodinguutf-8aurlunparseapathaparamsaqueryafragmentuCookie headers should be added with add_unredirected_header()ucookielib has no legitimate use for this method; add it back if you find one.ais_unverifiableaget_origin_req_hosta_headersuMake a MockResponse for `cookielib` to read.

        :param headers: a httplib.HTTPMessage or analogous carrying the headers
        agetheadersa_original_responseaMockRequestaMockResponsearesponseamsgaextract_cookiesuExtract the cookies from the response into a CookieJar.

    :param jar: cookielib.CookieJar (not necessarily a RequestsCookieJar)
    :param request: our own requests.Request object
    :param response: urllib3.HTTPResponse object
    aadd_cookie_headeraget_new_headersTaCookieu
    Produce an appropriate Cookie header string to be sent with `request`, or None.

    :rtype: str
    anameadomainaclearablesaappendutoo many values to unpack (expected 3)acookiejaraclearuUnsets a cookie by name, by default over all domains and paths.

    Wraps CookieJar.clear(), is O(n).
    a_find_no_duplicatesuDict-like get() that also supports optional domain and path args in
        order to resolve naming collisions from using one cookie jar over
        multiple domains.

        .. warning:: operation is O(n), not O(1).
        aremove_cookie_by_nameTadomainapathaMorselamorsel_to_cookieacreate_cookieaset_cookieuDict-like set() that also supports optional domain and path args in
        order to resolve naming collisions from using one cookie jar over
        multiple domains.
        uDict-like iterkeys() that returns an iterator of names of cookies
        from the jar.

        .. seealso:: itervalues() and iteritems().
        aselfaiterkeysuRequestsCookieJar.iterkeysuDict-like keys() that returns a list of names of cookies from the
        jar.

        .. seealso:: values() and items().
        uDict-like itervalues() that returns an iterator of values of cookies
        from the jar.

        .. seealso:: iterkeys() and iteritems().
        avalueaitervaluesuRequestsCookieJar.itervaluesuDict-like values() that returns a list of values of cookies from the
        jar.

        .. seealso:: keys() and items().
        uDict-like iteritems() that returns an iterator of name-value tuples
        from the jar.

        .. seealso:: iterkeys() and itervalues().
        aiteritemsuRequestsCookieJar.iteritemsuDict-like items() that returns a list of name-value tuples from the
        jar. Allows client-code to call ``dict(RequestsCookieJar)`` and get a
        vanilla python dict of key value pairs.

        .. seealso:: keys() and values().
        adomainsuUtility method to list all the domains in the jar.apathsuUtility method to list all the paths in the jar.uReturns True if there are multiple domains in the jar.
        Returns False otherwise.

        :rtype: bool
        acookieadictionaryuTakes as an argument an optional domain and path and returns a plain
        old Python dict of name-value pairs of cookies that meet the
        requirements.

        :rtype: dict
        a__class__a__contains__aCookieConflictErroruDict-like __getitem__() for compatibility with client code. Throws
        exception if there are more than one cookie with name. In that case,
        use the more explicit get() method instead.

        .. warning:: operation is O(n), not O(1).
        asetuDict-like __setitem__ for compatibility with client code. Throws
        exception if there is already a cookie of that name in the jar. In that
        case, use the more explicit set() method instead.
        uDeletes a cookie given a name. Wraps ``cookielib.CookieJar``'s
        ``remove_cookie_by_name()``.
        astartswithTw"aendswithareplaceTu\"uacookielibaCookieJaracopyaupdateuUpdates this jar with cookies from another CookieJar or dict-likeuname=uu, domain=u, path=uRequests uses this method internally to get cookie values.

        If there are conflicting cookies, _find arbitrarily chooses one.
        See _find_no_duplicates if you want an exception thrown if there are
        conflicting cookies.

        :param name: a string containing name of cookie
        :param domain: (optional) string containing domain of cookie
        :param path: (optional) string containing path of cookie
        :return: cookie.value
        atoReturnuThere are multiple cookies with name, uBoth ``__get_item__`` and ``get`` call this function: it's never
        used elsewhere in Requests.

        :param name: a string containing name of cookie
        :param domain: (optional) string containing domain of cookie
        :param path: (optional) string containing path of cookie
        :raises KeyError: if cookie is not found
        :raises CookieConflictError: if there are multiple cookies
            that match name and optionally domain and path
        :return: cookie.value
        apopTa_cookies_lockuUnlike a normal CookieJar, this class is pickleable.a_cookies_lockathreadingaRLockaRequestsCookieJaraset_policyaget_policyuReturn a copy of this RequestsCookieJar.a_policyuReturn the CookiePolicy instance used.anew_jaraversionlaportw/asecureaexpiresadiscardacommentacomment_urlarestDaHttpOnlynarfc2109ucreate_cookie() got unexpected keyword arguments: aport_specifiedadomain_specifiedTw.adomain_initial_dotapath_specifiedaCookieuMake a cookie from underspecified parameters.

    By default, the pair of `name` and `value` will be set for the domain ''
    and sent on every request (this is sometimes called a "supercookie").
    umax-ageatimeumax-age: u must be integeracalendaratimegmastrptimeu%a, %d-%b-%Y %H:%M:%S GMTakeyaHttpOnlyahttponlyT
acommentacomment_urladiscardadomainaexpiresanameapathaportarestarfc2109asecureavalueaversionuConvert a Morsel object into a Cookie containing the one k/v pair.uReturns a CookieJar from a key/value dictionary.

    :param cookie_dict: Dict of key/values to insert into CookieJar.
    :param cookiejar: (optional) A cookiejar to add the cookies to.
    :param overwrite: (optional) If False, will not replace cookies
        already in the jar with new ones.
    :rtype: CookieJar
    uYou can only merge into CookieJaracookiejar_from_dictTacookiejaraoverwriteacookiesuAdd cookies to cookiejar and returns a merged CookieJar.

    :param cookiejar: CookieJar object to add the cookies to.
    :param cookies: Dictionary or CookieJar object to be added.
    :rtype: CookieJar
    u
requests.cookies
~~~~~~~~~~~~~~~~

Compatibility code to be able to use `cookielib.CookieJar` with requests.

requests.utils imports from here, so be careful with imports.
a__doc__a__file__a__spec__aoriginahas_locationa__cached__a_internal_utilsTato_native_stringlacompatTaMorselaMutableMappingacookielibaurlparseaurlunparseaMutableMappingadummy_threadingurequests.cookiesa__module__uWraps a `requests.Request` to mimic a `urllib2.Request`.

    The code in `cookielib.CookieJar` expects this interface in order to correctly
    manage cookie policies, i.e., determine whether a cookie can be set, given the
    domains of the request and the cookie.

    The original request object is read-only. The client is responsible for collecting
    the new headers via `get_new_headers()` and interpreting them appropriately. You
    probably want `get_cookie_header`, defined below.
    a__qualname__a__init__uMockRequest.__init__aget_typeuMockRequest.get_typeuMockRequest.get_hostuMockRequest.get_origin_req_hostaget_full_urluMockRequest.get_full_urluMockRequest.is_unverifiableahas_headeruMockRequest.has_headerTnaget_headeruMockRequest.get_headeraadd_headeruMockRequest.add_headeraadd_unredirected_headeruMockRequest.add_unredirected_headeruMockRequest.get_new_headersaunverifiableuMockRequest.unverifiableaorigin_req_hostuMockRequest.origin_req_hostahostuMockRequest.hostTuWraps a `httplib.HTTPMessage` to mimic a `urllib.addinfourl`.

    ...what? Basically, expose the parsed HTTP headers from the server response
    the way `cookielib` expects to see them.
    uMockResponse.__init__ainfouMockResponse.infouMockResponse.getheadersaextract_cookies_to_jaraget_cookie_headerTnnTERuntimeErrora__prepare__a__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>uThere are two cookies that meet the criteria specified in the cookie jar.
    Use .get and .set and include domain and path args in order to be more specific.
    a__orig_bases__uCompatibility class; is a cookielib.CookieJar, but exposes a dict
    interface.

    This is the CookieJar we create by default for requests and sessions that
    don't specify one, since some clients may expect response.cookies and
    session.cookies to support dict operations.

    Requests does not use the dict interface internally; it's just for
    compatibility with external client code. All requests code should work
    out of the box with externally provided instances of ``CookieJar``, e.g.
    ``LWPCookieJar`` and ``FileCookieJar``.

    Unlike a regular CookieJar, this class is pickleable.

    .. warning:: dictionary operations that are normally O(1) may be O(n).
    TnnnuRequestsCookieJar.getuRequestsCookieJar.setakeysuRequestsCookieJar.keysavaluesuRequestsCookieJar.valuesaitemsuRequestsCookieJar.itemsalist_domainsuRequestsCookieJar.list_domainsalist_pathsuRequestsCookieJar.list_pathsamultiple_domainsuRequestsCookieJar.multiple_domainsaget_dictuRequestsCookieJar.get_dictuRequestsCookieJar.__contains__uRequestsCookieJar.__getitem__a__setitem__uRequestsCookieJar.__setitem__a__delitem__uRequestsCookieJar.__delitem__uRequestsCookieJar.set_cookieuRequestsCookieJar.updatea_finduRequestsCookieJar._finduRequestsCookieJar._find_no_duplicatesa__getstate__uRequestsCookieJar.__getstate__a__setstate__uRequestsCookieJar.__setstate__uRequestsCookieJar.copyuRequestsCookieJar.get_policya_copy_cookie_jarTntamerge_cookiesurequests\cookies.pyu<module requests.cookies>Ta__class__Taselfanamea__class__TaselfanameTaselfastateTaselfaheadersTaselfarequestTaselfanameavalueTajaranew_jaracookieTaselfanameadomainapathacookieTaselfanameadomainapathatoReturnacookieTaselfakeyavalTacookie_dictacookiejaraoverwriteanames_from_jaranameTaselfanew_cjTanameavalueakwargsaresultabadargsTajararequestaresponseareqaresTaselfanameadefaultadomainapathTajararequestwrTaselfadomainapathadictionaryacookieTaselfahostaparsedTaselfanameadefaultTaselfTaselfacookieTaselfadomainsacookieTaselfapathsacookieTacookiejaracookiesacookie_in_jarTamorselaexpiresatime_templateTacookiejaranameadomainapathaclearablesacookieTaselfanameavalueakwargswcTaselfacookieaargsakwargsa__class__Taselfaotheracookiea__class__.requests.exceptions
^aresponseapopTarequestnarequesta__class__a__init__uInitialize RequestException with `request` and `response` objects.aCompatJSONDecodeErroraInvalidJSONErroraargsu
        Construct the JSONDecodeError instance first with all
        args. Then use it's args to construct the IOError so that
        the json specific args aren't used as IOError specific args
        and the error message from JSONDecodeError is preserved.
        u
requests.exceptions
~~~~~~~~~~~~~~~~~~~

This module contains the set of Requests' exceptions.
a__doc__a__file__a__spec__aoriginahas_locationa__cached__uurllib3.exceptionsTaHTTPErrorlaHTTPErroraBaseHTTPErroracompatTaJSONDecodeErrorlaJSONDecodeErrorTEOSErrora__prepare__aRequestExceptiona__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>urequests.exceptionsa__module__uThere was an ambiguous exception that occurred while handling your
    request.
    a__qualname__uRequestException.__init__a__orig_bases__uA JSON error occurred.uCouldn't decode the text into jsonuJSONDecodeError.__init__uAn HTTP error occurred.aConnectionErroruA Connection error occurred.aProxyErroruA proxy error occurred.aSSLErroruAn SSL error occurred.aTimeoutuThe request timed out.

    Catching this error will catch both
    :exc:`~requests.exceptions.ConnectTimeout` and
    :exc:`~requests.exceptions.ReadTimeout` errors.
    aConnectTimeoutuThe request timed out while trying to connect to the remote server.

    Requests that produced this error are safe to retry.
    aReadTimeoutuThe server did not send any data in the allotted amount of time.aURLRequireduA valid URL is required to make a request.aTooManyRedirectsuToo many redirects.aMissingSchemauThe URL scheme (e.g. http or https) is missing.aInvalidSchemauThe URL scheme provided is either invalid or unsupported.aInvalidURLuThe URL provided was somehow invalid.aInvalidHeaderuThe header value provided was somehow invalid.aInvalidProxyURLuThe proxy URL provided is invalid.aChunkedEncodingErroruThe server declared chunked encoding but sent an invalid chunk.aContentDecodingErroruFailed to decode response content.aStreamConsumedErroruThe content for this response was already consumed.aRetryErroruCustom retries logic failedaUnrewindableBodyErroruRequests encountered an error when trying to rewind a body.aWarningaRequestsWarninguBase warning for Requests.aDeprecationWarningaFileModeWarninguA file was opened in text mode, but Requests determined its binary length.aRequestsDependencyWarninguAn imported dependency doesn't match the expected version range.urequests\exceptions.pyu<module requests.exceptions>Ta__class__TaselfaargsakwargsTaselfaargsakwargsaresponsea__class__u.requests.hooksaHOOKSageta__call__ahook_datauDispatches a hook dictionary on a given piece of data.u
requests.hooks
~~~~~~~~~~~~~~

This module provides the capabilities for the Requests hooks system.

Available hooks:

``response``:
    The response generated from a Request.
a__doc__a__file__a__spec__aoriginahas_locationa__cached__aresponseadefault_hooksadispatch_hookurequests\hooks.pyu<module requests.hooks>Takeyahooksahook_dataakwargsahooka_hook_datau.requests.models6aurlsplitaurlapathw/aqueryw?uuBuild the path URL to use.TOstrObytesareada__iter__ato_key_val_listutoo many values to unpack (expected 2)abasestringaresultaappendwkaencodeTuutf-8aurlencodeDadoseqtuEncode parameters in a piece of data.

        Will successfully encode parameters when passed as a dict or a list of
        2-tuples. Order is retained if data is a list of 2-tuples but arbitrary
        if parameters are supplied as a dict.
        uFiles must be provided.uData must not be a string.anew_fieldsafieldadecodeTOtupleOlistutoo many values to unpack (expected 3)utoo many values to unpack (expected 4)aguess_filenameTOstrObytesObytearrayaRequestFieldTanameadataafilenameaheadersamake_multipartTacontent_typeaencode_multipart_formdatauBuild the body for a multipart/form-data request.

        Will successfully encode files when passed as a dict or a list of
        tuples. Order is retained if data is a list of tuples but arbitrary
        if parameters are supplied as a dict.
        The tuples may be 2-tuples (filename, fileobj), 3-tuples (filename, fileobj, contentype)
        or 4-tuples (filename, fileobj, contentype, custom_headers).
        ahooksuUnsupported event specified, with event name "w"aCallableaextenduProperly register a hook.u<genexpr>uRequestHooksMixin.register_hook.<locals>.<genexpr>aremoveuDeregister a previously registered hook.
        Returns True if the hook existed, False if not.
        adefault_hooksaitemsaselfaregister_hookTaeventahookamethodaheadersafilesadataajsonaparamsaauthacookiesu<Request [u]>aPreparedRequestaprepareT
amethodaurlaheadersafilesadataajsonaparamsaauthacookiesahooksuConstructs a :class:`PreparedRequest <PreparedRequest>` for transmission and returns it.a_cookiesabodya_body_positionaprepare_methodaprepare_urlaprepare_headersaprepare_cookiesaprepare_bodyaprepare_authaprepare_hooksuPrepares the entire request with the given parameters.u<PreparedRequest [acopya_copy_cookie_jarato_native_stringaupperuPrepares the given HTTP method.aidnalDauts46taIDNAErrorTautf8alstripw:alowerastartswithTahttpaparse_urlutoo many values to unpack (expected 7)aLocationParseErroraInvalidURLaargsaMissingSchemauInvalid URL u: No scheme supplied. Perhaps you meant https://u: No host suppliedaunicode_is_asciia_get_idna_encoded_hostTuURL has an invalid label.TTw*w.w@ahosta_encode_paramsw&arequote_uriaurlunparseuPrepares the given HTTP URL.aCaseInsensitiveDictacheck_header_validityuPrepares the given HTTP headers.uapplication/jsonacomplexjsonadumpsDaallow_nanFaInvalidJSONErrorTarequestaMappingasuper_lenaUnsupportedOperationatelluStreamed bodies and files are mutually exclusive.abuiltin_struContent-LengthachunkeduTransfer-Encodinga_encode_filesuapplication/x-www-form-urlencodedaprepare_content_lengthucontent-typeuContent-TypeuPrepares the given HTTP body data.TaGETaHEADagetTuContent-Lengthw0uPrepare Content-Length header based on request method and bodyaget_auth_from_urlaHTTPBasicAuthaupdateuPrepares the given HTTP auth data.acookielibaCookieJaracookiejar_from_dictaget_cookie_headeraCookieuPrepares the given HTTP cookie data.

        This function eventually generates a ``Cookie`` header from the
        given cookies using cookielib. Due to cookielib's design, the header
        will not be regenerated if it already exists, meaning this function
        can only be called once for the life of the
        :class:`PreparedRequest <PreparedRequest>` object. Any subsequent calls
        to ``prepare_cookies`` will have no actual effect, unless the "Cookie"
        header is removed beforehand.
        uPrepares the given hooks.a_contenta_content_consumeda_nextastatus_codearawaencodingahistoryareasonadatetimeatimedeltaTlaelapsedarequestacloseacontenta__attrs__u<Response [aokuReturns True if :attr:`status_code` is less than 400.

        This attribute checks if the status code of the response is between
        400 and 600 to see if there was a client error or a server error. If
        the status code, is between 200 and 400, this will return True. This
        is **not** a check to see if the response code is ``200 OK``.
        aiter_contentTluAllows you to use a response as an iterator.araise_for_statusaHTTPErroruReturns True if :attr:`status_code` is less than 400, False if not.

        This attribute checks if the status code of the response is between
        400 and 600 to see if there was a client error or a server error. If
        the status code is between 200 and 400, this will return True. This
        is **not** a check to see if the response code is ``200 OK``.
        alocationaREDIRECT_STATIuTrue if this Response is a well-formed HTTP redirect that could have
        been processed automatically (by :meth:`Session.resolve_redirects`).
        acodesamoved_permanentlyapermanent_redirectuTrue if this Response one of the permanent versions of redirect.uReturns a PreparedRequest for the next request in a redirect chain, if there is one.achardetadetectuThe apparent encoding, provided by the charset_normalizer or chardet libraries.agenerateuResponse.iter_content.<locals>.generateaStreamConsumedErroruchunk_size must be an int, it is instead a w.aiter_slicesastream_decode_response_unicodeuIterates over the response data.  When stream=True is set on the
        request, this avoids reading the content at once into memory for
        large responses.  The chunk size is the number of bytes it should
        read into memory.  This is not necessarily the length of each item
        returned as decoding can take place.

        chunk_size must be of type int or None. A value of None will
        function differently depending on the value of `stream`.
        stream=True will read data as it arrives in whatever size the
        chunks are received. If stream=False, data is returned as
        a single chunk.

        If decode_unicode is True, content will be decoded using the best
        available encoding based on the response.
        astreamachunk_sizeDadecode_contenttaProtocolErroraChunkedEncodingErroraDecodeErroraContentDecodingErroraReadTimeoutErroraConnectionErroraSSLErroraRequestsSSLErroruIterates over the response data, one line at a time.  When
        stream=True is set on the request, this avoids reading the
        content at once into memory for large responses.

        .. note:: This method is not reentrant safe.
        adecode_unicodeTachunk_sizeadecode_unicodeapendingadelimiterasplitasplitlineslachunkapopaiter_linesuResponse.iter_linesuThe content for this response was already consumedcaCONTENT_CHUNK_SIZEuContent of the response, in bytes.aapparent_encodingareplaceTELookupErrorETypeErroruContent of the response, in unicode.

        If Response.encoding is None, encoding will be guessed using
        ``charset_normalizer`` or ``chardet``.

        The encoding of the response content is determined based solely on HTTP
        headers, following RFC 2616 to the letter. If you can take advantage of
        non-HTTP knowledge to make a better guess at the encoding, you should
        set ``r.encoding`` appropriately before accessing this property.
        aguess_json_utfaloadsaJSONDecodeErroraRequestsJSONDecodeErroramsgadocaposatextuReturns the json-encoded content of a response, if any.

        :param \*\*kwargs: Optional arguments that ``json.loads`` takes.
        :raises requests.exceptions.JSONDecodeError: If the response body does not
            contain valid json.
        Talinkaparse_header_linksTarelTaurlaresolved_linksuReturns the parsed header links of the response, if any.Tuiso-8859-1llu Client Error: u for url: lXu Server Error: TaresponseuRaises :class:`HTTPError`, if one occurred.arelease_connuReleases the connection back to the pool. Once this method has been
        called the underlying ``raw`` object must not be accessed again.

        *Note: Should not normally need to be called explicitly.*
        u
requests.models
~~~~~~~~~~~~~~~

This module contains the primary objects that power Requests.
a__doc__a__file__a__spec__aoriginahas_locationa__cached__uencodings.idnaaencodingsuurllib3.exceptionsTaDecodeErroraLocationParseErroraProtocolErroraReadTimeoutErroraSSLErroruurllib3.fieldsTaRequestFielduurllib3.filepostTaencode_multipart_formdatauurllib3.utilTaparse_urla_internal_utilsTato_native_stringaunicode_is_asciilTaHTTPBasicAuthacompatTaCallableaJSONDecodeErroraMappingabasestringabuiltin_strachardetacookielibTajsonTaurlencodeaurlsplitaurlunparseTa_copy_cookie_jaracookiejar_from_dictaget_cookie_headeraexceptionsTaChunkedEncodingErroraConnectionErroraContentDecodingErroraHTTPErroraInvalidJSONErroraInvalidURLTaJSONDecodeErrorTaMissingSchemaTaSSLErrorTaStreamConsumedErrorTadefault_hooksastatus_codesTacodesastructuresTaCaseInsensitiveDictautilsT
acheck_header_validityaget_auth_from_urlaguess_filenameaguess_json_utfaiter_slicesaparse_header_linksarequote_uriastream_decode_response_unicodeasuper_lenato_key_val_listamovedafoundaotheratemporary_redirectlaDEFAULT_REDIRECT_LIMITl(laITER_CHUNK_SIZEurequests.modelsa__module__aRequestEncodingMixina__qualname__apath_urluRequestEncodingMixin.path_urluRequestEncodingMixin._encode_paramsuRequestEncodingMixin._encode_filesTaRequestHooksMixinuRequestHooksMixin.register_hookaderegister_hookuRequestHooksMixin.deregister_hooka__prepare__aRequesta__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>uA user-created :class:`Request <Request>` object.

    Used to prepare a :class:`PreparedRequest <PreparedRequest>`, which is sent to the server.

    :param method: HTTP method to use.
    :param url: URL to send.
    :param headers: dictionary of headers to send.
    :param files: dictionary of {filename: fileobject} files to multipart upload.
    :param data: the body to attach to the request. If a dictionary or
        list of tuples ``[(key, value)]`` is provided, form-encoding will
        take place.
    :param json: json for the body to attach to the request (if files or data is not specified).
    :param params: URL parameters to append to the URL. If a dictionary or
        list of tuples ``[(key, value)]`` is provided, form-encoding will
        take place.
    :param auth: Auth handler or (user, pass) tuple.
    :param cookies: dictionary or CookieJar of cookies to attach to this request.
    :param hooks: dictionary of callback hooks, for internal usage.

    Usage::

      >>> import requests
      >>> req = requests.Request('GET', 'https://httpbin.org/get')
      >>> req.prepare()
      <PreparedRequest [GET]>
    T
nnnnnnnnnna__init__uRequest.__init__a__repr__uRequest.__repr__uRequest.preparea__orig_bases__uThe fully mutable :class:`PreparedRequest <PreparedRequest>` object,
    containing the exact bytes that will be sent to the server.

    Instances are generated from a :class:`Request <Request>` object, and
    should not be instantiated manually; doing so may produce undesirable
    effects.

    Usage::

      >>> import requests
      >>> req = requests.Request('GET', 'https://httpbin.org/get')
      >>> r = req.prepare()
      >>> r
      <PreparedRequest [GET]>

      >>> s = requests.Session()
      >>> s.send(r)
      <Response [200]>
    uPreparedRequest.__init__uPreparedRequest.prepareuPreparedRequest.__repr__uPreparedRequest.copyuPreparedRequest.prepare_methodastaticmethoduPreparedRequest._get_idna_encoded_hostuPreparedRequest.prepare_urluPreparedRequest.prepare_headersTnuPreparedRequest.prepare_bodyuPreparedRequest.prepare_content_lengthTuuPreparedRequest.prepare_authuPreparedRequest.prepare_cookiesuPreparedRequest.prepare_hooksuThe :class:`Response <Response>` object, which contains a
    server's response to an HTTP request.
    aResponseL
a_contentastatus_codeaheadersaurlahistoryaencodingareasonacookiesaelapsedarequestuResponse.__init__a__enter__uResponse.__enter__a__exit__uResponse.__exit__a__getstate__uResponse.__getstate__a__setstate__uResponse.__setstate__uResponse.__repr__a__bool__uResponse.__bool__a__nonzero__uResponse.__nonzero__uResponse.__iter__uResponse.okais_redirectuResponse.is_redirectais_permanent_redirectuResponse.is_permanent_redirectanextuResponse.nextuResponse.apparent_encodingTlFuResponse.iter_contentuResponse.contentuResponse.textuResponse.jsonalinksuResponse.linksuResponse.raise_for_statusuResponse.closeurequests\models.pyTa.0whu<module requests.models>Ta__class__TaselfTaselfaargsT
aselfamethodaurlaheadersafilesadataaparamsaauthacookiesahooksajsonwkwvTaselfastateanameavalueTafilesadataanew_fieldsafieldsafieldavalwvwkaftafhafnafpafdataarfabodyacontent_typeTadataaresultwkavswvTahostaidnaTaselfarelease_connTaselfwpTaselfaeventahookTweachunkaselfachunk_sizeTachunk_sizeaselfTaselfachunk_sizeadecode_unicodeagenerateareused_chunksastream_chunksachunksTaselfachunk_sizeadecode_unicodeadelimiterapendingachunkalinesTaselfakwargsaencodingweTaselfaheaderaresolved_linksalinksalinkakeyTaselfaurlwpapathaqueryTaselfamethodaurlaheadersafilesadataaparamsaauthacookiesahooksajsonTaselfaauthaurlaurl_authwrTaselfadataafilesajsonabodyacontent_typeaveais_streamalengthTaselfabodyalengthTaselfacookiesacookie_headerTaselfaheadersaheaderanameavalueTaselfahooksaeventTaselfamethodT
aselfaurlaparamsaschemeaauthahostaportapathaqueryafragmentweanetlocaenc_paramsTaselfahttp_error_msgareasonTaselfacontentaencoding.requests.packagesVa__doc__a__file__a__spec__aoriginahas_locationa__cached__asysachardetlawarningsacharset_normalizerafilterwarningsTaignoreuTrying to detectacharset_normalizerTamoduleTaurllib3aidnaapackageamodulesamodastartswithuw.urequests.packages.a__name__atargetareplaceurequests\packages.pyu<module requests.packages>.requests.sessions336aMappingato_key_val_listaupdateaitemsutoo many values to unpack (expected 2)uDetermines appropriate setting for a given request, taking into account
    the explicit setting on that request, and the setting in the session. If a
    setting is a dictionary, they will be merged together using `dict_class`
    agetTaresponseasession_hooksamerge_settingarequest_hooksuProperly merges both requests and session hooks.

    This is necessary because when request_hooks == {'response': []}, the
    merge breaks Session hooks entirely.
    ais_redirectaheadersalocationaencodeTalatin1ato_native_stringautf8uReceives a Response. Returns a redirect URI or ``None``aurlparseahostnameaschemeahttpaportTlPnahttpsTlnaDEFAULT_PORTSuDecide whether Authorization header should be removed when redirectinguReceives a Response. Returns a generator of Responses or Requests.aselfaget_redirect_targetarespareqaurlafragmentacopyahistaappend:lnnahistoryacontentaChunkedEncodingErroraContentDecodingErrorarawareadTFTadecode_contentamax_redirectsaTooManyRedirectsuExceeded uu redirects.acloseastartswithTu//w:aprevious_fragmenta_replaceTafragmentaparsedageturlanetlocaurljoinarequote_uriarebuild_methodastatus_codeacodesatemporary_redirectapermanent_redirectTuContent-LengthuContent-TypeuTransfer-Encodingaprepared_requestapopabodyTaCookienaextract_cookies_to_jara_cookiesamerge_cookiesacookiesaprepare_cookiesarebuild_proxiesaproxiesarebuild_autha_body_positionuContent-LengthuTransfer-Encodingarewind_bodyayield_requestsasendastreamatimeoutaverifyacertaallow_redirectsaadapter_kwargsaresolve_redirectsuSessionRedirectMixin.resolve_redirectsaAuthorizationashould_strip_autharequestatrust_envaget_netrc_authaprepare_authuWhen being redirected we may want to strip authentication from the
        request to avoid leaking credentials. This method intelligently removes
        and reapplies authentication where possible to avoid credential loss.
        aresolve_proxiesuProxy-Authorizationaget_auth_from_urlTnnTahttpsa_basic_auth_struThis method re-evaluates the proxy configuration by considering the
        environment variables. If we are redirected to a URL covered by
        NO_PROXY, we strip the proxy configuration. Otherwise, we set missing
        proxy keys for this URL (in case they were stripped by a previous
        redirect).

        This method also replaces the Proxy-Authorization header where
        necessary.

        :rtype: dict
        amethodasee_otheraHEADaGETafoundamovedaPOSTuWhen being redirected we may want to change the method of the request
        based on certain specs or browser behavior.
        adefault_headersaauthadefault_hooksahooksaparamsaDEFAULT_REDIRECT_LIMITacookiejar_from_dictaOrderedDictaadaptersamountuhttps://aHTTPAdapteruhttp://acookielibaCookieJaraRequestsCookieJaraPreparedRequestaprepareaupperafilesadataajsonaCaseInsensitiveDictTadict_classamerge_hooksT
amethodaurlafilesadataajsonaheadersaparamsaauthacookiesahooksuConstructs a :class:`PreparedRequest <PreparedRequest>` for
        transmission and returns it. The :class:`PreparedRequest` has settings
        merged from the :class:`Request <Request>` instance and those of the
        :class:`Session`.

        :param request: :class:`Request` instance to prepare with this
            session's settings.
        :rtype: requests.PreparedRequest
        aRequestT
amethodaurlaheadersafilesadataajsonaparamsaauthacookiesahooksaprepare_requestamerge_environment_settingsuConstructs a :class:`Request <Request>`, prepares it and sends it.
        Returns :class:`Response <Response>` object.

        :param method: method for the new :class:`Request` object.
        :param url: URL for the new :class:`Request` object.
        :param params: (optional) Dictionary or bytes to be sent in the query
            string for the :class:`Request`.
        :param data: (optional) Dictionary, list of tuples, bytes, or file-like
            object to send in the body of the :class:`Request`.
        :param json: (optional) json to send in the body of the
            :class:`Request`.
        :param headers: (optional) Dictionary of HTTP Headers to send with the
            :class:`Request`.
        :param cookies: (optional) Dict or CookieJar object to send with the
            :class:`Request`.
        :param files: (optional) Dictionary of ``'filename': file-like-objects``
            for multipart encoding upload.
        :param auth: (optional) Auth tuple or callable to enable
            Basic/Digest/Custom HTTP Auth.
        :param timeout: (optional) How long to wait for the server to send
            data before giving up, as a float, or a :ref:`(connect timeout,
            read timeout) <timeouts>` tuple.
        :type timeout: float or tuple
        :param allow_redirects: (optional) Set to True by default.
        :type allow_redirects: bool
        :param proxies: (optional) Dictionary mapping protocol or protocol and
            hostname to the URL of the proxy.
        :param stream: (optional) whether to immediately download the response
            content. Defaults to ``False``.
        :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use. Defaults to ``True``. When set to
            ``False``, requests will accept any TLS certificate presented by
            the server, and will ignore hostname mismatches and/or expired
            certificates, which will make your application vulnerable to
            man-in-the-middle (MitM) attacks. Setting verify to ``False``
            may be useful during local development or testing.
        :param cert: (optional) if String, path to ssl client cert file (.pem).
            If Tuple, ('cert', 'key') pair.
        :rtype: requests.Response
        uSends a GET request. Returns :class:`Response` object.

        :param url: URL for the new :class:`Request` object.
        :param \*\*kwargs: Optional arguments that ``request`` takes.
        :rtype: requests.Response
        aOPTIONSuSends a OPTIONS request. Returns :class:`Response` object.

        :param url: URL for the new :class:`Request` object.
        :param \*\*kwargs: Optional arguments that ``request`` takes.
        :rtype: requests.Response
        uSends a HEAD request. Returns :class:`Response` object.

        :param url: URL for the new :class:`Request` object.
        :param \*\*kwargs: Optional arguments that ``request`` takes.
        :rtype: requests.Response
        uSends a POST request. Returns :class:`Response` object.

        :param url: URL for the new :class:`Request` object.
        :param data: (optional) Dictionary, list of tuples, bytes, or file-like
            object to send in the body of the :class:`Request`.
        :param json: (optional) json to send in the body of the :class:`Request`.
        :param \*\*kwargs: Optional arguments that ``request`` takes.
        :rtype: requests.Response
        aPUTuSends a PUT request. Returns :class:`Response` object.

        :param url: URL for the new :class:`Request` object.
        :param data: (optional) Dictionary, list of tuples, bytes, or file-like
            object to send in the body of the :class:`Request`.
        :param \*\*kwargs: Optional arguments that ``request`` takes.
        :rtype: requests.Response
        aPATCHuSends a PATCH request. Returns :class:`Response` object.

        :param url: URL for the new :class:`Request` object.
        :param data: (optional) Dictionary, list of tuples, bytes, or file-like
            object to send in the body of the :class:`Request`.
        :param \*\*kwargs: Optional arguments that ``request`` takes.
        :rtype: requests.Response
        aDELETEuSends a DELETE request. Returns :class:`Response` object.

        :param url: URL for the new :class:`Request` object.
        :param \*\*kwargs: Optional arguments that ``request`` takes.
        :rtype: requests.Response
        asetdefaultuYou can only send PreparedRequests.Taallow_redirectstTastreamaget_adapterTaurlapreferred_clockatimedeltaTasecondsaelapsedadispatch_hookaresponseainsertlwrDayield_requeststa_nextuSend a given PreparedRequest.

        :rtype: requests.Response
        Tano_proxyaget_environ_proxiesaosaenvironTaREQUESTS_CA_BUNDLETaCURL_CA_BUNDLEu
        Check the environment and merge it with some settings.

        :rtype: dict
        aloweraInvalidSchemauNo connection adapters were found for u
        Returns the appropriate connection adapter for the given URL.

        :rtype: requests.adapters.BaseAdapter
        avaluesuCloses all adapters and as such the sessionuRegisters a connection adapter to a prefix.

        Adapters are sorted in descending order by prefix length.
        a__attrs__aSessionu
    Returns a :class:`Session` for context-management.

    .. deprecated:: 1.0.0

        This method has been deprecated since version 1.0.0 and is only kept for
        backwards compatibility. New code should use :class:`~requests.sessions.Session`
        to create a session. This may be removed at a future date.

    :rtype: Session
    u
requests.sessions
~~~~~~~~~~~~~~~~~

This module provides a Session object to manage and persist settings across
requests (cookies, auth, proxies).
a__doc__a__file__a__spec__aoriginahas_locationa__cached__asysatimeacollectionsTaOrderedDictadatetimeTatimedeltaa_internal_utilsTato_native_stringlTaHTTPAdapterTa_basic_auth_stracompatTaMappingacookielibaurljoinaurlparseTaRequestsCookieJaracookiejar_from_dictaextract_cookies_to_jaramerge_cookiesaexceptionsTaChunkedEncodingErroraContentDecodingErroraInvalidSchemaaTooManyRedirectsTadefault_hooksadispatch_hookamodelsTaDEFAULT_REDIRECT_LIMITaREDIRECT_STATIaPreparedRequestaRequestaREDIRECT_STATIastatus_codesTacodesastructuresTaCaseInsensitiveDictautilsT
aDEFAULT_PORTSadefault_headersaget_auth_from_urlaget_environ_proxiesaget_netrc_autharequote_uriaresolve_proxiesarewind_bodyashould_bypass_proxiesato_key_val_listashould_bypass_proxiesaperf_counterurequests.sessionsa__module__aSessionRedirectMixina__qualname__uSessionRedirectMixin.get_redirect_targetuSessionRedirectMixin.should_strip_authTFntnnFuSessionRedirectMixin.rebuild_authuSessionRedirectMixin.rebuild_proxiesuSessionRedirectMixin.rebuild_methodTa__prepare__a__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>uA Requests session.

    Provides cookie persistence, connection-pooling, and configuration.

    Basic Usage::

      >>> import requests
      >>> s = requests.Session()
      >>> s.get('https://httpbin.org/get')
      <Response [200]>

    Or as a context manager::

      >>> with requests.Session() as s:
      ...     s.get('https://httpbin.org/get')
      <Response [200]>
    Laheadersacookiesaauthaproxiesahooksaparamsaverifyacertaadaptersastreamatrust_envamax_redirectsa__init__uSession.__init__a__enter__uSession.__enter__a__exit__uSession.__exit__uSession.prepare_requestTnnnnnnntnnnnnnuSession.requestuSession.getaoptionsuSession.optionsaheaduSession.headapostuSession.postTnaputuSession.putapatchuSession.patchadeleteuSession.deleteuSession.senduSession.merge_environment_settingsuSession.get_adapteruSession.closeuSession.mounta__getstate__uSession.__getstate__a__setstate__uSession.__setstate__a__orig_bases__asessionurequests\sessions.pyu<module requests.sessions>Ta__class__TaselfTaselfaargsTaselfastateTaselfastateaattravalueTaselfwvTaselfaurlakwargsTaselfaurlaprefixaadapterTaselfarespalocationT
aselfaurlaproxiesastreamaverifyacertano_proxyaenv_proxieswkwvTarequest_hooksasession_hooksadict_classTarequest_settingasession_settingadict_classamerged_settinganone_keysakeyTaselfaprefixaadapterakeys_to_moveakeyTaselfaurladataakwargsTaselfaurladataajsonakwargsTaselfarequestacookiesamerged_cookiesaauthwpTaselfaprepared_requestaresponseaheadersaurlanew_authTaselfaprepared_requestaresponseamethodTaselfaprepared_requestaproxiesaheadersaschemeanew_proxiesausernameapasswordTaselfamethodaurlaparamsadataaheadersacookiesafilesaauthatimeoutaallow_redirectsaproxiesahooksastreamaverifyacertajsonareqaprepasettingsasend_kwargsarespTaselfarespareqastreamatimeoutaverifyacertaproxiesayield_requestsaadapter_kwargsahistaurlaprevious_fragmentaprepared_requestaparsed_rurlaparsedapurged_headersaheaderaheadersarewindableT
aselfarequestakwargsaallow_redirectsastreamahooksaadapterastartwraelapsedarespagenahistoryTaselfaold_urlanew_urlaold_parsedanew_parsedachanged_portachanged_schemeadefault_port.requests.status_codes!
(a_codesaitemsutoo many values to unpack (expected 2)acodesastartswithTTw\w/aupperadocu_init.<locals>.doca__doc__w
asortedu, u* %d: %su``uu<genexpr>u_init.<locals>.doc.<locals>.<genexpr>u_init.<locals>.<genexpr>u
The ``codes`` object defines a mapping from common names for HTTP statuses
to their numerical codes, accessible either as attributes or as dictionary
items.

Example::

    >>> import requests
    >>> requests.codes['temporary_redirect']
    307
    >>> requests.codes.teapot
    418
    >>> requests.codes['\o/']
    200

Some codes have multiple names, and both upper- and lower-case versions of
the names are allowed. For example, ``codes.ok``, ``codes.OK``, and
``codes.okay`` all correspond to the HTTP status code 200.
a__file__a__spec__aoriginahas_locationa__cached__astructuresTaLookupDictlaLookupDictlDDldlelflglzlllllllllll,l-l.l/l0l1l2l3l4llllllllllllllllllllllllllllllllllllllllllllTacontinueTaswitching_protocolsTaprocessingTacheckpointTauri_too_longarequest_uri_too_longTaokaokayaall_okaall_okayaall_goodu\o/u✓TacreatedTaacceptedTanon_authoritative_infoanon_authoritative_informationTano_contentTareset_contentaresetTapartial_contentapartialTamulti_statusamultiple_statusamulti_statiamultiple_statiTaalready_reportedTaim_usedTamultiple_choicesTamoved_permanentlyamovedu\o-TafoundTasee_otheraotherTanot_modifiedTause_proxyTaswitch_proxyTatemporary_redirectatemporary_movedatemporaryTapermanent_redirectaresume_incompletearesumeTabad_requestabadTaunauthorizedTapayment_requiredapaymentTaforbiddenTanot_foundu-o-Tamethod_not_allowedanot_allowedTanot_acceptableTaproxy_authentication_requiredaproxy_authaproxy_authenticationTarequest_timeoutatimeoutTaconflictTagoneTalength_requiredTaprecondition_failedapreconditionTarequest_entity_too_largeTarequest_uri_too_largeTaunsupported_media_typeaunsupported_mediaamedia_typeTarequested_range_not_satisfiablearequested_rangearange_not_satisfiableTaexpectation_failedTaim_a_teapotateapotai_am_a_teapotTamisdirected_requestTaunprocessable_entityaunprocessableTalockedTafailed_dependencyadependencyTaunordered_collectionaunorderedTaupgrade_requiredaupgradeTaprecondition_requiredapreconditionTatoo_many_requestsatoo_manyTaheader_fields_too_largeafields_too_largeTano_responseanoneTaretry_witharetryTablocked_by_windows_parental_controlsaparental_controlsTaunavailable_for_legal_reasonsalegal_reasonsTaclient_closed_requestTainternal_server_erroraserver_erroru/o\u✗Tanot_implementedTabad_gatewayTaservice_unavailableaunavailableTagateway_timeoutTahttp_version_not_supportedahttp_versionTavariant_also_negotiatesTainsufficient_storageTabandwidth_limit_exceededabandwidthTanot_extendedTanetwork_authentication_requiredanetwork_authanetwork_authenticationTastatus_codesTanamea_initurequests\status_codes.pyTa.0acodeadocTa.0wnu<module requests.status_codes>TacodeatitlesatitleadocTacodeanames.requests.structuresRaOrderedDicta_storeaupdatealowerlavaluesutoo many values to unpack (expected 2)u<genexpr>uCaseInsensitiveDict.__iter__.<locals>.<genexpr>aitemsuLike iteritems(), but with all lowercase keys.uCaseInsensitiveDict.lower_items.<locals>.<genexpr>aMappingaCaseInsensitiveDictalower_itemsanamea__class__a__init__u<lookup 'uu'>agetu
requests.structures
~~~~~~~~~~~~~~~~~~~

Data structures that power Requests.
a__doc__a__file__a__spec__aoriginahas_locationa__cached__acollectionsTaOrderedDictlacompatTaMappingaMutableMappingaMutableMappinga__prepare__a__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>urequests.structuresa__module__uA case-insensitive ``dict``-like object.

    Implements all methods and operations of
    ``MutableMapping`` as well as dict's ``copy``. Also
    provides ``lower_items``.

    All keys are expected to be strings. The structure remembers the
    case of the last key to be set, and ``iter(instance)``,
    ``keys()``, ``items()``, ``iterkeys()``, and ``iteritems()``
    will contain case-sensitive keys. However, querying and contains
    testing is case insensitive::

        cid = CaseInsensitiveDict()
        cid['Accept'] = 'application/json'
        cid['aCCEPT'] == 'application/json'  # True
        list(cid) == ['Accept']  # True

    For example, ``headers['content-encoding']`` will return the
    value of a ``'Content-Encoding'`` response header, regardless
    of how the header name was originally stored.

    If the constructor, ``.update``, or equality comparison
    operations are given keys that have equal ``.lower()``s, the
    behavior is undefined.
    a__qualname__TnuCaseInsensitiveDict.__init__a__setitem__uCaseInsensitiveDict.__setitem__uCaseInsensitiveDict.__getitem__a__delitem__uCaseInsensitiveDict.__delitem__a__iter__uCaseInsensitiveDict.__iter__a__len__uCaseInsensitiveDict.__len__uCaseInsensitiveDict.lower_itemsa__eq__uCaseInsensitiveDict.__eq__acopyuCaseInsensitiveDict.copya__repr__uCaseInsensitiveDict.__repr__a__orig_bases__TOdictaLookupDictuDictionary lookup object.uLookupDict.__init__uLookupDict.__repr__uLookupDict.__getitem__uLookupDict.geturequests\structures.pyTa.0acasedkeyamappedvalueTa.0alowerkeyakeyvalu<module requests.structures>Ta__class__TaselfakeyTaselfaotherTaselfadataakwargsTaselfanamea__class__TaselfTaselfakeyavalueTaselfakeyadefault.requests.utils{8awinreglaOpenKeyaHKEY_CURRENT_USERuSoftware\Microsoft\Windows\CurrentVersion\Internet SettingsaQueryValueExaProxyEnableaProxyOverrideTEOSErrorEValueErrorasplitTw;u<local>w.ahostareplaceTw.u\.Tw*u.*Tw?w.areamatchwIagetproxies_environmentaproxy_bypass_environmentaproxy_bypass_registryuReturn True, if the host should be bypassed.

        Checks proxy settings gathered from the environment, if specified,
        or the registry.
        aitemsuReturns an internal sequence dictionary update.a__len__alenafilenoaioaUnsupportedOperationaosafstatast_sizewbamodeawarningsawarnuRequests has determined the content-length for this request using the binary size of the file: however, the file has been opened in text mode (i.e. without the 'b' flag in the mode). This may lead to an incorrect content-length. In Requests 3.0, support will be removed for files in text mode.aFileModeWarningwoatellaseekatotal_lengthTllamaxaenvironagetTaNETRCaNETRC_FILESanetrcTaNetrcParseErroranetrcaNetrcParseErrorapathaexpanduseraexistsaurlparsed:astraasciianetlocaauthenticatorsllTEImportErrorEAttributeErroruReturns the Requests tuple auth for a given url from netrc.u~/uu<genexpr>uget_netrc_auth.<locals>.<genexpr>anameabasestringw<lw>uTries to guess the filename of the given object.utoo many values to unpack (expected 2)aarchivew/amemberazipfileais_zipfileaZipFileanamelistatempfileagettempdirajoinTw/aatomic_opena__enter__a__exit__awriteareadTnnnuReplace nonexistent paths that look like they refer to a member of a zip
    archive with the location of an extracted copy of the target, or else
    just return the provided path unchanged.
    uWrite a file to the disk in an atomic fashionamkstempadirnameafilenameTadirafdopenawbaremoveabytesucannot encode objects that are not 2-tuplesaOrderedDictuTake an object and test to see if it can be represented as a
    dictionary. Unless it can not be represented as such, return an
    OrderedDict, e.g.,

    ::

        >>> from_key_val_list([('key', 'val')])
        OrderedDict([('key', 'val')])
        >>> from_key_val_list('string')
        Traceback (most recent call last):
        ...
        ValueError: cannot encode objects that are not 2-tuples
        >>> from_key_val_list({'key': 'val'})
        OrderedDict([('key', 'val')])

    :rtype: OrderedDict
    aMappinguTake an object and test to see if it can be represented as a
    dictionary. If it can be, return a list of tuples, e.g.,

    ::

        >>> to_key_val_list([('key', 'val')])
        [('key', 'val')]
        >>> to_key_val_list({'key': 'val'})
        [('key', 'val')]
        >>> to_key_val_list('string')
        Traceback (most recent call last):
        ...
        ValueError: cannot encode objects that are not 2-tuples

    :rtype: list
    a_parse_list_header:lnn:nlnw"aunquote_header_value:llnaresultaappenduParse lists as described by RFC 2068 Section 2.

    In particular, parse comma-separated lists where the elements of
    the list may include quoted-strings.  A quoted-string could
    contain a comma.  A non-quoted string could have quotes in the
    middle.  Quotes are removed automatically after parsing.

    It basically works like :func:`parse_set_header` just that items
    may appear multiple times and case sensitivity is preserved.

    The return value is a standard :class:`list`:

    >>> parse_list_header('token, "quoted value"')
    ['token', 'quoted value']

    To create a header from the :class:`list` again, use the
    :func:`dump_header` function.

    :param value: a string with a list header.
    :return: :class:`list`
    :rtype: list
    w=Tw=luParse lists of key, value pairs as described by RFC 2068 Section 2 and
    convert them into a python dict:

    >>> d = parse_dict_header('foo="is a fish", bar="as well"')
    >>> type(d) is dict
    True
    >>> sorted(d.items())
    [('bar', 'as well'), ('foo', 'is a fish')]

    If there is no value for a key it will be `None`:

    >>> parse_dict_header('key_without_value')
    {'key_without_value': None}

    To create a header from the :class:`dict` again, use the
    :func:`dump_header` function.

    :param value: a string with a dict header.
    :return: :class:`dict`
    :rtype: dict
    :nlnu\\Tu\\w\Tu\"w"uUnquotes a header value.  (Reversal of :func:`quote_header_value`).
    This does not use the real unquoting but what browsers are actually
    using for quoting.

    :param value: the header value to unquote.
    :rtype: str
    avalueacookie_dictuReturns a key/value dictionary from a CookieJar.

    :param cj: CookieJar object to extract cookies from.
    :rtype: dict
    acookiejar_from_dictuReturns a CookieJar from a key/value dictionary.

    :param cj: CookieJar to insert cookies into.
    :param cookie_dict: Dict of key/values to insert into CookieJar.
    :rtype: CookieJar
    uIn requests 3.0, get_encodings_from_content will be removed. For more information, please see the discussion on issue #2266. (This warning should only appear once.)aDeprecationWarningacompileTu<meta.*?charset=["\']*(.+?)["\'>]TaflagsTu<meta.*?content=["\']*;?charset=(.+?)["\'>]Tu^<\?xml.*?encoding=["\']*(.+?)["\'>]afindalluReturns encodings from given content string.

    :param content: bytestring to extract encodings from.
    astrip:lnnu"' afindTw=aitems_to_stripaparams_dictaloweruReturns content type and parameters from given header

    :param header: string
    :return: tuple containing content type and dictionary of
         parameters
    Tucontent-typea_parse_content_type_headeracharsetTu'"atextuISO-8859-1uapplication/jsonuutf-8uReturns encodings from given HTTP Header Dict.

    :param headers: dictionary to extract encoding from.
    :rtype: str
    uStream decodes an iterator.wraencodingaiteratoracodecsagetincrementaldecoderTareplaceTaerrorsadecoderadecodeTctTafinalastream_decode_response_unicodeuIterate over slices of a string.aslice_lengthastringaposaiter_slicesuIn requests 3.0, get_unicode_from_response will be removed. For more information, please see the discussion on issue #2266. (This warning should only appear once.)aget_encoding_from_headersaheadersacontentDaerrorsareplaceuReturns the requested content back in unicode.

    :param r: Response object to get unicode content from.

    Tried:

    1. charset from content-type
    2. fall back and replace all unicode characters

    :rtype: str
    Tw%aparts:llnaisalnumlaInvalidURLuInvalid percent-escape sequence: 'w'aUNRESERVED_SET:lnnw%uUn-escape any percent-escape sequences in a URI that are unreserved
    characters. This leaves all reserved, illegal and non-ASCII bytes encoded.

    :rtype: str
    aquoteaunquote_unreservedDasafeu!#$%&'()*+,/:;=?@[]~Dasafeu!#$&'()*+,/:;=?@[]~uRe-quote the given URI.

    This function passes the given URI through an unquote/quote cycle to
    ensure that it is fully and consistently quoted.

    :rtype: str
    astructaunpacku=Lasocketainet_atonadotted_netmaskuThis function allows you to check if an IP belongs to a network subnet

    Example: returns True if ip = 192.168.1.1 and net = 192.168.1.0/24
             returns False if ip = 192.168.1.1 and net = 192.168.100.0/24

    :rtype: bool
    ql ainet_ntoaapacku>IuConverts mask from /xx format to xxx.xxx.xxx.xxx

    Example: if mask is 24 function returns 255.255.255.0

    :rtype: str
    u
    :rtype: bool
    acountu
    Very simple check of the cidr format in no_proxy variable.

    :rtype: bool
    uSet the environment variable 'env_name' to 'value'

    Save previous value, yield, and then restore the previous value stored in
    the environment variable 'env_name'.

    If 'value' is None, do nothingaenv_nameaold_valueaset_environaget_proxyushould_bypass_proxies.<locals>.get_proxyTano_proxyahostnameano_proxyTw uTw,ais_ipv4_addressais_valid_cidraaddress_in_networkaparsedaportw:aendswithahost_with_portaproxy_bypassagaierrorabypassu
    Returns whether we should bypass proxies or not.

    :rtype: bool
    aupperushould_bypass_proxies.<locals>.<genexpr>ashould_bypass_proxiesagetproxiesu
    Return a dict of environment proxies.

    :rtype: dict
    aschemeTaallu://uall://aalluSelect a proxy for the url, if applicable.

    :param url: The url being for the request
    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs
    aurlacopyaget_environ_proxiesasetdefaultanew_proxiesuThis method takes proxy information from a request and configuration
    input to resolve a mapping of target proxies. This will consider settings
    such a NO_PROXY to strip proxy configurations.

    :param request: Request or PreparedRequest
    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs
    :param trust_env: Boolean declaring whether to trust environment configs

    :rtype: dict
    a__version__u
    Return a string representing the default user agent.

    :rtype: str
    aCaseInsensitiveDictuUser-Agentadefault_user_agentuAccept-EncodingaDEFAULT_ACCEPT_ENCODINGaAcceptu*/*aConnectionukeep-aliveu
    :rtype: requests.structures.CaseInsensitiveDict
    u '"u, *<Tw;lTu<> '"areplace_charsalinkalinksuReturn a list of parsed link headers proxies.

    i.e. Link: <http:/.../front.jpeg>; rel=front; type="image/jpeg",<http://.../back.jpeg>; rel=back;type="image/jpeg"

    :rtype: list
    :nlnaBOM_UTF32_LEaBOM_UTF32_BEuutf-32:nlnaBOM_UTF8uutf-8-sigaBOM_UTF16_LEaBOM_UTF16_BEuutf-16a_null:nnla_null2uutf-16-be:lnluutf-16-lela_null3uutf-32-beuutf-32-leu
    :rtype: str
    aparse_urlutoo many values to unpack (expected 7)w@aurlunparseuGiven a URL that may or may not have a scheme, prepend the given scheme.
    Does not replace a present scheme with the one provided as an argument.

    :rtype: str
    aunquoteausernameapasswordTEAttributeErrorETypeErrorTupuGiven a url with authentication components, extract them into a tuple of
    username,password.

    :rtype: (str,str)
    a_validate_header_partuVerifies that header parts don't contain leading whitespace
    reserved characters, or return characters.

    :param header: tuple, in the format (name, value).
    a_HEADER_VALIDATORS_STRa_HEADER_VALIDATORS_BYTEaInvalidHeaderuHeader part (u) from u must be of type str or bytes, not uInvalid leading whitespace, reserved character(s), or returncharacter(s) in header u: utoo many values to unpack (expected 6)arsplitTw@lu
    Given a url remove the fragment and the authentication part.

    :rtype: str
    abodya_body_positionainteger_typesaUnrewindableBodyErrorTuAn error occurred when rewinding request body for redirect.TuUnable to rewind request body for redirect.uMove file pointer back to its recorded starting position
    so it can be read again on redirect.
    u
requests.utils
~~~~~~~~~~~~~~

This module provides utility functions that are used within Requests
that are also useful for external consumption.
a__doc__a__file__a__spec__aoriginahas_locationa__cached__acontextlibasysacollectionsTaOrderedDictuurllib3.utilTamake_headersaparse_urlamake_headersTacertsacertsTa__version__a_internal_utilsTa_HEADER_VALIDATORS_BYTEa_HEADER_VALIDATORS_STRaHEADER_VALIDATORSato_native_stringaHEADER_VALIDATORSato_native_stringacompatTaMappingabasestringabytesagetproxiesagetproxies_environmentainteger_typesTaparse_http_listaparse_http_listTaproxy_bypassaproxy_bypass_environmentaquoteastraunquoteaurlparseaurlunparseacookiesTacookiejar_from_dictaexceptionsTaFileModeWarningaInvalidHeaderaInvalidURLaUnrewindableBodyErrorastructuresTaCaseInsensitiveDictTu.netrca_netrcawhereaDEFAULT_CA_BUNDLE_PATHDahttpahttpslPlaDEFAULT_PORTSu, u,\s*TtTaaccept_encodinguaccept-encodingadict_to_sequenceasuper_lenTFaget_netrc_authaguess_filenameaextract_zipped_pathsacontextmanagerafrom_key_val_listato_key_val_listaparse_list_headeraparse_dict_headeradict_from_cookiejaraadd_dict_to_cookiejaraget_encodings_from_contentaget_unicode_from_responsePBwMwFwYwiwrwxw2wpw-wdw8wXwKwQwcwDwywlwEwSwOw3whwHwmw.wPwBwUwJwzwVwAw6wIwLwkw5wowaw0wZw7w_wgw1wTwGwfwewWwuwwwRwnwbwqwjwtw~wNwsw4w9wvwCarequote_uriTnaselect_proxyaresolve_proxiesTupython-requestsadefault_headersaparse_header_linkswaguess_json_utfaprepend_scheme_if_neededaget_auth_from_urlacheck_header_validityaurldefragautharewind_bodyurequests\utils.pyTa.0wfTa.0ahostu<module requests.utils>T
aheaderatokensacontent_typeaparamsaparams_dictaitems_to_stripaparamakeyavalueaindex_of_equalsTaheaderaheader_partaheader_validator_indexavalidatoraheader_kindTacjacookie_dictTaipanetaipaddranetaddrabitsanetmaskanetworkTafilenameatmp_descriptoratmp_nameatmp_handlerTaheaderanameavalueTanameTacjacookie_dictacookieTwdTamaskabitsTapathaarchiveamemberaprefixazip_fileatmpaextracted_pathafile_handlerTavalueTaurlaparsedaauthTaheadersacontent_typeaparamsTacontentacharset_reapragma_reaxml_reTaurlano_proxyTaurlaraise_errorsanetrc_fileanetrc_locationsaNetrcParseErroranetrcanetrc_pathwfalocariasplitstrahosta_netrcalogin_iTakeyTwratried_encodingsaencodingTaobjanameTadataasampleanullcountTastring_ipTastring_networkamaskTastringaslice_lengthaposTavaluearesultaitemanameTavaluealinksareplace_charsavalaurlaparamsalinkaparamakeyTavaluearesultaitemTaurlanew_schemeaparsedaschemeaauthahostaportapathaqueryafragmentanetlocTahostTahostawinregainternetSettingsaproxyEnableaproxyOverrideatestTauriasafe_with_percentasafe_without_percentTarequestaproxiesatrust_envaurlaschemeano_proxyanew_proxiesaenviron_proxiesaproxyTaprepared_requestabody_seekTaurlaproxiesaurlpartsaproxy_keysaproxyaproxy_keyTaenv_nameavalueavalue_changedaold_valueTaurlano_proxyaget_proxyano_proxy_argaparsedaproxy_ipahost_with_portahostabypassTaiteratorwradecoderachunkarvTwoatotal_lengthacurrent_positionafilenoTavalueais_filenameTauriapartswiwhwcTaurlaschemeanetlocapathaparamsaqueryafragment.soupsieve.__meta__gluAll version parts except 'release' should be integers.aREL_MAPw'uu' is not a valid release type.u.dev-candidateafinaluImplicit pre-releases not allowed.uVersion is not a development release.uPost-releases are not allowed with pre-releases.aalphau.devuImplicit pre-release not allowed.uVersion is not a pre-release.a__class__a__new__uValidate version info.apreuIs prerelease.areleaseuIs development.apostuIs post.aDEV_STATUSuGet development status string.amicroamajorw.aminora_is_prea_is_postu.posta_is_devadevuGet the canonical output string.aRE_VERamatchu' is not a valid versionagroupTamajorTaminorTamicroTatypeaPRE_REL_MAPTapreTadevu.dev-TapostaVersionuParse version into a comparable Version tuple.uMeta related things.a__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationsacollectionsTanamedtupleanamedtupleareacompileTu(?x)
    (?P<major>\d+)(?:\.(?P<minor>\d+))?(?:\.(?P<micro>\d+))?
    (?:(?P<type>a|b|rc)(?P<pre>\d+))?
    (?:\.post(?P<post>\d+))?
    (?:\.dev(?P<dev>\d+))?
    Du.devu.dev-alphau.dev-betau.dev-candidateaalphaabetaacandidateafinaluwawbarcwawbarcuDu.devu.dev-alphau.dev-betau.dev-candidateaalphaabetaacandidateafinalu2 - Pre-Alphau2 - Pre-Alphau2 - Pre-Alphau2 - Pre-Alphau3 - Alphau4 - Betau4 - Betau5 - Production/StableDwawbarcaalphaabetaacandidateLamajoraminoramicroareleaseapreapostadeva__prepare__a__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>usoupsieve.__meta__a__module__u
    Get the version (PEP 440).

    A biased approach to the PEP 440 semantic version.

    Provides a tuple structure which is sorted for comparisons `v1 > v2` etc.
      (major, minor, micro, release type, pre-release build, post-release build, development release build)
    Release types are named in is such a way they are comparable with ease.
    Accessors to check if a development, pre-release, or post-release build. Also provides accessor to get
    development status for setup files.

    How it works (currently):

    - You must specify a release type as either `final`, `alpha`, `beta`, or `candidate`.
    - To define a development release, you can use either `.dev`, `.dev-alpha`, `.dev-beta`, or `.dev-candidate`.
      The dot is used to ensure all development specifiers are sorted before `alpha`.
      You can specify a `dev` number for development builds, but do not have to as implicit development releases
      are allowed.
    - You must specify a `pre` value greater than zero if using a prerelease as this project (not PEP 440) does not
      allow implicit prereleases.
    - You can optionally set `post` to a value greater than zero to make the build a post release. While post releases
      are technically allowed in prereleases, it is strongly discouraged, so we are rejecting them. It should be
      noted that we do not allow `post0` even though PEP 440 does not restrict this. This project specifically
      does not allow implicit post releases.
    - It should be noted that we do not support epochs `1!` or local versions `+some-custom.version-1`.

    Acceptable version releases:

    ```
    Version(1, 0, 0, "final")                    1.0
    Version(1, 2, 0, "final")                    1.2
    Version(1, 2, 3, "final")                    1.2.3
    Version(1, 2, 0, ".dev-alpha", pre=4)        1.2a4
    Version(1, 2, 0, ".dev-beta", pre=4)         1.2b4
    Version(1, 2, 0, ".dev-candidate", pre=4)    1.2rc4
    Version(1, 2, 0, "final", post=1)            1.2.post1
    Version(1, 2, 3, ".dev")                     1.2.3.dev0
    Version(1, 2, 3, ".dev", dev=1)              1.2.3.dev1
    ```

    a__qualname__TafinallppDamajoraminoramicroareleaseapreapostadevareturnaintppastraintppaVersionuVersion.__new__DareturnabooluVersion._is_preuVersion._is_devuVersion._is_postDareturnastra_get_dev_statusuVersion._get_dev_statusa_get_canonicaluVersion._get_canonicala__orig_bases__DaverareturnastraVersionaparse_versionTlllafinala__version_info__a__version__usoupsieve\__meta__.pyu<module soupsieve.__meta__>Ta__class__T
aclsamajoraminoramicroareleaseapreapostadevavaluea__class__TaselfaverTaselfTaverwmamajoraminoramicroareleaseapreadevapost.soupsieve
ZaSoupSieveuCannot process 'flags' argument on a compiled selector listuCannot process 'namespaces' argument on a compiled selector listuCannot process 'custom' argument on a compiled selector listacpa_cached_css_compileactaNamespacesaCustomSelectorsuCompile CSS pattern.a_purge_cacheuPurge cached patterns.acompileaclosestuMatch closest ancestor.amatchuMatch node.afilteruFilter list of nodes.aselect_oneuSelect a single tag.aselectuSelect the specified tags.uIterate the specified tags.anamespacesaflagsakwargsaiselectatagalimitaescapeuEscape identifier.u
Soup Sieve.

A CSS selector filter for BeautifulSoup4.

MIT License

Copyright (c) 2018 Isaac Muse

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
a__doc__a__file__apathadirnameaenvironagetTaNUITKA_PACKAGE_soupsieveu\not_existinga__path__a__spec__aoriginahas_locationasubmodule_search_locationsa__cached__aannotationsa__meta__Ta__version__a__version_info__la__version__la__version_info__uTacss_parseracss_parserTacss_matchacss_matchacmTacss_typesacss_typesautilTaDEBUGaSelectorSyntaxErroraDEBUGaSelectorSyntaxErrorabs4aAnyaIteratoraIterableT
aDEBUGaSelectorSyntaxErroraSoupSieveaclosestacompileafilteraiselectamatchaselectaselect_onea__all__TnlDacustomnDapatternanamespacesaflagsacustomakwargsareturnastrudict[str, str] | Noneaintudict[str, str] | NoneaAnyucm.SoupSieveDareturnaNoneapurgeDaselectataganamespacesaflagsacustomakwargsareturnastrubs4.Tagudict[str, str] | Noneaintudict[str, str] | NoneaAnyubs4.TagDaselectataganamespacesaflagsacustomakwargsareturnastrubs4.Tagudict[str, str] | Noneaintudict[str, str] | NoneaAnyaboolDaselectaiterableanamespacesaflagsacustomakwargsareturnastruIterable[bs4.Tag]udict[str, str] | Noneaintudict[str, str] | NoneaAnyulist[bs4.Tag]TnlpDaselectataganamespacesalimitaflagsacustomakwargsareturnastrubs4.Tagudict[str, str] | Noneaintpudict[str, str] | NoneaAnyulist[bs4.Tag]Daselectataganamespacesalimitaflagsacustomakwargsareturnastrubs4.Tagudict[str, str] | Noneaintpudict[str, str] | NoneaAnyuIterator[bs4.Tag]Daidentareturnastrpusoupsieve\__init__.pyu<module soupsieve>TaselectataganamespacesaflagsacustomakwargsTapatternanamespacesaflagsacustomakwargsTaidentTaselectaiterableanamespacesaflagsacustomakwargsTaselectataganamespacesalimitaflagsacustomakwargs.soupsieve.css_matchAacontentsuInitialize.uLength.ais_taguExpected a BeautifulSoup 'Tag', but instead received type uuCheck if valid input tag or document.abs4aBeautifulSoupuIs `BeautifulSoup` object.aTaguIs tag.aDeclarationuIs declaration.aCDatauIs CDATA.aProcessingInstructionuIs processing instruction.aNavigableStringuIs navigable string.aCommentaDoctypeuIs special string.ais_navigable_stringais_special_stringuCheck if node is content string.a_FakeParentuCreate fake parent for a given element.a_is_xmluCheck if element (or document) is from a XML tree.ais_xml_treeanameautilaloweraiframeais_html_taguCheck if element is an `iframe`.arootaget_parentais_htmlais_iframeu
        Return whether element is a root element.

        We check that the element is the root of the tree (which we have already pre-calculated),
        and we check if it is the root element under an `iframe`.
        uGet contents or contents in reverse.ano_iframeaselfaelaget_contentsu_DocumentNav.get_contentsuGet children.astartareverselllaindexaincratagsanodeaget_childrenu_DocumentNav.get_childrenuGet descendants.adescendantsanext_goodanext_siblingalast_childanext_elementaget_descendantsu_DocumentNav.get_descendantsaparentuGet parent.acastustr | NoneuGet tag.aprefixuGet prefix.anamespaceuGet namespace `URI`.aclsasiblinguGet next sibling tag.aprevious_siblinguGet previous sibling tag.aNS_XHTMLu
        Check if element has an HTML namespace.

        This is a bit different than whether a element is treated as having an HTML namespace,
        like we do in the case of `is_html_tag`.
        uReturn namespace and attribute name without the prefix.adecodeTautf8aSequenceTOstrObytesanew_valueaappendanormalize_valueuNormalize the value to be a string or list of strings.aattrsaitemsutoo many values to unpack (expected 2)uGet attribute by name.uIterate attributes.aiter_attributesu_DocumentNav.iter_attributesaget_attribute_by_nameaclassaRE_NOT_WSafindalluGet classes.Tatagsano_iframeais_content_stringuGet text.Tano_iframeuGet Own Text.aLONG_MONTHaFEBlldlaFEB_LEAP_MONTHaFEB_MONTHaMONTHS_30aSHORT_MONTHuValidate day.adatetimeastrptimelw-lu%m-%d-%Yaisocalendarl5uValidate week.uValidate month.uValidate year.luValidate hour.l;uValidate minutes.adateaRE_DATEamatchagroupTayearl
TamonthTadayavalidate_yearavalidate_monthavalidate_dayamonthadayaRE_MONTHaweekaRE_WEEKTaweekavalidate_weekatimeaRE_TIMETahourTaminutesavalidate_houravalidate_minutesaminutesudatetime-localaRE_DATETIMEahourTanumberarangeaRE_NUMTavalueuParse the input value.aassert_valid_inputatagacached_meta_langacached_default_formsacached_indeterminate_formsaselectorsanamespacesaflagsaiframe_restrictais_docadocascopeahas_html_nsahas_html_namespaceais_xmluCheck if namespaces are supported in the HTML type.asupports_namespacesaget_uriuGet tag namespace.aget_tag_nsuCheck if tag is in HTML namespace.aget_tag_nameaget_prefix_nameDatagsFaDIR_MAPagetadiraget_tagTabdiascriptastyleatextareaaiframeafind_bidiaunicodedataabidirectionalTaALwRwLwLactaSEL_DIR_LTRaSEL_DIR_RTLuGet directionality from element text.aRE_WILD_STRIPasubasplitTw-w*arindexasindexuFilter the language tags.asplit_namespaceaattruMatch attribute name and return value if it exists.TuuMatch the namespace of the element.amatch_attribute_nameaattributeaxml_type_patternapatternw uMatch attributes.uMatch tag name.amatch_namespaceamatch_tagnameuMatch the tag.aSelectorNullarel_typeaREL_PARENTafoundamatch_selectorsarelationaREL_CLOSE_PARENTaREL_SIBLINGaget_previousaREL_CLOSE_SIBLINGuMatch past relationship.uMatch future child.aREL_HAS_PARENTamatch_future_childaREL_HAS_CLOSE_PARENTaREL_HAS_SIBLINGaget_nextaREL_HAS_CLOSE_SIBLINGuMatch future relationship.astartswithTw:amatch_future_relationsamatch_past_relationsuMatch relationship to other elements.aiduMatch element's ID.aget_classesuMatch element's classes.ais_rootastripais_cdatauMatch element as root.uMatch element as scope.uMatch tag type for `nth` matches.acreate_fake_parentalastwawbwnaidxalast_indexaadjustacountacount_incralowestafactorTastartareverseatagsaof_typeamatch_nth_tag_typearelative_indexachildamatcheduMatch `nth` elements.aRE_NOT_EMPTYasearchuCheck if element is empty (if requested).uMatch selectors.acontentaownaget_own_textaget_textacontain_listatextuMatch element if it contains text.Dano_iframetaformTainputabuttonatypeasubmituMatch default.Daelareturnubs4.Tagubs4.Tag | NoneuFind this input's form.aget_parent_formuCSSMatch.match_indeterminate.<locals>.get_parent_formutoo many values to unpack (expected 3)ainputaradiowvacheckedais_radioacheckahas_nameafound_langalangaNS_XMLahtmlTahtmlaheadametauhttp-equivucontent-languageac_langaextended_language_filteruMatch languages.TnlatextareaabdiatelTatextasearchatelaurlaemailavalueamatch_diruCheck directionality.u<genexpr>uCSSMatch.match_dir.<locals>.<genexpr>aInputsaparse_valueaminamaxTadateudatetime-localamonthaweekanumberarangeaSEL_IN_RANGEu
        Match range.

        Behavior is modeled after what we see in browsers. Browsers seem to evaluate
        if the value is out of range, and if not, it is in range. So a missing value
        will not evaluate out of range; therefore, value is in range. Personally, I
        feel like this should evaluate as neither in or out of range.
        afindaget_prefixu
        Match defined.

        `:defined` is related to custom elements in a browser.

        - If the document is XML (not XHTML), all tags will match.
        - Tags that are not custom (don't have a hyphen) are marked defined.
        - If the tag has a prefix (without or without a namespace), it will not match.

        This is of course requires the parser to provide us with the proper prefix and namespace info,
        if it doesn't, there is nothing we can do.
        Tuw
u
        Match placeholder shown according to HTML spec.

        - text area should be checked if they have content. A single newline does not count as content.

        ais_notamatch_tagaSEL_DEFINEDamatch_definedaSEL_ROOTamatch_rootaSEL_SCOPEamatch_scopeaSEL_PLACEHOLDER_SHOWNamatch_placeholder_shownamatch_nthanthaSEL_EMPTYamatch_emptyaidsamatch_idaclassesamatch_classesamatch_attributesaattributesaRANGESamatch_rangeamatch_langamatch_subselectorsamatch_relationsaSEL_DEFAULTamatch_defaultaSEL_INDETERMINATEamatch_indeterminateaDIR_FLAGSacontainsamatch_containsuCheck if element matches one of the selectors.uMatch all tags under the targeted tag.alimitalimaselectuCSSMatch.selectaclosestacurrentuMatch closest ancestor.uFilter tag's children.uMatch.a__class__a__init__TapatternaselectorsanamespacesacustomaflagsaCSSMatchafilteru
        Filter.

        `CSSMatch` can cache certain searches for tags of the same document,
        so if we are given a tag, all tags are from the same document,
        and we can take advantage of the optimization.

        Any other kind of iterable could have tags from different documents or detached tags,
        so for those, we use a new `CSSMatch` for each item in the iterable.
        DalimitluSelect a single tag.aiselectuSelect the specified tags.uIterate the specified tags.uSoupSieve.iselectuSoupSieve(pattern=u, namespaces=u, custom=acustomu, flags=w)uRepresentation.uCSS matcher.a__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationsTadatetimeTautilareTacss_typesacss_typesaIteratoraIterableaAnyaCallableacompileTu[^
]Tu[^
]+w>w~w+u: u:>u:~u:+uhttp://www.w3.org/1999/xhtmluhttp://www.w3.org/XML/1998/namespaceaSEL_OUT_OF_RANGEaltrartlaautoTu^(?P<value>-?(?:[0-9]{1,}(\.[0-9]+)?|\.[0-9]+))$Tu^(?P<hour>[0-9]{2}):(?P<minutes>[0-9]{2})$Tu^(?P<year>[0-9]{4,})-(?P<month>[0-9]{2})$Tu^(?P<year>[0-9]{4,})-W(?P<week>[0-9]{2})$Tu^(?P<year>[0-9]{4,})-(?P<month>[0-9]{2})-(?P<day>[0-9]{2})$Tu^(?P<year>[0-9]{4,})-(?P<month>[0-9]{2})-(?P<day>[0-9]{2})T(?P<hour>[0-9]{2}):(?P<minutes>[0-9]{2})$Tu(?:(?:-\*-)(?:\*(?:-|$))*|-\*$)TlllllllllaDAYS_IN_WEEKusoupsieve.css_matcha__module__u
    Fake parent class.

    When we have a fragment with no `BeautifulSoup` document object,
    we can't evaluate `nth` selectors properly.  Create a temporary
    fake parent so we can traverse the root element as a child.
    a__qualname__Daelementareturnubs4.TagaNoneu_FakeParent.__init__Dareturnubs4.PageElementa__len__u_FakeParent.__len__TuNavigate a Beautiful Soup document.a_DocumentNavDatagareturnaAnyaNoneu_DocumentNav.assert_valid_inputDaobjareturnubs4.Tagaboolu_DocumentNav.is_docDaobjareturnubs4.PageElementaboolu_DocumentNav.is_tagais_declarationu_DocumentNav.is_declarationu_DocumentNav.is_cdataais_processing_instructionu_DocumentNav.is_processing_instructionu_DocumentNav.is_navigable_stringu_DocumentNav.is_special_stringu_DocumentNav.is_content_stringDaelareturnubs4.Taga_FakeParentu_DocumentNav.create_fake_parentDaelareturnubs4.Tagaboolu_DocumentNav.is_xml_treeu_DocumentNav.is_iframeu_DocumentNav.is_rootTFDaelano_iframeareturnubs4.TagabooluIterator[bs4.PageElement]TnFtFDaelastartareverseatagsano_iframeareturnubs4.Taguint | NoneaboolppuIterator[bs4.PageElement]TtFDaelatagsano_iframeareturnubs4.TagaboolpuIterator[bs4.PageElement]Daelano_iframeareturnubs4.Tagaboolubs4.Tagu_DocumentNav.get_parentDaelareturnubs4.Tagustr | Noneu_DocumentNav.get_tag_nameu_DocumentNav.get_prefix_nameu_DocumentNav.get_uriTtDaelatagsareturnubs4.Tagaboolubs4.PageElementu_DocumentNav.get_nextu_DocumentNav.get_previousu_DocumentNav.has_html_nsDaelaattr_nameareturnubs4.Tagastrutuple[str | None, str | None]u_DocumentNav.split_namespaceDavalueareturnaAnyustr | Sequence[str]u_DocumentNav.normalize_valueTnDaelanameadefaultareturnubs4.Tagastrustr | Sequence[str] | Noneustr | Sequence[str] | Noneu_DocumentNav.get_attribute_by_nameDaelareturnubs4.TaguIterator[tuple[str, str | Sequence[str] | None]]Daelareturnubs4.TaguSequence[str]u_DocumentNav.get_classesDaelano_iframeareturnubs4.Tagaboolastru_DocumentNav.get_textDaelano_iframeareturnubs4.Tagaboolulist[str]u_DocumentNav.get_own_textuClass for parsing and validating input items.DayearamonthadayareturnaintppabooluInputs.validate_dayDayearaweekareturnaintpabooluInputs.validate_weekDamonthareturnaintabooluInputs.validate_monthDayearareturnaintabooluInputs.validate_yearDahourareturnaintabooluInputs.validate_hourDaminutesareturnaintabooluInputs.validate_minutesDaitypeavalueareturnastrustr | Noneutuple[float, ...] | NoneuInputs.parse_valuea__prepare__a__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>uPerform CSS matching.Daselectorsascopeanamespacesaflagsareturnuct.SelectorListubs4.Taguct.Namespaces | NoneaintaNoneuCSSMatch.__init__DareturnabooluCSSMatch.supports_namespacesDaelareturnubs4.TagastruCSSMatch.get_tag_nsuCSSMatch.is_html_taguCSSMatch.get_taguCSSMatch.get_prefixDaelareturnubs4.Taguint | NoneuCSSMatch.find_bidiDalang_rangealang_tagareturnastrpabooluCSSMatch.extended_language_filterDaelaattraprefixareturnubs4.Tagastrustr | Noneustr | Sequence[str] | NoneuCSSMatch.match_attribute_nameDaelatagareturnubs4.Taguct.SelectorTagabooluCSSMatch.match_namespaceDaelaattributesareturnubs4.Tagutuple[ct.SelectorAttribute, ...]abooluCSSMatch.match_attributesuCSSMatch.match_tagnameDaelatagareturnubs4.Taguct.SelectorTag | NoneabooluCSSMatch.match_tagDaelarelationareturnubs4.Taguct.SelectorListabooluCSSMatch.match_past_relationsDaparentarelationarecursiveareturnubs4.Taguct.SelectorListaboolpuCSSMatch.match_future_childuCSSMatch.match_future_relationsuCSSMatch.match_relationsDaelaidsareturnubs4.Tagutuple[str, ...]abooluCSSMatch.match_idDaelaclassesareturnubs4.Tagutuple[str, ...]abooluCSSMatch.match_classesuCSSMatch.match_rootuCSSMatch.match_scopeDaelachildareturnubs4.Tagubs4.TagabooluCSSMatch.match_nth_tag_typeDaelanthareturnubs4.Tagubs4.TagabooluCSSMatch.match_nthuCSSMatch.match_emptyDaelaselectorsareturnubs4.Tagutuple[ct.SelectorList, ...]abooluCSSMatch.match_subselectorsDaelacontainsareturnubs4.Tagutuple[ct.SelectorContains, ...]abooluCSSMatch.match_containsuCSSMatch.match_defaultuCSSMatch.match_indeterminateDaelalangsareturnubs4.Tagutuple[ct.SelectorLang, ...]abooluCSSMatch.match_langDaeladirectionalityareturnubs4.TagaintabooluCSSMatch.match_dirDaelaconditionareturnubs4.TagaintabooluCSSMatch.match_rangeuCSSMatch.match_defineduCSSMatch.match_placeholder_shownDaelaselectorsareturnubs4.Taguct.SelectorListabooluCSSMatch.match_selectorsTlDalimitareturnaintuIterator[bs4.Tag]Dareturnubs4.Tag | NoneuCSSMatch.closestDareturnulist[bs4.Tag]uCSSMatch.filteruCSSMatch.matcha__orig_bases__aImmutableaSoupSieveuCompiled Soup Sieve selector matching object.a__annotations__astruct.SelectorListuct.Namespaces | Noneudict[str, str]aintTapatternaselectorsanamespacesacustomaflagsa_hasha__slots__Dapatternaselectorsanamespacesacustomaflagsastruct.SelectorListuct.Namespaces | Noneuct.CustomSelectors | NoneaintuSoupSieve.__init__Datagareturnubs4.TagabooluSoupSieve.matchDatagareturnubs4.Tagubs4.TaguSoupSieve.closestDaiterableareturnuIterable[bs4.Tag]ulist[bs4.Tag]uSoupSieve.filteraselect_oneuSoupSieve.select_oneDatagalimitareturnubs4.Tagaintulist[bs4.Tag]uSoupSieve.selectDatagalimitareturnubs4.TagaintuIterator[bs4.Tag]Dareturnastra__repr__uSoupSieve.__repr__a__str__apickle_registerusoupsieve\css_match.pyTa.0anodeaselfu<module soupsieve.css_match>Ta__class__TaselfaelementTaselfapatternaselectorsanamespacesacustomaflagsa__class__TaselfaselectorsascopeanamespacesaflagsadocaparentarootachildTaselfTaclsatagTaselfacurrentaclosestTaselfatagTaelTaselfalang_rangealang_tagamatcharangesasubtagsalengthaslengtharindexasindexwrwsTaselfaiterableTaselfaelanodeadirectionavaluewcabidiTaclsaelanameadefaultavaluewkwvTaselfaelastartareverseatagsano_iframealastaindexaendaincranodeTaclsaelaclassesTaselfaelano_iframeTaselfaelatagsano_iframeanext_goodachildais_tagalast_childTaclsaelatagsasiblingTaselfaelano_iframeaparentTaelaformaparentalast_parentaselfTaselfaelaprefixTaselfaelanameTaselfaelanamespaceansTaelansTaobjTaclsaobjTaselfaelTaselfaelarootaparentTaselfatagalimitTaclsaelwkwvT
aselfaelaattraprefixavalueanswkwvanamespaceanameTaselfaelaattributesamatchwaatempapatternavalueTaselfaelaclassesacurrent_classesafoundwcTaselfaelacontainsamatchacontentacontain_listafoundatextwcTaselfaelamatchaformaparentafound_formwfwtachildanamewvT
aselfaeladirectionalityadirectionais_rootanameais_inputais_textareaais_bdiaitypeavaluewcabidiTaselfaelais_emptyachildTaselfaparentarelationarecursiveamatchachildrenachildTaselfaelarelationafoundasiblingTaselfaelaidsafoundwiTaselfaelamatchanameaget_parent_formaformafound_formwfwnwiacheckedachildatag_nameais_radioacheckahas_namewkwvTaselfaelalangsamatchahas_nsarootahas_html_namespaceaparentafound_langalastahas_html_nswkwvaattr_nsaattracacheafoundatagachildac_langacontentapatternsapatternTaselfaelatagamatchanamespaceadefault_namespaceatag_nsTaselfaelanthamatchedwnaparentalastalast_indexaindexarelative_indexwawbavaracountacount_incrafactoraidxalast_idxaadjustadiff_lowadiffadiff_highalowestachildTaselfaelachildTaselfaelarelationafoundaparentasiblingTaselfaelamatchacontentTaselfaelaconditionaout_of_rangeaitypeamnamxavalueTaselfaelarelationafoundTaselfaelais_rootasiblingTaselfaelaselectorsamatchais_notais_htmlanamespacesaiframe_restrictaselectorTaselfaelaselectorsamatchaselTaselfaelatagamatchTaselfaelataganameTaclsavalueanew_valuewvTaclsaitypeavalueaparsedwmayearamonthadayaweekahouraminutesTaselfalimitalimachildTaselfatagatagsTaelaattr_nameTayearamonthadayamax_daysTayearaweekamax_week.soupsieve.css_parser:naprocess_customacmaSoupSieveaCSSParserTacustomaflagsaprocess_selectorsuCached CSS compile.a_cached_css_compileacache_clearuPurge the cache.aitemsutoo many values to unpack (expected 2)autilaloweraRE_CUSTOMamatchaSelectorSyntaxErroruThe name 'uu' is not a valid custom pseudo-class nameacustom_selectorsuThe custom selector 'u' has already been registeredacss_unescapeuProcess custom.DwmareturnuMatch[str]astruReplace with the appropriate substitute.areplaceucss_unescape.<locals>.replaceaRE_CSS_ESCaRE_CSS_STR_ESCasubu
    Unescape CSS value.

    Strings allow for spanning the value on multiple strings by escaping a new line.
    agroupTl:lnnlaUNICODE_REPLACEMENT_CHARTlTlu�lw-w\astringaappendTu�lllwxw l0l9Tl-l_llAlZlalzuEscape identifier.anameareacompilewIwXwUare_patternuInitialize.uGet name.uMatch the selector.apatternsllapatternaselfamatched_nameaPAT_PSEUDO_CLASS_SPECIALare_pseudo_nameaget_nameTanameagetatagaidsaclassesaattributesanthaselectorsarelationsarel_typeacontainsalangaflagsano_matchaextendactaSelectorListafreezeuFreeze relation.aSelectorNullaSelectora_freeze_relationsuFreeze self.u_Selector(tag=u, ids=u, classes=u, attributes=u, nth=u, selectors=u, relations=u, rel_type=u, contains=u, lang=u, flags=u, no_match=w)uString representation.Twu�aDEBUGadebugacustomTacmpTacaseTaattr_ns:nlnTaattr_namewiaDOTALLatypeTavalueastartswithTTw"w':llnTw^u^%s.*aescapeTw$u.*?%s$Tw*u.*?%s.*Tw~aRE_WSasearchu[^\s\S]avalueu.*?(?:(?<=^)|(?<=[ \t\r\n\f]))%s(?=(?:[ \t\r\n\f]|$)).*Tw|u^%s(?:-.*)?$u^%s$Tw!aSelectorAttributeaattra_SelectoruCreate attribute selector from the returned regex match.Tatag_nsTatag_nameaSelectorTaguParse tag pattern from regex match.uUndefined custom selector 'u' found at position aendTlaFLG_PSEUDOTaflagsaselectoru
        Parse custom pseudo class alias.

        Compile custom selectors as we need them. When compiling a custom selector,
        set it to `None` in the dictionary so we can avoid an infinite loop.
        TaopenaPSEUDO_COMPLEXaparse_pseudo_openaPSEUDO_SIMPLEu:rootaSEL_ROOTu:definedaSEL_DEFINEDu:scopeaSEL_SCOPEu:emptyaSEL_EMPTYTu:linku:any-linkaCSS_LINKu:checkedaCSS_CHECKEDu:defaultaCSS_DEFAULTu:indeterminateaCSS_INDETERMINATEu:disabledaCSS_DISABLEDu:enabledaCSS_ENABLEDu:requiredaCSS_REQUIREDu:optionalaCSS_OPTIONALu:read-onlyaCSS_READ_ONLYu:read-writeaCSS_READ_WRITEu:in-rangeaCSS_IN_RANGEu:out-of-rangeaCSS_OUT_OF_RANGEu:placeholder-shownaCSS_PLACEHOLDER_SHOWNu:first-childaSelectorNthu:last-childu:first-of-typeu:last-of-typeu:only-childu:only-of-typeaPSEUDO_COMPLEX_NO_MATCHaparse_selectorsaFLG_OPENaPSEUDO_SIMPLE_NO_MATCHaPSEUDO_SUPPORTEDuInvalid syntax for pseudo class 'w'astartu' pseudo-class is not implemented at this timeuParse pseudo class.agroupdictTapseudo_nth_childa_childa_typeaevenaoddacastaMatchaRE_NTHTas1TwaaendswithTwnw1Tas2Twbw0l
TaofaCSS_NTH_OF_S_DEFAULTu:nth-childu:nth-last-childu:nth-of-typeu:nth-last-of-typeuParse `nth` pseudo.u:notaFLG_NOTu:hasaFLG_RELATIVETu:whereu:isaFLG_FORGIVEuParse pseudo with opening bracket.TarelationastripaWS_COMBINATORaCOMMA_COMBINATORlw:uThe multiple combinators at position uParse combinator tokens.uThe combinator 'u' at position u, must have a selector before it:nnnTw*nTw.uParse HTML classes and ids.u:containsawarningsawarnuThe pseudo class ':contains' is deprecated, ':-soup-contains' should be used moving forward.aFutureWarningu:-soup-contains-ownTavaluesaRE_VALUESafinditerTasplitTTw'w"aSelectorContainsuParse contains.aSelectorLanguParse pseudo language.TadiraltraSEL_DIR_LTRaSEL_DIR_RTLuParse pseudo direction.aFLG_HTMLaFLG_DEFAULTaFLG_INDETERMINATEaFLG_IN_RANGEaFLG_OUT_OF_RANGEaFLG_PLACEHOLDER_SHOWNaprintTu    is_pseudo: TrueTu    is_open: TrueTu    is_relative: TrueTu    is_not: TrueTu    is_html: TrueTu    is_default: TrueTu    is_indeterminate: TrueTu    is_in_range: TrueTu    is_out_of_range: TrueTu    is_placeholder_shown: TrueTu    is_forgive: Trueaiselectoraat_ruleuAt-rules found at position apseudo_class_customaparse_pseudo_class_customaselahas_selectorapseudo_classaparse_pseudo_classais_htmlapseudo_elementuPseudo-element found at position apseudo_containsaparse_pseudo_containsTapseudo_nth_typeapseudo_nth_childaparse_pseudo_nthapseudo_langaparse_pseudo_langapseudo_diraparse_pseudo_dirapseudo_closeais_forgiveuExpected a selector at position uUnmatched pseudo-class close at position acombineaparse_has_combinatoraindexutoo many values to unpack (expected 3)aparse_combinatorais_pseudoaattributeaparse_attribute_selectoruTag name found at position u instead of at the startaparse_tag_patternTaclassaidaparse_class_idwmuUnclosed pseudo-class at position aSEL_DEFAULTaSEL_INDETERMINATEaSEL_IN_RANGEaSEL_OUT_OF_RANGEaSEL_PLACEHOLDER_SHOWNuParse selectors.uIterate selector tokens.aRE_WS_BEGINaRE_WS_ENDu## PARSING: acss_tokensuTOKEN: 'u' --> u at position w[uMalformed attribute selector at position w.uMalformed class selector at position w#uMalformed id selector at position uMalformed pseudo-class selector at position uInvalid character u position Tu## END PARSINGaselector_iteruCSSParser.selector_iteruProcess selectors.uCSS selector parser.a__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationsalru_cacheTautilTacss_matchacss_matchTacss_typesacss_typesTaSelectorSyntaxErroraAnyaIteratorlSu:read-onlyu:emptyu:only-childu:defaultu:requiredu:last-of-typeu:linku:last-childu:only-of-typeu:optionalu:rootu:any-linku:read-writeu:definedu:scopeu:enabledu:checkedu:in-rangeu:first-childu:indeterminateu:first-of-typeu:out-of-rangeu:disabledu:placeholder-shownSu:user-invalidu:target-withinu:pastu:currentu:focus-visibleu:local-linku:targetu:focus-withinu:hoveru:visitedu:hostu:activeu:focusu:playingu:futureu:pausedSu:matchesu:-soup-containsu:notu:-soup-contains-ownu:hasu:whereu:containsu:isSu:hostu:host-contextu:currentSu:nth-last-childu:nth-last-of-typeu:langu:nth-of-typeu:diru:nth-childaPSEUDO_SPECIALu(?:\r\n|(?!\r\n)[\n\f\r])aNEWLINEu(?:[ \t]|aWSu(?:/\*[^*]*\*+(?:[^/*][^*]*\*+)*/)aCOMMENTSu(?:w|aWSCu(?:\\(?:[a-f0-9]{1,6}u?|[^\r\n\f]|$))aCSS_ESCAPESu?|[^\r\n\f]|$|u))aCSS_STRING_ESCAPESu
(?:(?:-?(?:[^\x00-\x2f\x30-\x40\x5B-\x5E\x60\x7B-\x9f]|u)+|--)
(?:[^\x00-\x2c\x2e\x2f\x3A-\x40\x5B-\x5E\x60\x7B-\x9f]|u)*)
aIDENTIFIERu(?:[-+])?(?:[0-9]+n?|n)(?:(?<=n)u*(?:[-+])u*(?:[0-9]+))?aNTHu(?:"(?:\\(?:.|u)|[^\\"\r\n\f]+)*?"|'(?:\\(?:.|u)|[^\\'\r\n\f]+)*?'|u+)aVALUEu*(?P<cmp>[!~^|*$]?=)u*(?P<value>u)(?:u*(?P<case>[is]))?)?u*\]aATTRu\#aPAT_IDu\.aPAT_CLASSu(?P<tag_ns>(?:u|\*)?\|)?(?P<tag_name>u|\*)aPAT_TAGu\[u*(?P<attr_ns>(?:u|\*)?\|)?(?P<attr_name>aPAT_ATTRu(?P<name>:u)(?P<open>\(u*)?aPAT_PSEUDO_CLASSu*)u(?P<name>:(?=--)aPAT_PSEUDO_CLASS_CUSTOMu*\)aPAT_PSEUDO_CLOSEaPAT_PSEUDO_ELEMENTu@PaPAT_AT_RULEu
(?P<pseudo_nth_child>u
(?P<nth_child>u|even|odd))(?:u*\)|(?P<of>w*u*ofu*))
aPAT_PSEUDO_NTH_CHILDu
(?P<pseudo_nth_type>u
(?P<nth_type>u|even|odd))u*\)
aPAT_PSEUDO_NTH_TYPEu(?P<values>u*,u)*)aPAT_PSEUDO_LANGu(?P<dir>ltr|rtl)aPAT_PSEUDO_DIRu*?(?P<relation>[,+>~]|u(?![,+>~]))aPAT_COMBINEaPAT_PSEUDO_CONTAINSu(?:(\\[a-f0-9]{1,6}u?)|(\\[^\r\n\f])|(\\$))u?)|(\\[^\r\n\f])|(\\$)|(\\u(?P<s1>[-+])?(?P<a>[0-9]+n?|n)(?:(?<=n)u*(?P<s2>[-+])u*(?P<b>[0-9]+))?u(?:(?P<value>u)|(?P<split>u*))w^u*$w$w,lll l@lllla_MAXCACHETamaxsizeDapatternanamespacesacustomaflagsareturnastruct.Namespaces | Noneuct.CustomSelectors | Noneaintucm.SoupSieveDareturnaNonea_purge_cacheDacustomareturnuct.CustomSelectors | Noneudict[str, str | ct.SelectorList]TFDacontentastringareturnastraboolastrDaidentareturnastrpusoupsieve.css_parsera__module__uSelector pattern.aSelectorPatterna__qualname__DanameapatternareturnastrpaNonea__init__uSelectorPattern.__init__DareturnastruSelectorPattern.get_nameDaselectoraindexaflagsareturnastraintpuMatch[str] | NoneuSelectorPattern.matchTa__prepare__aSpecialPseudoPatterna__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>Dapatternsareturnutuple[tuple[str, tuple[str, ...], str, type[SelectorPattern]], ...]aNoneuSpecialPseudoPattern.__init__uSpecialPseudoPattern.get_nameuSpecialPseudoPattern.matcha__orig_bases__u
    Intermediate selector class.

    This stores selector data for a compound selector as we are acquiring them.
    Once we are done collecting the data for a compound selector, we freeze
    the data in an object that can be pickled and hashed.
    DakwargsareturnaAnyaNoneu_Selector.__init__Darelationsareturnulist[_Selector]uct.SelectorListu_Selector._freeze_relationsDareturnuct.Selector | ct.SelectorNullu_Selector.freezea__str__u_Selector.__str__a__repr__uParse CSS selectors.Tu:containsu:-soup-containsu:-soup-contains-ownapseudo_nth_childTu:nth-childu:nth-last-childapseudo_nth_typeTu:nth-of-typeu:nth-last-of-typeTu:langTu:diraidaclassTnlDaselectoracustomaflagsareturnastrudict[str, str | ct.SelectorList] | NoneaintaNoneuCSSParser.__init__Daselwmahas_selectorareturna_SelectoruMatch[str]aboolpuCSSParser.parse_attribute_selectoruCSSParser.parse_tag_patternuCSSParser.parse_pseudo_class_customDaselwmahas_selectoraiselectorais_htmlareturna_SelectoruMatch[str]abooluIterator[tuple[str, Match[str]]]aboolutuple[bool, bool]uCSSParser.parse_pseudo_classDaselwmahas_selectoraiselectorareturna_SelectoruMatch[str]abooluIterator[tuple[str, Match[str]]]abooluCSSParser.parse_pseudo_nthDaselanameahas_selectoraiselectoraindexareturna_SelectorastrabooluIterator[tuple[str, Match[str]]]aintabooluCSSParser.parse_pseudo_openDaselwmahas_selectoraselectorsarel_typeaindexareturna_SelectoruMatch[str]aboolulist[_Selector]astraintutuple[bool, _Selector, str]uCSSParser.parse_has_combinatorDaselwmahas_selectoraselectorsarelationsais_pseudoais_forgiveaindexareturna_SelectoruMatch[str]aboolulist[_Selector]ulist[_Selector]aboolpaintutuple[bool, _Selector]uCSSParser.parse_combinatoruCSSParser.parse_class_iduCSSParser.parse_pseudo_containsuCSSParser.parse_pseudo_languCSSParser.parse_pseudo_dirTlpDaiselectoraindexaflagsareturnuIterator[tuple[str, Match[str]]]aintpuct.SelectorListuCSSParser.parse_selectorsDapatternareturnastruIterator[tuple[str, Match[str]]]Daindexaflagsareturnaintpuct.SelectorListuCSSParser.process_selectorsTuhtml|*:is(a, area)[href]Tu
    html|*:is(input[type=checkbox], input[type=radio])[checked], html|option[selected]
    Tu
    :checked,

    /*
    This pattern must be at the end.
    Special logic is applied to the last selector.
    */
    html|form html|*:is(button, input)[type="submit"]
    Tu
    html|input[type="checkbox"][indeterminate],
    html|input[type="radio"]:is(:not([name]), [name=""]):not([checked]),
    html|progress:not([value]),

    /*
    This pattern must be at the end.
    Special logic is applied to the last selector.
    */
    html|input[type="radio"][name]:not([name='']):not([checked])
    Tu
    html|*:is(input:not([type=hidden]), button, select, textarea, fieldset, optgroup, option, fieldset)[disabled],
    html|optgroup[disabled] > html|option,
    html|fieldset[disabled] > html|*:is(input:not([type=hidden]), button, select, textarea, fieldset),
    html|fieldset[disabled] >
        html|*:not(legend:nth-of-type(1)) html|*:is(input:not([type=hidden]), button, select, textarea, fieldset)
    Tu
    html|*:is(input:not([type=hidden]), button, select, textarea, fieldset, optgroup, option, fieldset):not(:disabled)
    Tuhtml|*:is(input, textarea, select)[required]Tuhtml|*:is(input, textarea, select):not([required])Tu
    html|input:is(
        :not([type]),
        [type=""],
        [type=text],
        [type=search],
        [type=url],
        [type=tel],
        [type=email],
        [type=password],
        [type=number]
    )[placeholder]:not([placeholder='']):is(:not([value]), [value=""]),
    html|textarea[placeholder]:not([placeholder=''])
    Tu*|*Tu
    html|*:is(
        textarea,
        input:is(
            :not([type]),
            [type=""],
            [type=text],
            [type=search],
            [type=url],
            [type=tel],
            [type=email],
            [type=number],
            [type=password],
            [type=date],
            [type=datetime-local],
            [type=month],
            [type=time],
            [type=week]
        )
    ):not([readonly], :disabled),
    html|*:is([contenteditable=""], [contenteditable="true" i])
    Tu
    html|*:not(:read-write)
    Tu
    html|input:is(
        [type="date"],
        [type="month"],
        [type="week"],
        [type="time"],
        [type="datetime-local"],
        [type="number"],
        [type="range"]
    ):is(
        [min],
        [max]
    )
    usoupsieve\css_parser.pyu<module soupsieve.css_parser>Ta__class__TaselfakwargsTaselfanameapatternTaselfapatternswpanameapatternapseudoTaselfaselectoracustomaflagsTaselfTapatternanamespacesacustomaflagsacustom_selectorsTaselfarelationsaselTacontentastringareplaceTaidentastringalengthastart_dashaindexwcacodepointTaselfaselectoraindexaflagsTaselfaselectoraindexaflagsapseudowmanameapatternTaselfaselwmahas_selectorainverseaopacaseansaattrais_typeapattern2avalueaflagsapatternasel_attrasub_selanot_listTaselfaselwmahas_selectoraselectorT
aselfaselwmahas_selectoraselectorsarelationsais_pseudoais_forgiveaindexacombinatorTaselfaselwmahas_selectoraselectorsarel_typeaindexacombinatorTaselfaselwmahas_selectoraiselectorais_htmlacomplex_pseudoapseudoTaselfaselwmahas_selectorapseudoaselectorT
aselfaselwmahas_selectorapseudoacontains_ownavaluesapatternsatokenavalueTaselfaselwmahas_selectoravalueTaselfaselwmahas_selectoravaluesapatternsatokenavalueTaselfaselwmahas_selectoraiselectoramdictapostfixacontentas1as2avaranth_partsa_s1waa_s2apseudo_selanth_selTaselfaselanameahas_selectoraiselectoraindexaflagsTaselfaiselectoraindexaflagsaselaselectorsahas_selectoraclosedarelationsarel_typeais_openais_pseudoais_relativeais_notais_htmlais_defaultais_indeterminateais_in_rangeais_out_of_rangeais_placeholder_shownais_forgiveakeywmTaselfaselwmahas_selectoraprefixatagTacustomacustom_selectorsakeyavalueanameTaselfaindexaflagsTwmacodepointavalueTaselfapatternwmaindexaendwvanamewcamsg.soupsieve.css_types
utoo many values to unpack (expected 2)atempaappenda__class__a__setattr__a_hashuInitialize.uGet base class.a__base__a__slots__uEqual.aotheraselfu<genexpr>uImmutable.__eq__.<locals>.<genexpr>uImmutable.__ne__.<locals>.<genexpr>uHash.w'a__name__uu' is immutableuPrevent mutability.u, :nlnw=w(w)uRepresentation.aprintaprettyuPretty print.a_validatea_dasortedaitemsavaluesu values must be hashableuValidate arguments.aHashableuImmutableDict._validate.<locals>.<genexpr>uIterator.uLength.uGet item: `namespace['key']`.a__init__u keys and values must be Unicode stringsuNamespaces._validate.<locals>.<genexpr>uCustomSelectors._validate.<locals>.<genexpr>Tatagaidsaclassesaattributesanthaselectorsarelationarel_typeacontainsalangaflagsTanameaprefixTaattributeaprefixapatternaxml_type_patternTatextaownTwawnwbaof_typealastaselectorsTalanguagesalanguagesuGet item.TTaselectorsais_notais_htmlaselectorsacopyregapicklea_pickleuAllow object to be pickled.uCSS selector structure items.a__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationslTaprettylaAnyaIteratoraPatternaIterableaMappingT
aSelectoraSelectorNullaSelectorTagaSelectorAttributeaSelectorContainsaSelectorNthaSelectorLangaSelectorListaNamespacesaCustomSelectorsa__all__aSEL_EMPTYlaSEL_ROOTlaSEL_DEFAULTlaSEL_INDETERMINATElaSEL_SCOPEl aSEL_DIR_LTRl@aSEL_DIR_RTLlaSEL_IN_RANGElaSEL_OUT_OF_RANGElaSEL_DEFINEDlaSEL_PLACEHOLDER_SHOWNusoupsieve.css_typesa__module__uImmutable.aImmutablea__qualname__a__annotations__Ta_hashutuple[str, ...]aintDakwargsareturnaAnyaNoneuImmutable.__init__Dareturnutype[Immutable]uImmutable.__base__DaotherareturnaAnyaboola__eq__uImmutable.__eq__a__ne__uImmutable.__ne__Dareturnainta__hash__uImmutable.__hash__DanameavalueareturnastraAnyaNoneuImmutable.__setattr__Dareturnastra__repr__uImmutable.__repr__a__str__DareturnaNoneuImmutable.prettya__prepare__aImmutableDicta__getitem__u%s.__prepare__() must return a mapping, not %su<metaclass>uHashable, immutable dictionary.Daargareturnudict[Any, Any] | Iterable[tuple[Any, Any]]aNoneuImmutableDict.__init__uImmutableDict._validateDareturnuIterator[Any]a__iter__uImmutableDict.__iter__a__len__uImmutableDict.__len__DakeyareturnaAnypuImmutableDict.__getitem__uImmutableDict.__hash__uImmutableDict.__repr__a__orig_bases__aNamespacesuNamespaces.Daargareturnudict[str, str] | Iterable[tuple[str, str]]aNoneuNamespaces.__init__uNamespaces._validateaCustomSelectorsuCustom selectors.uCustomSelectors.__init__uCustomSelectors._validateaSelectoruSelector.Tatagaidsaclassesaattributesanthaselectorsarelationarel_typeacontainsalangaflagsa_hashuSelectorTag | Noneatagaidsaclassesutuple[SelectorAttribute, ...]aattributesutuple[SelectorNth, ...]anthutuple[SelectorList, ...]aSelectorListarelationustr | Nonearel_typeutuple[SelectorContains, ...]acontainsutuple[SelectorLang, ...]alangaflagsDatagaidsaclassesaattributesanthaselectorsarelationarel_typeacontainsalangaflagsuSelectorTag | Noneutuple[str, ...]utuple[str, ...]utuple[SelectorAttribute, ...]utuple[SelectorNth, ...]utuple[SelectorList, ...]aSelectorListustr | Noneutuple[SelectorContains, ...]utuple[SelectorLang, ...]aintuSelector.__init__aSelectorNulluNull Selector.uSelectorNull.__init__aSelectorTaguSelector tag.Tanameaprefixa_hashastranameaprefixDanameaprefixareturnastrustr | NoneaNoneuSelectorTag.__init__aSelectorAttributeuSelector attribute rule.Taattributeaprefixapatternaxml_type_patterna_hashaattributeuPattern[str] | Noneapatternaxml_type_patternDaattributeaprefixapatternaxml_type_patternareturnastrpuPattern[str] | NoneuPattern[str] | NoneaNoneuSelectorAttribute.__init__aSelectorContainsuSelector contains rule.Tatextaowna_hashatextaboolaownDatextaownareturnuIterable[str]aboolaNoneuSelectorContains.__init__aSelectorNthuSelector nth type.Twawnwbaof_typealastaselectorsa_hashwawnwbaof_typealastDwawnwbaof_typealastaselectorsareturnaintaboolaintaboolpaSelectorListaNoneuSelectorNth.__init__aSelectorLanguSelector language rules.Talanguagesa_hashDalanguagesuIterable[str]uSelectorLang.__init__DareturnuIterator[str]uSelectorLang.__iter__uSelectorLang.__len__DaindexareturnaintastruSelectorLang.__getitem__uSelector list.Taselectorsais_notais_htmla_hashutuple[Selector | SelectorNull, ...]ais_notais_htmlTnFpDaselectorsais_notais_htmlareturnuIterable[Selector | SelectorNull] | NoneaboolpaNoneuSelectorList.__init__DareturnuIterator[Selector | SelectorNull]uSelectorList.__iter__uSelectorList.__len__DaindexareturnaintuSelector | SelectorNulluSelectorList.__getitem__DwpareturnaAnypDaobjareturnaAnyaNoneapickle_registerusoupsieve\css_types.pyTa.0wkwvTa.0akeyaotheraselfTa.0wvu<module soupsieve.css_types>Ta__class__TaclsTaselfaotherTaselfaindexTaselfakeyTaselfTaselfa__class__Taselfwawnwbaof_typealastaselectorsa__class__TaselfaargTaselfaarga__class__Taselfaattributeaprefixapatternaxml_type_patterna__class__Taselfakwargsatempwkwva__class__Taselfalanguagesa__class__Taselfanameaprefixa__class__Taselfaselectorsais_notais_htmla__class__T
aselfatagaidsaclassesaattributesanthaselectorsarelationarel_typeacontainsalangaflagsa__class__Taselfatextaowna__class__TaselfwrTaselfanameavalueTwpTaobj.soupsieve.prettyUlaindexaTOKENSaitemsutoo many values to unpack (expected 2)amatchaselaendTlTaclassalstrtadstrtatstrtaindentlaoutputaappendagroupuw
w TaparamaintakwordasqstradqstraemptyTalendadendatendTasepTlTadsepuMake the object output string pretty.u
Format a pretty string of a `SoupSieve` object for easy debugging.

This won't necessarily support all types and such, and definitely
not support custom outputs.

It is mainly geared towards our types as the `SelectorList`
object is a beast to look at without some indentation and newlines.
The format and various output types is fairly known (though it
hasn't been tested extensively to make sure we aren't missing corners).

Example:
-------
```
>>> import soupsieve as sv
>>> sv.compile('this > that.class[name=value]').selectors.pretty()
SelectorList(
    selectors=(
        Selector(
            tag=SelectorTag(
                name='that',
                prefix=None),
            ids=(),
            classes=(
                'class',
                ),
            attributes=(
                SelectorAttribute(
                    attribute='name',
                    prefix='',
                    pattern=re.compile(
                        '^value$'),
                    xml_type_pattern=None),
                ),
            nth=(),
            selectors=(),
            relation=SelectorList(
                selectors=(
                    Selector(
                        tag=SelectorTag(
                            name='this',
                            prefix=None),
                        ids=(),
                        classes=(),
                        attributes=(),
                        nth=(),
                        selectors=(),
                        relation=SelectorList(
                            selectors=(),
                            is_not=False,
                            is_html=False),
                        rel_type='>',
                        contains=(),
                        lang=(),
                        flags=0),
                    ),
                is_not=False,
                is_html=False),
            rel_type=None,
            contains=(),
            lang=(),
            flags=0),
        ),
    is_not=False,
    is_html=False)
```

a__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationsareaAnyacompileTu(?i)[a-z_][_a-z\d\.]+\(aRE_CLASSTu(?i)[_a-z][_a-z\d]+=aRE_PARAMTu\(\)|\[\]|\{\}aRE_EMPTYTu\[aRE_LSTRTTu\{aRE_DSTRTTu\(aRE_TSTRTTu\]aRE_LENDTu\}aRE_DENDTu\)aRE_TENDTu\d+aRE_INTTu(?i)[_a-z][_a-z\d]+aRE_KWORDTu"(?:\\.|[^"\\])*"aRE_DQSTRTu'(?:\\.|[^'\\])*'aRE_SQSTRTu\s*(,)\s*aRE_SEPTu\s*(:)\s*aRE_DSEPaclassaparamaemptyalstrtadstrtatstrtalendadendatendasqstrasepadsepaintakwordadqstrDaobjareturnaAnyastraprettyusoupsieve\pretty.pyu<module soupsieve.pretty>T
aobjaselaindexaendaindentaoutputwmwkwvaname.soupsieve.utilkanew_stringaappendaUC_AaUC_Zl uuLower.alineacolacontextaget_pattern_contextutoo many values to unpack (expected 3)u
  line u:
a__class__a__init__uInitialize.DafuncareturnuCallable[..., Any]uCallable[..., Any]a_wrapperudeprecated.<locals>._wrapperu
    Raise a `DeprecationWarning` when wrapped function/method is called.

    Usage:

        @deprecated("This method will be removed in version X; use Y instead.")
        def some_method()"
            pass
    awrapsDaargsakwargsareturnaAnyppa_deprecated_funcudeprecated.<locals>._wrapper.<locals>._deprecated_funcawarningsawarnw'afunca__name__u' is deprecated. amessageaDeprecationWarningastacklevelTacategoryastackleveluWarn deprecated.llaRE_PATTERN_LINE_SPLITafinditeralastastartTlagroupatextlaindexaendu--> u    Tw
w w^acurrent_lineuGet the pattern context.uUtility.a__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationsalru_cacheareaCallableaAnyaDEBUGacompileTu(?:\r\n|(?!\r\n)[\n\r])|$lAlZTlTamaxsizeDastringareturnastrpalowerTEExceptiona__prepare__aSelectorSyntaxErrora__getitem__u%s.__prepare__() must return a mapping, not %su<metaclass>usoupsieve.utila__module__uSyntax error in a CSS selector.a__qualname__TnnDamsgapatternaindexareturnastrustr | Noneuint | NoneaNoneuSelectorSyntaxError.__init__a__orig_bases__TlDamessageastacklevelareturnastraintuCallable[..., Any]adeprecatedDamessageastacklevelareturnastraintaNoneawarn_deprecatedDapatternaindexareturnastraintutuple[str, int, int]usoupsieve\util.pyu<module soupsieve.util>Ta__class__Taselfamsgapatternaindexa__class__TaargsakwargsafuncamessageastacklevelTafuncamessageastacklevelTafunca_deprecated_funcTamessageastacklevelTamessageastacklevela_wrapperTapatternaindexalastacurrent_lineacolatextalineaoffsetwmalinetextaindentTastringanew_stringwcwo.urllib3._base_connectionK8a__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationsatypinguutil.connectionTa_TYPE_SOCKET_OPTIONSla_TYPE_SOCKET_OPTIONSluutil.timeoutTa_DEFAULT_TIMEOUTa_TYPE_TIMEOUTa_DEFAULT_TIMEOUTa_TYPE_TIMEOUTuutil.urlTaUrlaUrlaUnionaIOaAnyaIterablea_TYPE_BODYaNamedTuplea__prepare__aProxyConfiga__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>uurllib3._base_connectiona__module__a__qualname__a__annotations__ussl.SSLContext | Noneassl_contextaboolause_forwarding_for_httpsuNone | str | Literal[False]aassert_hostnameustr | Noneaassert_fingerprinta__orig_bases__a_ResponseOptionsastrarequest_methodarequest_urlapreload_contentadecode_contentaenforce_content_lengthuurllib3\_base_connection.pyu<module urllib3._base_connection>Ta__class__u.urllib3._collections!aHTTPHeaderDictatypingaMappingacastTOstrpaIterableaTupleakeysa__getitem__aHasGettableStringKeysa__class__a__init__a_maxsizeadispose_funcaOrderedDicta_containeraRLockalocka__enter__a__exit__apopTnnnapopitemTFTalastutoo many values to unpack (expected 2)avalueuIteration over this class is unlikely to be threadsafe.avaluesaclearaselfa_headersaiteritemsaitema_has_value_for_headera_copy_fromaextendadecodeTulatin-1aloweru, :lnnasetdefaultaensure_can_construct_http_header_dictaitermergeda__eq__la__iter__uHTTPHeaderDict.__iter__laappenduAdds a (name, value) pair, doesn't overwrite the value if it already
        exists.

        If this is called with combine=True, instead of adding a new header value
        as a distinct item during iteration, this will instead append the value to
        any existing header value with a comma. If no existing header value exists
        for the key, then the value will simply be added, ignoring the combine parameter.

        >>> headers = HTTPHeaderDict(foo='bar')
        >>> headers.add('Foo', 'baz')
        >>> headers['foo']
        'bar, baz'
        >>> list(headers.items())
        [('foo', 'bar'), ('foo', 'baz')]
        >>> headers.add('foo', 'quz', combine=True)
        >>> list(headers.items())
        [('foo', 'bar, baz, quz')]
        uextend() takes at most 1 positional arguments (uu given)TaaddaitemsuGeneric import function for any type of header-like object.
        Adapted version of MutableMapping.update in order to insert items
        with self.add instead of self.__setitem__
        a_Sentinelanot_passeduReturns a list of all the values for the named field. Returns an
        empty list if the key doesn't exist.LuContent-EncodinguContent-LanguageuContent-LocationuContent-TypeuContent-LengthaDigestuLast-Modifiedadiscardu
        Remove content-specific header fields before changing the request
        method to GET or HEAD according to RFC 9110, Section 15.4.
        a__name__w(w)aotheragetlistuIterate over all header lines, including duplicate ones.uHTTPHeaderDict.iteritemsuIterate over all headers, merging duplicate ones together.uHTTPHeaderDict.itermergedaHTTPHeaderDictItemViewacopya__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationsacollectionsTaOrderedDictaenumTaEnumaautoaEnumaautoathreadingTaRLockaRecentlyUsedContainera__all__aTypeVarTa_KTa_KTTa_VTa_VTTa_DTa_DTaUnionaValidHTTPHeaderSourcea__prepare__u%s.__prepare__() must return a mapping, not %su<metaclass>uurllib3._collectionsa__module__a__qualname__a__orig_bases__DapotentialareturnaobjectuValidHTTPHeaderSource | NoneaGenericaMutableMappingu
    Provides a thread-safe dict-like container which maintains up to
    ``maxsize`` keys while throwing away the least-recently-used keys beyond
    ``maxsize``.

    :param maxsize:
        Maximum number of recent elements to retain.

    :param dispose_func:
        Every time an item is evicted from the container,
        ``dispose_func(value)`` is called.  Callback which will get called
    a__annotations__utyping.OrderedDict[_KT, _VT]aintutyping.Callable[[_VT], None] | NoneTl
nDamaxsizeadispose_funcareturnaintutyping.Callable[[_VT], None] | NoneaNoneuRecentlyUsedContainer.__init__Dakeyareturna_KTa_VTuRecentlyUsedContainer.__getitem__Dakeyavalueareturna_KTa_VTaNonea__setitem__uRecentlyUsedContainer.__setitem__Dakeyareturna_KTaNonea__delitem__uRecentlyUsedContainer.__delitem__Dareturnainta__len__uRecentlyUsedContainer.__len__Dareturnutyping.NoReturnuRecentlyUsedContainer.__iter__DareturnaNoneuRecentlyUsedContainer.clearDareturnuset[_KT]uRecentlyUsedContainer.keysaSetu
    HTTPHeaderDict is unusual for a Mapping[str, str] in that it has two modes of
    address.

    If we directly try to get an item with a particular name, we will get a string
    back that is the concatenated version of all the values:

    >>> d['X-Header-Name']
    'Value1, Value2, Value3'

    However, if we iterate over an HTTPHeaderDict's items, we will optionally combine
    these values based on whether combine=True was called when building up the dictionary

    >>> d = HTTPHeaderDict({"A": "1", "B": "foo"})
    >>> d.add("A", "2", combine=True)
    >>> d.add("B", "bar")
    >>> list(d.items())
    [
        ('A', '1, 2'),
        ('B', 'foo'),
        ('B', 'bar'),
    ]

    This class conforms to the interface required by the MutableMapping ABC while
    also giving us the nonstandard iteration behavior we want; items with duplicate
    keys, ordered by time of first insertion.
    DaheadersareturnaHTTPHeaderDictaNoneuHTTPHeaderDictItemView.__init__uHTTPHeaderDictItemView.__len__Dareturnutyping.Iterator[tuple[str, str]]uHTTPHeaderDictItemView.__iter__Daitemareturnaobjectaboola__contains__uHTTPHeaderDictItemView.__contains__u
    :param headers:
        An iterable of field-value pairs. Must not contain multiple field names
        when compared case-insensitively.

    :param kwargs:
        Additional field-value pairs to pass in to ``dict.update``.

    A ``dict`` like container for storing HTTP Headers.

    Field names are stored and compared case-insensitively in compliance with
    RFC 7230. Iteration provides the first case-sensitive key seen for each
    case-insensitive pair.

    Using ``__setitem__`` syntax overwrites fields that compare equal
    case-insensitively in order to maintain ``dict``'s api. For fields that
    compare equal, instead create a new ``HTTPHeaderDict`` and use ``.add``
    in a loop.

    If multiple fields that are equal case-insensitively are passed to the
    constructor or ``.update``, the behavior is undefined and some will be
    lost.

    >>> headers = HTTPHeaderDict()
    >>> headers.add('Set-Cookie', 'foo=bar')
    >>> headers.add('set-cookie', 'baz=quxx')
    >>> headers['content-length'] = '7'
    >>> headers['SET-cookie']
    'foo=bar, baz=quxx'
    >>> headers['Content-Length']
    '7'
    utyping.MutableMapping[str, list[str]]TnDaheadersakwargsuValidHTTPHeaderSource | NoneastruHTTPHeaderDict.__init__DakeyavalareturnastrpaNoneuHTTPHeaderDict.__setitem__DakeyareturnastrpuHTTPHeaderDict.__getitem__DakeyareturnastraNoneuHTTPHeaderDict.__delitem__DakeyareturnaobjectabooluHTTPHeaderDict.__contains__TuDakeyadefaultareturnastrppuHTTPHeaderDict.setdefaultDaotherareturnaobjectabooluHTTPHeaderDict.__eq__a__ne__uHTTPHeaderDict.__ne__uHTTPHeaderDict.__len__Dareturnutyping.Iterator[str]uHTTPHeaderDict.discardDacombineFDakeyavalacombineareturnastrpaboolaNoneuHTTPHeaderDict.addDaargsakwargsareturnaValidHTTPHeaderSourceastraNoneuHTTPHeaderDict.extendaoverloadDakeyareturnastrulist[str]uHTTPHeaderDict.getlistDakeyadefaultareturnastra_DTulist[str] | _DTDakeyadefaultareturnastru_Sentinel | _DTulist[str] | _DTDareturnaSelfa_prepare_for_method_changeuHTTPHeaderDict._prepare_for_method_changeagetheadersagetallmatchingheadersaigetaget_allDareturnastra__repr__uHTTPHeaderDict.__repr__DaotherareturnaHTTPHeaderDictaNoneuHTTPHeaderDict._copy_fromDareturnaHTTPHeaderDictuHTTPHeaderDict.copyDareturnaHTTPHeaderDictItemViewuHTTPHeaderDict.itemsDaheader_nameapotential_valueareturnastrpabooluHTTPHeaderDict._has_value_for_headerDaotherareturnaobjectaHTTPHeaderDicta__ior__uHTTPHeaderDict.__ior__a__or__uHTTPHeaderDict.__or__a__ror__uHTTPHeaderDict.__ror__uurllib3\_collections.pyu<module urllib3._collections>Ta__class__Taselfaitemapassed_keyapassed_valTaselfakeyTaselfakeyavalueTaselfaotheramaybe_constructableaother_as_http_header_dictTaselfakeyaitemTaselfakeyavalTaselfaheadersTaselfaheadersakwargsa__class__Taselfamaxsizeadispose_funca__class__Taselfaotheramaybe_constructableTaselfTaselfavalsTaselfaotherTaselfaotheramaybe_constructablearesultTaselfakeyavalueaevicted_itemw_aevicted_valueTaselfaotherakeyavalTaselfaheader_nameapotential_valueTaselfacontent_specific_headersaheaderTaselfakeyavalacombineakey_loweranew_valsavalsTaselfavaluesavalueTaselfacloneTapotentialTaselfaargsakwargsaotherakeyavalavalueTaselfakeyadefaultTaselfakeyadefaultavalsTaselfakeyavalsavalTaselfakeyadefaulta__class__.urllib3._request_methodsbaheadersuClasses extending RequestMethods must implement their own ``urlopen`` method.aupperurequest got values for both 'body' and 'json' parameters which are mutually exclusiveacopyucontent-typealowerakeysuapplication/jsonuContent-Typea_jsonadumpsDaseparatorsaensure_asciiTw,w:FaencodeTuutf-8abodyaselfa_encode_url_methodsarequest_encode_urlafieldsaurlopen_kwarequest_encode_bodyu
        Make a request using :meth:`urlopen` with the appropriate encoding of
        ``fields`` based on the ``method`` used.

        This is a convenience method that requires the least amount of manual
        effort. It can be used in most situations, while still having the
        option to drop down to more specific methods when necessary, such as
        :meth:`request_encode_url`, :meth:`request_encode_body`,
        or even the lowest level :meth:`urlopen`.
        w?aurlencodeaurlopenu
        Make a request using :meth:`urlopen` with the ``fields`` encoded in
        the url. This is useful for request methods like GET, HEAD, DELETE, etc.
        aHTTPHeaderDicturequest got values for both 'fields' and 'body', can only specify one.aencode_multipart_formdataTaboundaryutoo many values to unpack (expected 2)uapplication/x-www-form-urlencodedasetdefaultaextra_kwu
        Make a request using :meth:`urlopen` with the ``fields`` encoded in
        the body. This is useful for request methods like POST, PUT, PATCH, etc.

        When ``encode_multipart=True`` (default), then
        :func:`urllib3.encode_multipart_formdata` is used to encode
        the payload with the appropriate content type. Otherwise
        :func:`urllib.parse.urlencode` is used with the
        'application/x-www-form-urlencoded' content type.

        Multipart encoding must be used when posting files, and it's reasonably
        safe to use it in other times too. However, it may break request
        signing, such as with OAuth.

        Supports an optional ``fields`` parameter of key/value strings AND
        key/filetuple. A filetuple is a (filename, data, MIME type) tuple where
        the MIME type is optional. For example::

            fields = {
                'foo': 'bar',
                'fakefile': ('foofile.txt', 'contents of foofile'),
                'realfile': ('barfile.txt', open('realfile').read()),
                'typedfile': ('bazfile.bin', open('bazfile').read(),
                              'image/jpeg'),
                'nonamefile': 'contents of nonamefile field',
            }

        When uploading a file, providing a filename (the first parameter of the
        tuple) is optional but recommended to best mimic behavior of browsers.

        Note that if ``headers`` are supplied, the 'Content-Type' header will
        be overwritten because it depends on the dynamic random boundary string
        which is used to compose the body of the request. The random boundary
        string can be explicitly set with the ``multipart_boundary`` parameter.
        a__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationsajsonlatypinguurllib.parseTaurlencodea_base_connectionTa_TYPE_BODYla_TYPE_BODYa_collectionsTaHTTPHeaderDictafilepostTa_TYPE_FIELDSaencode_multipart_formdataa_TYPE_FIELDSaresponseTaBaseHTTPResponseaBaseHTTPResponseaRequestMethodsa__all__aUnionaSequenceaTupleTOstrObytesaMappinga_TYPE_ENCODE_URL_FIELDSuurllib3._request_methodsa__module__u
    Convenience mixin for classes who implement a :meth:`urlopen` method, such
    as :class:`urllib3.HTTPConnectionPool` and
    :class:`urllib3.PoolManager`.

    Provides behavior for making common types of HTTP request methods and
    decides which type of request field encoding to use.

    Specifically,

    :meth:`.request_encode_url` is for sending requests whose fields are
    encoded in the URL (such as GET, HEAD, DELETE).

    :meth:`.request_encode_body` is for sending requests whose fields are
    encoded in the *body* of the request using multipart or www-form-urlencoded
    (such as for POST, PUT, PATCH).

    :meth:`.request` is for making any kind of request, it will look up the
    appropriate encoding format and use one of the above two methods to make
    the request.

    Initializer parameters:

    :param headers:
        Headers to include with all requests, unless other headers are given
        explicitly.
    a__qualname__SaGETaHEADaOPTIONSaDELETETnDaheadersareturnutyping.Mapping[str, str] | NoneaNonea__init__uRequestMethods.__init__TnntnDamethodaurlabodyaheadersaencode_multipartamultipart_boundaryakwareturnastrpu_TYPE_BODY | Noneutyping.Mapping[str, str] | Noneaboolustr | Noneutyping.AnyaBaseHTTPResponseuRequestMethods.urlopenTnnnnDamethodaurlabodyafieldsaheadersajsonaurlopen_kwareturnastrpu_TYPE_BODY | Noneu_TYPE_FIELDS | Noneutyping.Mapping[str, str] | Noneutyping.Any | Noneutyping.AnyaBaseHTTPResponsearequestuRequestMethods.requestTnnDamethodaurlafieldsaheadersaurlopen_kwareturnastrpu_TYPE_ENCODE_URL_FIELDS | Noneutyping.Mapping[str, str] | NoneastraBaseHTTPResponseuRequestMethods.request_encode_urlDamethodaurlafieldsaheadersaencode_multipartamultipart_boundaryaurlopen_kwareturnastrpu_TYPE_FIELDS | Noneutyping.Mapping[str, str] | Noneaboolustr | NoneastraBaseHTTPResponseuRequestMethods.request_encode_bodyTuurllib3\_request_methods.pyu<module urllib3._request_methods>TaselfaheadersTaselfamethodaurlabodyafieldsaheadersajsonaurlopen_kwTaselfamethodaurlafieldsaheadersaencode_multipartamultipart_boundaryaurlopen_kwaextra_kwabodyacontent_typeTaselfamethodaurlafieldsaheadersaurlopen_kwaextra_kwTaselfamethodaurlabodyaheadersaencode_multipartamultipart_boundaryakwu.urllib3._versiona__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationsu2.1.0a__version__uurllib3\_version.pyu<module urllib3._version>u.urllib3.connection;a__class__a__init__aTimeoutaresolve_default_timeoutTahostaportatimeoutasource_addressablocksizeasocket_optionsaproxyaproxy_configa_has_connected_to_proxya_response_optionsa_tunnel_hosta_tunnel_porta_tunnel_schemea_dns_hostarstripTw.u
        Getter method to remove any trailing dots that indicate the hostname is an FQDN.

        In general, SSL certificates don't include the trailing dot indicating a
        fully-qualified domain name, and thus, they don't validate properly when
        checked against a domain name that includes the dot. In addition, some
        servers may not expect to receive the trailing dot when provided.

        However, the hostname with trailing dot is critical to DNS resolution; doing a
        lookup with the trailing dot will properly only resolve the appropriate FQDN,
        whereas a lookup without a trailing dot will search the system's search domain
        list. Thus, it's important to keep the original host around for use only in
        those cases where it's appropriate (i.e., when doing DNS lookup to establish the
        actual TCP connection across which we're going to send HTTP requests).
        u
        Setter for the `host` property.

        We assume that only urllib3 uses the _dns_host attribute; httplib itself
        only uses `host`, and it seems reasonable that other libraries follow suit.
        aconnectionacreate_connectionaportatimeoutasource_addressTasource_addressasocket_optionsasocketagaierroraNameResolutionErrorahostaSocketTimeoutaConnectTimeoutErroruConnection to uu timed out. (connect timeout=w)aNewConnectionErroruFailed to establish a new connection: a_HAS_SYS_AUDITasysaaudituhttp.client.connectuEstablish a socket connection and set nodelay settings on it.

        :return: New socket connection.
        TahttpahttpsuInvalid proxy scheme for tunneling: u, must be either 'http' or 'https'aset_tunnelTaportaheadersa_new_connasocka_tunnelawait_for_readDatimeoutZacloseais_verifiedaproxy_is_verifieda_CONTAINS_CONTROL_CHAR_REasearchuMethod cannot contain non-token characters u (found at least agroupaputrequestTaskip_hostaskip_accept_encodingaputheaderato_straloweraSKIPPABLE_HEADERSu', 'asortedatitleuurllib3.util.SKIP_HEADER only supports 'w'aSKIP_HEADERu<genexpr>uHTTPConnection.putheader.<locals>.<genexpr>asettimeouta_ResponseOptionsTarequest_methodarequest_urlapreload_contentadecode_contentaenforce_content_lengthuaccept-encodingTaskip_accept_encodingaskip_hostabody_to_chunksablocksizeTamethodablocksizeachunksacontent_lengthutransfer-encodingTuTransfer-Encodingachunkeducontent-lengthuContent-Lengthuuser-agentuUser-Agenta_get_default_user_agentaitemsutoo many values to unpack (expected 2)aselfaendheadersaencodeTuutf-8asendc%x
%b
Tc0

uHTTPConnection.request.<locals>.<genexpr>awarningsawarnaDeprecationWarninglTuHTTPConnection.request_chunked() is deprecated and will be removed in urllib3 v2.1.0. Instead use HTTPConnection.request(..., chunked=True).TacategoryastacklevelarequestTabodyaheadersachunkedu
        Alternative to the common request method, which sends the
        body with chunked encoding and not as one block
        aResponseNotReadyaresponseTaHTTPResponselaHTTPResponselagetresponseaassert_header_parsingamsgaHeaderParsingErroralogawarninguFailed to parse headers (url=%s): %sa_url_from_connectionarequest_urlDaexc_infotaHTTPHeaderDictastatusaversionareasonaresp_optionsapreload_contentadecode_contentaenforce_content_lengtharequest_methodTabodyaheadersastatusaversionareasonapreload_contentadecode_contentaoriginal_responseaenforce_content_lengtharequest_methodarequest_urlu
        Get the response from the server.

        If the HTTPConnection is in the correct state, returns an instance of HTTPResponse or of whatever object is returned by the response_class variable.

        If a request has not been sent or if a previous response has not be handled, ResponseNotReady is raised. If the HTTP response indicates that the connection should be closed, then it will be closed before the response is returned. When the connection is closed, the underlying socket is closed.
        Taportatimeoutasource_addressablocksizeasocket_optionsaproxyaproxy_configakey_fileacert_fileakey_passwordassl_contextaserver_hostnameaassert_hostnameaassert_fingerprintassl_versionassl_minimum_versionassl_maximum_versionaexpanduseraca_certsaca_cert_diraca_cert_dataaverify_modearesolve_cert_reqsTnacert_reqsTuHTTPSConnection.set_cert() is deprecated and will be removed in urllib3 v2.1.0. Instead provide the parameters to the HTTPSConnection constructor.u
        This method should only be called once, before the connection is used.
        ahttpsa_connect_tls_proxyadatetimeadateatodayaRECENT_DATEuSystem time is way off (before u). This will probably lead to SSL verification errorsaSystemTimeWarninga_ssl_wrap_socket_and_match_hostnameTasockacert_reqsassl_versionassl_minimum_versionassl_maximum_versionaca_certsaca_cert_diraca_cert_dataacert_fileakey_fileakey_passwordaserver_hostnameassl_contextatls_in_tlsaassert_hostnameaassert_fingerprintatypingacastaProxyConfigTacert_reqsassl_versionassl_minimum_versionassl_maximum_versionaca_certsaca_cert_diraca_cert_dataaserver_hostnameassl_contextaassert_hostnameaassert_fingerprintacert_fileakey_fileakey_passwordatls_in_tlsu
        Establish a TLS connection to the proxy using the provided SSL context.
        acreate_urllib3_contextaresolve_ssl_versionTassl_versionassl_minimum_versionassl_maximum_versionacert_reqsassl_aIS_PYOPENSSLaHAS_NEVER_CHECK_COMMON_NAMEacheck_hostnameaload_default_certsastripTu[]w%arfindTw%ais_ipaddressassl_wrap_socketT
asockakeyfileacertfileakey_passwordaca_certsaca_cert_diraca_cert_dataaserver_hostnameassl_contextatls_in_tlsa_assert_fingerprintagetpeercertTtTabinary_formasslaCERT_NONEahostname_checks_common_namea_match_hostnamea_WrappedAndVerifiedSocketassl_sockaCERT_REQUIREDTasocketais_verifieduLogic for constructing an SSLContext from all TLS parameters, passing
    that down into ssl_wrap_socket, and then doing certificate verification
    either via hostname or fingerprint. This function exists to guarantee
    that both proxies and targets have the same behavior when connecting via TLS.
    amatch_hostnameaasserted_hostnameaCertificateErroruCertificate did not match expected hostname: %s. Certificate: %sa_peer_certw areasplitu[^a-z]uwrong version numberuunknown protocolaProxyErroruUnable to connect to proxyu. Your proxy appears to only use HTTP and not HTTPS, try changing your proxy URL to be HTTP. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#https-proxy-error-http-proxya__cause__upython-urllib3/a__version__aHTTPSConnectionahttpaUrlTaschemeahostaportapathaurluReturns the URL from a given connection. This is mainly used for testing and logging.a__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationsaloggingaosuhttp.clientTaHTTPConnectionaHTTPConnectiona_HTTPConnectionTaHTTPExceptionaHTTPExceptionTaResponseNotReadyTatimeouta_collectionsTaHTTPHeaderDictuutil.responseTaassert_header_parsinguutil.timeoutTa_DEFAULT_TIMEOUTa_TYPE_TIMEOUTaTimeouta_DEFAULT_TIMEOUTa_TYPE_TIMEOUTuutil.utilTato_struutil.waitTawait_for_readaSSLErroraBaseSSLErrorTEImportErrorEAttributeErrorTEBaseExceptiona__prepare__a__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>uurllib3.connectiona__module__a__qualname__a__orig_bases__a_base_connectionTa_TYPE_BODYa_TYPE_BODYTaProxyConfigTa_ResponseOptionsa_versionTa__version__aexceptionsTaConnectTimeoutErroraHeaderParsingErroraNameResolutionErroraNewConnectionErroraProxyErroraSystemTimeWarningautilTaSKIP_HEADERaSKIPPABLE_HEADERSaconnectionassl_uutil.requestTabody_to_chunksuutil.ssl_Taassert_fingerprintTacreate_urllib3_contextais_ipaddressaresolve_cert_reqsaresolve_ssl_versionassl_wrap_socketuutil.ssl_match_hostnameTaCertificateErroramatch_hostnameuutil.urlTaUrlaConnectionErroraBrokenPipeErroragetLoggerTuurllib3.connectionDahttpahttpslPlaport_by_schemeTllpacompileTu[^-!#$%&'*+.^_`|~0-9a-zA-Z]u
    Based on :class:`http.client.HTTPConnection` but provides an extra constructor
    backwards-compatibility layer between older and newer Pythons.

    Additional keyword parameters are used to configure attributes of the connection.
    Accepted parameters include:

    - ``source_address``: Set the source address for the current connection.
    - ``socket_options``: Set specific options on the underlying socket. If not specified, then
      defaults are loaded from ``HTTPConnection.default_socket_options`` which includes disabling
      Nagle's algorithm (sets TCP_NODELAY to 1) unless the connection is behind a proxy.

      For example, if you wish to enable TCP Keep Alive in addition to the defaults,
      you might pass:

      .. code-block:: python

         HTTPConnection.default_socket_options + [
             (socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1),
         ]

      Or you may want to disable the defaults by passing an empty list (e.g., ``[]``).
    a__annotations__adefault_portutyping.ClassVar[int]aIPPROTO_TCPaTCP_NODELAYadefault_socket_optionsutyping.ClassVar[connection._TYPE_SOCKET_OPTIONS]aboolubool | Noneaintutuple[str, int] | Noneuconnection._TYPE_SOCKET_OPTIONS | Noneu_ResponseOptions | Noneustr | Noneuint | Nonel@Dahostaportatimeoutasource_addressablocksizeasocket_optionsaproxyaproxy_configareturnastruint | Nonea_TYPE_TIMEOUTutuple[str, int] | NoneaintuNone | connection._TYPE_SOCKET_OPTIONSuUrl | NoneuProxyConfig | NoneaNoneuHTTPConnection.__init__apropertyDareturnastruHTTPConnection.hostasetterDavalueareturnastraNoneDareturnusocket.socketuHTTPConnection._new_connTnnahttpDahostaportaheadersaschemeareturnastruint | Noneutyping.Mapping[str, str] | NoneastraNoneuHTTPConnection.set_tunnelDareturnaNoneaconnectuHTTPConnection.connectDareturnaboolais_closeduHTTPConnection.is_closedais_connecteduHTTPConnection.is_connectedahas_connected_to_proxyuHTTPConnection.has_connected_to_proxyuHTTPConnection.closeTFpDamethodaurlaskip_hostaskip_accept_encodingareturnastrpaboolpaNoneuHTTPConnection.putrequestDaheaderavaluesareturnastrpaNoneuHTTPConnection.putheaderTnnDachunkedapreload_contentadecode_contentaenforce_content_lengthFtppDamethodaurlabodyaheadersachunkedapreload_contentadecode_contentaenforce_content_lengthareturnastrpu_TYPE_BODY | Noneutyping.Mapping[str, str] | NoneaboolpppaNoneuHTTPConnection.requestDamethodaurlabodyaheadersareturnastrpu_TYPE_BODY | Noneutyping.Mapping[str, str] | NoneaNonearequest_chunkeduHTTPConnection.request_chunkedDareturnaHTTPResponseuHTTPConnection.getresponseu
    Many of the parameters to this constructor are passed to the underlying SSL
    socket by means of :py:func:`urllib3.util.ssl_wrap_socket`.
    uint | str | NoneuNone | str | bytesDahostaportatimeoutasource_addressablocksizeasocket_optionsaproxyaproxy_configacert_reqsaassert_hostnameaassert_fingerprintaserver_hostnameassl_contextaca_certsaca_cert_diraca_cert_dataassl_minimum_versionassl_maximum_versionassl_versionacert_fileakey_fileakey_passwordareturnastruint | Nonea_TYPE_TIMEOUTutuple[str, int] | NoneaintuNone | connection._TYPE_SOCKET_OPTIONSuUrl | NoneuProxyConfig | Noneuint | str | NoneuNone | str | Literal[False]ustr | Noneustr | Noneussl.SSLContext | Noneustr | Noneustr | NoneuNone | str | bytesuint | Noneuint | Noneuint | str | Noneustr | Noneustr | Noneustr | NoneaNoneuHTTPSConnection.__init__TnnnnnnnnnD
akey_fileacert_fileacert_reqsakey_passwordaca_certsaassert_hostnameaassert_fingerprintaca_cert_diraca_cert_dataareturnustr | Noneustr | Noneuint | str | Noneustr | Noneustr | NoneuNone | str | Literal[False]ustr | Noneustr | NoneuNone | str | bytesaNoneaset_certuHTTPSConnection.set_certuHTTPSConnection.connectDahostnameasockareturnastrusocket.socketussl.SSLSocketuHTTPSConnection._connect_tls_proxyaNamedTupleu
    Wrapped socket and whether the connection is
    verified after the TLS handshake
    ussl.SSLSocket | SSLTransportDatls_in_tlsFDasockacert_reqsassl_versionassl_minimum_versionassl_maximum_versionacert_fileakey_fileakey_passwordaca_certsaca_cert_diraca_cert_dataaassert_hostnameaassert_fingerprintaserver_hostnameassl_contextatls_in_tlsareturnusocket.socketuNone | str | intuNone | str | intuint | Noneuint | Noneustr | Noneustr | Noneustr | Noneustr | Noneustr | NoneuNone | str | bytesuNone | str | Literal[False]ustr | Noneustr | Noneussl.SSLContext | Noneaboola_WrappedAndVerifiedSocketTFDacertaasserted_hostnameahostname_checks_common_nameareturnu_TYPE_PEER_CERT_RET_DICT | NoneastraboolaNoneDaerraproxy_schemeareturnaExceptionustr | NoneaProxyErrora_wrap_proxy_erroruUsed to detect a failed ConnectionCls import.aDummyConnectionTaVerifiedHTTPSConnectionDaconnapathareturnuHTTPConnection | HTTPSConnectionustr | Noneastruurllib3\connection.pyTa.0wkTa.0wvTa__class__u<module urllib3.connection>T
aselfahostaportatimeoutasource_addressablocksizeasocket_optionsaproxyaproxy_configa__class__Taselfahostaportatimeoutasource_addressablocksizeasocket_optionsaproxyaproxy_configacert_reqsaassert_hostnameaassert_fingerprintaserver_hostnameassl_contextaca_certsaca_cert_diraca_cert_dataassl_minimum_versionassl_maximum_versionassl_versionacert_fileakey_fileakey_passworda__class__Taselfahostnameasockaproxy_configassl_contextasock_and_verifiedTacertaasserted_hostnameahostname_checks_common_nameastripped_hostnameweTaselfasockweTasockacert_reqsassl_versionassl_minimum_versionassl_maximum_versionacert_fileakey_fileakey_passwordaca_certsaca_cert_diraca_cert_dataaassert_hostnameaassert_fingerprintaserver_hostnameassl_contextatls_in_tlsacertadefault_ssl_contextacontextanormalizedassl_sockahostname_checks_common_nameTaconnapathaschemeTaerraproxy_schemeaerror_normalizedais_likely_http_proxyahttp_proxy_warninganew_errTaselfa__class__TaselfTaselfasockaserver_hostnameatls_in_tlsais_time_offasock_and_verifiedTaselfaresp_optionsaHTTPResponseahttplib_responseahpeaheadersaresponsea__class__TaselfavalueTaselfaheaderavaluesaskippable_headersa__class__Taselfamethodaurlaskip_hostaskip_accept_encodingamatcha__class__Taselfamethodaurlabodyaheadersachunkedapreload_contentadecode_contentaenforce_content_lengthaheader_keysaskip_accept_encodingaskip_hostachunks_and_clachunksacontent_lengthaheaderavalueachunkTaselfamethodaurlabodyaheadersT
aselfakey_fileacert_fileacert_reqsakey_passwordaca_certsaassert_hostnameaassert_fingerprintaca_cert_diraca_cert_dataTaselfahostaportaheadersaschemea__class__.urllib3.connectionpoolSoaLocationValueErrorTuNo host specified.a_normalize_hostaschemeTaschemeahostaportanormalize_hostalowera_tunnel_hosta__name__uu(host=u, port=w)acloseaConnectionPoola__init__aRequestMethodsaTimeoutafrom_floataRetryaDEFAULTatimeoutaretriesaQueueClsapoolablockaproxyaproxy_headersaproxy_configaselfaputTnlanum_connectionsanum_requestsaconn_kwasetdefaultasocket_optionsaweakrefafinalizea_close_pool_connectionslalogadebuguStarting new HTTP connection (%d): %s:%su80aConnectionClsaconnect_timeoutu
        Return a fresh :class:`HTTPConnection`.
        aClosedPoolErroruPool is closed.agetTablockatimeoutaqueueaEmptyaEmptyPoolErroruPool is empty and a new connection can't be opened due to blocking mode.ais_connection_droppeduResetting dropped connection: %saconna_new_connu
        Get a connection. Will return a pooled connection if one is available.

        If no connections are available and :prop:`.block` is ``False``, then a
        fresh connection is returned.

        :param timeout:
            Seconds to wait before giving up and raising
            :class:`urllib3.exceptions.EmptyPoolError` if the pool is empty and
            :prop:`.block` is ``True``.
        DablockFaFullaFullPoolErroruPool reached maximum size and no more connections are allowed.awarninguConnection pool is full, discarding connection: %s. Connection pool size: %saqsizeu
        Put a connection back into the pool.

        :param conn:
            Connection object for the current host and port as returned by
            :meth:`._new_conn` or :meth:`._get_conn`.

        If the pool is already full, the connection is closed and discarded
        because we exceeded maxsize. If connections are discarded frequently,
        then maxsize should be increased.

        If the pool is closed, then the connection will be closed and discarded.
        a_DEFAULT_TIMEOUTacloneuHelper that always returns a :class:`urllib3.util.Timeout`aSocketTimeoutaReadTimeoutErroruRead timed out. (read timeout=aerrnoa_blocking_errnosaerruIs the error actually a timeout? Will raise a ReadTimeout or passa_get_timeoutastart_connectaresolve_default_timeouta_validate_connaBaseSSLErrora_raise_timeoutTaerraurlatimeout_valueaNewConnectionErroraTimeoutErroraCertificateErroraSSLErrorahas_connected_to_proxya_wrap_proxy_errorarequestTabodyaheadersachunkedapreload_contentadecode_contentaenforce_content_lengthaBrokenPipeErroraEPROTOTYPEaread_timeoutais_closedaurlagetresponsea_connectiona_poolu%s://%s:%s "%s %s %s" %s %samethoda_http_vsn_strastatusalength_remainingu
        Perform a request on a given urllib connection object taken from our
        pool.

        :param conn:
            a connection from one of our connection pools

        :param method:
            HTTP request method (such as GET, POST, PUT, etc.)

        :param url:
            The URL to perform the request on.

        :param body:
            Data to send in the request body, either :class:`str`, :class:`bytes`,
            an iterable of :class:`str`/:class:`bytes`, or a file-like object.

        :param headers:
            Dictionary of custom headers to send, such as User-Agent,
            If-None-Match, etc. If None, pool headers are used. If provided,
            these headers completely replace any pool-specific headers.

        :param retries:
            Configure the number of retries to allow before raising a
            :class:`~urllib3.exceptions.MaxRetryError` exception.

            Pass ``None`` to retry until you receive a response. Pass a
            :class:`~urllib3.util.retry.Retry` object for fine-grained control
            over different types of retries.
            Pass an integer number to retry connection errors that many times,
            but no other types of errors. Pass zero to never retry.

            If ``False``, then retries are disabled and any exception is raised
            immediately. Also, instead of raising a MaxRetryError on redirects,
            the redirect response will be returned.

        :type retries: :class:`~urllib3.util.retry.Retry`, False, or an int.

        :param timeout:
            If specified, overrides the default timeout for this one
            request. It may be a float (in seconds) or an instance of
            :class:`urllib3.util.Timeout`.

        :param chunked:
            If True, urllib3 will send the body using chunked transfer
            encoding. Otherwise, urllib3 will send the body using the standard
            content-length form. Defaults to False.

        :param response_conn:
            Set this to ``None`` if you will handle releasing the connection or
            set the connection to have the response release it.

        :param preload_content:
          If True, the response's body will be preloaded during construction.

        :param decode_content:
            If True, will attempt to decode the body based on the
            'content-encoding' header.

        :param enforce_content_length:
            Enforce content length checking. Body returned by server must match
            value of Content-Length header, if present. Otherwise, raise error.
        utoo many values to unpack (expected 2)u
        Close all pooled connections and disable the pool.
        astartswithTw/aparse_urlunot enough values to unpack (expected at least 4, got %d)ahttpaport_by_schemeu
        Check if the given ``url`` is a member of the same host as this
        connection pool.
        aheadersafrom_intTaredirectadefaultais_same_hostaHostChangedErrorato_stra_encode_targetaconnection_requires_http_tunnelacopyaupdateaset_file_positiona_get_connTatimeouta_prepare_proxya_make_requestabodyachunkedaresponse_connapreload_contentadecode_contentaHTTPExceptionaProtocolErroraProxyErroruConnection aborted.aincrementasysaexc_infolTaerrora_poola_stacktraceasleepa_put_connuRetrying (%r) after connection broken by '%r': %saurlopenaredirectapool_timeoutarelease_connabody_posaresponseaget_redirect_locationl/aGETaHTTPHeaderDicta_prepare_for_method_changeTaresponsea_poolaMaxRetryErroraraise_on_redirectadrain_connasleep_for_retryuRedirecting %s -> %saassert_same_hostTuRetry-Afterais_retryaraise_on_statusuRetry: %su
        Get a connection from the pool and perform an HTTP request. This is the
        lowest level call for making a request, so you'll need to specify all
        the raw details.

        .. note::

           More commonly, it's appropriate to use a convenience method
           such as :meth:`request`.

        .. note::

           `release_conn` will only behave as expected if
           `preload_content=False` because we want to make
           `preload_content=False` the default behaviour someday soon without
           breaking backwards compatibility.

        :param method:
            HTTP request method (such as GET, POST, PUT, etc.)

        :param url:
            The URL to perform the request on.

        :param body:
            Data to send in the request body, either :class:`str`, :class:`bytes`,
            an iterable of :class:`str`/:class:`bytes`, or a file-like object.

        :param headers:
            Dictionary of custom headers to send, such as User-Agent,
            If-None-Match, etc. If None, pool headers are used. If provided,
            these headers completely replace any pool-specific headers.

        :param retries:
            Configure the number of retries to allow before raising a
            :class:`~urllib3.exceptions.MaxRetryError` exception.

            Pass ``None`` to retry until you receive a response. Pass a
            :class:`~urllib3.util.retry.Retry` object for fine-grained control
            over different types of retries.
            Pass an integer number to retry connection errors that many times,
            but no other types of errors. Pass zero to never retry.

            If ``False``, then retries are disabled and any exception is raised
            immediately. Also, instead of raising a MaxRetryError on redirects,
            the redirect response will be returned.

        :type retries: :class:`~urllib3.util.retry.Retry`, False, or an int.

        :param redirect:
            If True, automatically handle redirects (status codes 301, 302,
            303, 307, 308). Each redirect counts as a retry. Disabling retries
            will disable redirect, too.

        :param assert_same_host:
            If ``True``, will make sure that the host of the pool requests is
            consistent else will raise HostChangedError. When ``False``, you can
            use the pool on an HTTP proxy and request foreign hosts.

        :param timeout:
            If specified, overrides the default timeout for this one
            request. It may be a float (in seconds) or an instance of
            :class:`urllib3.util.Timeout`.

        :param pool_timeout:
            If set and the pool is set to block=True, then this method will
            block for ``pool_timeout`` seconds and raise EmptyPoolError if no
            connection is available within the time period.

        :param bool preload_content:
            If True, the response's body will be preloaded into memory.

        :param bool decode_content:
            If True, will attempt to decode the body based on the
            'content-encoding' header.

        :param release_conn:
            If False, then the urlopen call will not release the connection
            back into the pool once a response is received (but will release if
            you read the entire contents of the response such as when
            `preload_content=True`). This is useful if you're not preloading
            the response's content immediately. You will need to call
            ``r.release_conn()`` on the response ``r`` to return the connection
            back into the pool. If None, it takes the value of ``preload_content``
            which defaults to ``True``.

        :param bool chunked:
            If True, urllib3 will send the body using chunked transfer
            encoding. Otherwise, urllib3 will send the body using the standard
            content-length form. Defaults to False.

        :param int body_pos:
            Position to seek to in file-like body in the event of a retry or
            redirect. Typically this won't need to be set because urllib3 will
            auto-populate the value when needed.
        a__class__akey_fileacert_fileacert_reqsakey_passwordaca_certsaca_cert_dirassl_versionassl_minimum_versionassl_maximum_versionaassert_hostnameaassert_fingerprintahttpsaset_tunnelTaschemeahostaportaheadersaconnectuEstablishes a tunnel connection through HTTP CONNECT.uStarting new HTTPS connection (%d): %s:%su443aDummyConnectionuCan't connect to HTTPS URL because the SSL module is not available.u
        Return a fresh :class:`urllib3.connection.HTTPConnection`.
        ais_verifiedawarningsawarnuUnverified HTTPS request is being made to host 'u'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warningsaInsecureRequestWarningu
        Called right before a request is made, after the socket is created.
        lPaHTTPSConnectionPoolaHTTPConnectionPoolu
    Given a url, return an :class:`.ConnectionPool` instance of its host.

    This is a shortcut for not having to parse out the scheme, host, and port
    of the url before creating an :class:`.ConnectionPool` instance.

    :param url:
        Absolute URL string that must include the scheme. Port is optional.

    :param \**kw:
        Passes additional parameters to the constructor of the appropriate
        :class:`.ConnectionPool`. Useful for specifying things like
        timeout, maxsize, headers, etc.

    Example::

        >>> conn = connection_from_url('http://google.com/')
        >>> r = conn.request('GET', '/')
    Tw[aendswithTw]:llnu
    Normalize hosts for comparisons and use with sockets.
    aUrlTaschemeahostaportapathuReturns the URL from a given connection pool. This is mainly used for testing and logging.TFTablockuDrains a queue of connections and closes each one.a__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationsaloggingatypingasocketaTracebackTypea_base_connectionTa_TYPE_BODYa_TYPE_BODYa_collectionsTaHTTPHeaderDicta_request_methodsTaRequestMethodsaconnectionTaBaseSSLErroraBrokenPipeErroraDummyConnectionaHTTPConnectionaHTTPExceptionaHTTPSConnectionaProxyConfiga_wrap_proxy_erroraHTTPConnectionaHTTPSConnectionaProxyConfigTaport_by_schemeaexceptionsT
aClosedPoolErroraEmptyPoolErroraFullPoolErroraHostChangedErroraInsecureRequestWarningaLocationValueErroraMaxRetryErroraNewConnectionErroraProtocolErroraProxyErroraReadTimeoutErroraSSLErroraTimeoutErrorTaBaseHTTPResponseaBaseHTTPResponseuutil.connectionTais_connection_droppeduutil.proxyTaconnection_requires_http_tunneluutil.requestTa_TYPE_BODY_POSITIONaset_file_positiona_TYPE_BODY_POSITIONuutil.retryTaRetryuutil.ssl_match_hostnameTaCertificateErroruutil.timeoutTa_DEFAULT_TIMEOUTa_TYPE_DEFAULTaTimeouta_TYPE_DEFAULTuutil.urlTaUrla_encode_targetTa_normalize_hostTaparse_urluutil.utilTato_stragetLoggerTuurllib3.connectionpoolaUniona_TYPE_TIMEOUTaTypeVarTa_SelfTa_SelfTuurllib3.connectionpoola__module__u
    Base class for all connection pools, such as
    :class:`.HTTPConnectionPool` and :class:`.HTTPSConnectionPool`.

    .. note::
       ConnectionPool.urlopen() does not normalize or percent-encode target URIs
       which is useful if your target server doesn't support percent-encoded
       target URIs.
    a__qualname__a__annotations__ustr | NoneaLifoQueueDahostaportareturnastruint | NoneaNoneuConnectionPool.__init__Dareturnastra__str__uConnectionPool.__str__Daselfareturna_SelfTpa__enter__uConnectionPool.__enter__Daexc_typeaexc_valaexc_tbareturnutype[BaseException] | NoneuBaseException | NoneuTracebackType | NoneuLiteral[False]a__exit__uConnectionPool.__exit__DareturnaNoneuConnectionPool.closeTaEAGAINaEWOULDBLOCKa__prepare__a__getitem__u%s.__prepare__() must return a mapping, not %su<metaclass>u
    Thread-safe connection pool for one host.

    :param host:
        Host used for this HTTP Connection (e.g. "localhost"), passed into
        :class:`http.client.HTTPConnection`.

    :param port:
        Port used for this HTTP Connection (None is equivalent to 80), passed
        into :class:`http.client.HTTPConnection`.

    :param timeout:
        Socket timeout in seconds for each individual connection. This can
        be a float or integer, which sets the timeout for the HTTP request,
        or an instance of :class:`urllib3.util.Timeout` which gives you more
        fine-grained control over request timeouts. After the constructor has
        been parsed, this is always a `urllib3.util.Timeout` object.

    :param maxsize:
        Number of connections to save that can be reused. More than 1 is useful
        in multithreaded situations. If ``block`` is set to False, more
        connections will be created but they will not be saved once they've
        been used.

    :param block:
        If set to True, no more than ``maxsize`` connections will be used at
        a time. When no free connections are available, the call will block
        until a connection has been released. This is a useful side effect for
        particular multithreaded situations where one does not want to use more
        than maxsize connections per host to prevent flooding.

    :param headers:
        Headers to include with all requests, unless other headers are given
        explicitly.

    :param retries:
        Retry configuration to use by default with requests in this pool.

    :param _proxy:
        Parsed proxy URL, should not be used directly, instead, see
        :class:`urllib3.ProxyManager`

    :param _proxy_headers:
        A dictionary with proxy headers, should not be used directly,
        instead, see :class:`urllib3.ProxyManager`

    :param \**conn_kw:
        Additional parameters are used to create fresh :class:`urllib3.connection.HTTPConnection`,
        :class:`urllib3.connection.HTTPSConnection` instances.
    utype[BaseHTTPConnection] | type[BaseHTTPSConnection]Dahostaportatimeoutamaxsizeablockaheadersaretriesa_proxya_proxy_headersa_proxy_configaconn_kwastruint | Noneu_TYPE_TIMEOUT | Noneaintaboolutyping.Mapping[str, str] | NoneuRetry | bool | int | NoneuUrl | Noneutyping.Mapping[str, str] | NoneuProxyConfig | Noneutyping.AnyuHTTPConnectionPool.__init__DareturnaBaseHTTPConnectionuHTTPConnectionPool._new_connDatimeoutareturnufloat | NoneaBaseHTTPConnectionuHTTPConnectionPool._get_connDaconnareturnuBaseHTTPConnection | NoneaNoneuHTTPConnectionPool._put_connDaconnareturnaBaseHTTPConnectionaNoneuHTTPConnectionPool._validate_connuHTTPConnectionPool._prepare_proxyDatimeoutareturna_TYPE_TIMEOUTaTimeoutuHTTPConnectionPool._get_timeoutDaerraurlatimeout_valueareturnuBaseSSLError | OSError | SocketTimeoutastru_TYPE_TIMEOUT | NoneaNoneuHTTPConnectionPool._raise_timeoutD
aconnamethodaurlabodyaheadersaretriesatimeoutachunkedaresponse_connapreload_contentadecode_contentaenforce_content_lengthareturnaBaseHTTPConnectionastrpu_TYPE_BODY | Noneutyping.Mapping[str, str] | NoneuRetry | Nonea_TYPE_TIMEOUTabooluBaseHTTPConnection | NoneaboolppaBaseHTTPResponseuHTTPConnectionPool._make_requestuHTTPConnectionPool.closeDaurlareturnastrabooluHTTPConnectionPool.is_same_hostDamethodaurlabodyaheadersaretriesaredirectaassert_same_hostatimeoutapool_timeoutarelease_connachunkedabody_posapreload_contentadecode_contentaresponse_kwareturnastrpu_TYPE_BODY | Noneutyping.Mapping[str, str] | NoneuRetry | bool | int | Noneaboolpa_TYPE_TIMEOUTuint | Noneubool | Noneaboolu_TYPE_BODY_POSITION | Noneaboolputyping.AnyaBaseHTTPResponseuHTTPConnectionPool.urlopena__orig_bases__u
    Same as :class:`.HTTPConnectionPool`, but HTTPS.

    :class:`.HTTPSConnection` uses one of ``assert_fingerprint``,
    ``assert_hostname`` and ``host`` in this order to verify connections.
    If ``assert_hostname`` is False, no verification is done.

    The ``key_file``, ``cert_file``, ``cert_reqs``, ``ca_certs``,
    ``ca_cert_dir``, ``ssl_version``, ``key_password`` are only used if :mod:`ssl`
    is available and are fed into :meth:`urllib3.util.ssl_wrap_socket` to upgrade
    the connection socket into an SSL socket.
    utype[BaseHTTPSConnection]Dahostaportatimeoutamaxsizeablockaheadersaretriesa_proxya_proxy_headersakey_fileacert_fileacert_reqsakey_passwordaca_certsassl_versionassl_minimum_versionassl_maximum_versionaassert_hostnameaassert_fingerprintaca_cert_diraconn_kwareturnastruint | Noneu_TYPE_TIMEOUT | Noneaintaboolutyping.Mapping[str, str] | NoneuRetry | bool | int | NoneuUrl | Noneutyping.Mapping[str, str] | Noneustr | Noneustr | Noneuint | str | Noneustr | Noneustr | Noneuint | str | Noneussl.TLSVersion | Noneussl.TLSVersion | Noneustr | Literal[False] | Noneustr | Noneustr | Noneutyping.AnyaNoneuHTTPSConnectionPool.__init__DaconnareturnaHTTPSConnectionaNoneuHTTPSConnectionPool._prepare_proxyDareturnaBaseHTTPSConnectionuHTTPSConnectionPool._new_connuHTTPSConnectionPool._validate_connDaurlakwareturnastrutyping.AnyaHTTPConnectionPoolaconnection_from_urlaoverloadDahostaschemeareturnaNoneustr | NoneaNoneDahostaschemeareturnastrustr | NoneastrDahostaschemeareturnustr | Noneustr | Noneustr | NoneDapoolapathareturnuHTTPConnectionPool | HTTPSConnectionPoolustr | Noneastra_url_from_poolDapoolareturnuqueue.LifoQueue[typing.Any]aNoneuurllib3\connectionpool.pyu<module urllib3.connectionpool>Ta__class__TaselfTaselfaexc_typeaexc_valaexc_tbTaselfahostaportTaselfahostaportatimeoutamaxsizeablockaheadersaretriesa_proxya_proxy_headersa_proxy_configaconn_kww_apoolTaselfahostaportatimeoutamaxsizeablockaheadersaretriesa_proxya_proxy_headersakey_fileacert_fileacert_reqsakey_passwordaca_certsassl_versionassl_minimum_versionassl_maximum_versionaassert_hostnameaassert_fingerprintaca_cert_diraconn_kwa__class__TapoolaconnTaselfatimeoutaconnTaselfatimeoutTaselfaconnamethodaurlabodyaheadersaretriesatimeoutachunkedaresponse_connapreload_contentadecode_contentaenforce_content_lengthanew_eatimeout_objwearead_timeoutaresponseTaselfaactual_hostaactual_portTaselfaconnTahostaschemeTaselfaconnatunnel_schemeTaselfaerraurlatimeout_valueTapoolapathTaselfaconna__class__Taselfaold_poolTaurlakwaschemew_ahostaportTaselfaurlaschemew_ahostaportTaselfamethodaurlabodyaheadersaretriesaredirectaassert_same_hostatimeoutapool_timeoutarelease_connachunkedabody_posapreload_contentadecode_contentaresponse_kwanew_eaparsed_urladestination_schemeaconnarelease_this_connahttp_tunnel_requiredaerraclean_exitatimeout_objwearesponse_connaresponsearedirect_locationahas_retry_after.urllib35
raloggingagetLoggerTaurllib3aStreamHandlerasetFormatteraFormatterTu%(asctime)s %(levelname)s %(message)saaddHandlerasetLeveladebugTuAdded a stderr logging handler to logger: %saurllib3u
    Helper for quickly adding a StreamHandler to the logger. Useful for
    debugging.

    Returns the handler after adding it.
    awarningsasimplefilteraignoreu
    Helper for quickly disabling all urllib3 warnings.
    a_DEFAULT_POOLarequestTabodyafieldsaheadersapreload_contentadecode_contentaredirectaretriesatimeoutajsonu
    A convenience, top-level request method. It uses a module-global ``PoolManager`` instance.
    Therefore, its side effects could be shared across dependencies relying on it.
    To avoid side effects create a new ``PoolManager`` instance and use it instead.
    The method does not accept low-level ``**urlopen_kw`` keyword arguments.
    u
Python HTTP library with thread-safe connection pooling, file post support, user friendly, and more
a__doc__a__file__apathadirnameaenvironagetTaNUITKA_PACKAGE_urllib3u\not_existinga__path__a__spec__aoriginahas_locationasubmodule_search_locationsa__cached__aannotationslatypingTaNullHandleraNullHandleruTaexceptionslaexceptionsa_base_connectionTa_TYPE_BODYa_TYPE_BODYa_collectionsTaHTTPHeaderDictaHTTPHeaderDicta_versionTa__version__a__version__aconnectionpoolTaHTTPConnectionPoolaHTTPSConnectionPoolaconnection_from_urlaHTTPConnectionPoolaHTTPSConnectionPoolaconnection_from_urlafilepostTa_TYPE_FIELDSaencode_multipart_formdataa_TYPE_FIELDSaencode_multipart_formdataapoolmanagerTaPoolManageraProxyManageraproxy_from_urlaPoolManageraProxyManageraproxy_from_urlaresponseTaBaseHTTPResponseaHTTPResponseaBaseHTTPResponseaHTTPResponseuutil.requestTamake_headersamake_headersuutil.retryTaRetryaRetryuutil.timeoutTaTimeoutaTimeoutasslaOPENSSL_VERSIONastartswithTuOpenSSL awarnuurllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with u. See: https://github.com/urllib3/urllib3/issues/3020aNotOpenSSLWarningaOPENSSL_VERSION_INFOTlppu. See: https://github.com/urllib3/urllib3/issues/2168uAndrey Petrov (andrey.petrov@shazow.net)a__author__aMITa__license__TaHTTPConnectionPoolaHTTPHeaderDictaHTTPSConnectionPoolaPoolManageraProxyManageraHTTPResponseaRetryaTimeoutaadd_stderr_loggeraconnection_from_urladisable_warningsaencode_multipart_formdataamake_headersaproxy_from_urlarequestaBaseHTTPResponsea__all__aDEBUGDalevelareturnaintulogging.StreamHandler[typing.TextIO]aadd_stderr_loggeraalwaysaSecurityWarningDaappendtadefaultaInsecurePlatformWarningaHTTPWarningDacategoryareturnutype[Warning]aNoneadisable_warningsDabodyafieldsaheadersapreload_contentadecode_contentaredirectaretriesatimeoutajsonnnntppnlnDamethodaurlabodyafieldsaheadersapreload_contentadecode_contentaredirectaretriesatimeoutajsonareturnastrpu_TYPE_BODY | Noneu_TYPE_FIELDS | Noneutyping.Mapping[str, str] | Noneubool | Noneubool | Noneubool | NoneuRetry | bool | int | NoneuTimeout | float | int | Noneutyping.Any | NoneaBaseHTTPResponseuurllib3\__init__.pyu<module urllib3>TalevelaloggerahandlerTacategoryTamethodaurlabodyafieldsaheadersapreload_contentadecode_contentaredirectaretriesatimeoutajson.urllib3.contrib'a__doc__a__file__apathadirnameajoinaenvironagetTaNUITKA_PACKAGE_urllib3u\not_existingacontribTaNUITKA_PACKAGE_urllib3_contribu\not_existinga__path__a__spec__aoriginahas_locationasubmodule_search_locationsa__cached__uurllib3\contrib\__init__.pyu<module urllib3.contrib>u.urllib3.contrib.pyopenssl;#Fa_validate_dependencies_metaPyOpenSSLContextautilaSSLContextassl_aIS_PYOPENSSLuMonkey-patch urllib3 with PyOpenSSL-backed SSL-support.aorig_util_SSLContextuUndo monkey-patching by :func:`inject_into_urllib3`.ucryptography.x509.extensionsTaExtensionslaExtensionsaget_extension_for_classu'cryptography' module missing required functionality.  Try upgrading to v1.3.4 or newer.uOpenSSL.cryptoTaX509aX509a_x509u'pyOpenSSL' module missing required functionality. Try upgrading to v0.14 or newer.u
    Verifies that PyOpenSSL's package-level dependencies have been met.
    Throws `ImportError` if they are not met.
    Danameareturnastrubytes | Noneu
        Borrowed wholesale from the Python Cryptography Project. It turns out
        that we can't just safely call `idna.encode`: it can explode for
        wildcard names. This avoids that problem.
        aidna_encodeu_dnsname_to_stdlib.<locals>.idna_encodew:adecodeTuutf-8u
    Converts a dNSName SubjectAlternativeName field to the form used by the
    standard library on the given Python version.

    Cryptography produces a dNSName as a unicode string that was idna-decoded
    from ASCII bytes. We need to idna-encode that string to get it back, and
    then on Python 3 we also need to convert to unicode via UTF-8 (the stdlib
    uses PyUnicode_FromStringAndSize on it, which decodes via UTF-8).

    If the name cannot be idna-encoded then we return None signalling that
    the name given should be skipped.
    aidnaTu*.w.anameastartswithaencodeTaasciiacoreaIDNAErrorato_cryptographyaextensionsax509aSubjectAlternativeNameavalueaExtensionNotFoundaDuplicateExtensionaUnsupportedExtensionaUnsupportedGeneralNameTypealogawarninguA problem was encountered with the certificate that prevented urllib3 from finding the SubjectAlternativeName field. This can affect certificate validation. The error was %sa_dnsname_to_stdlibaget_values_for_typeaDNSNameaDNSaextendaIPAddressu
    Given an PyOpenSSL certificate, provides all the subject alternative names.
    uIP Addressu<genexpr>uget_subj_alt_name.<locals>.<genexpr>aconnectionasocketasuppress_ragged_eofsa_io_refsa_closedafilenolaclosearecvaOpenSSLaSSLaSysCallErroraargsTluUnexpected EOFcweaZeroReturnErroraget_shutdownaRECEIVED_SHUTDOWNaWantReadErrorawait_for_readagettimeoutatimeoutTuThe read operation timed outaErrorasslaSSLErroruread error: uarecv_intoasettimeoutaselfasendadataaWantWriteErrorawait_for_writeatotal_senta_send_until_doneaSSL_WRITE_BLOCKSIZEashutdowna_real_closeaget_peer_certificateacryptoadump_certificateaFILETYPE_ASN1asubjectacommonNameaget_subjectaCNasubjectAltNameaget_subj_alt_nameaget_protocol_version_namea_openssl_versionsaprotocolaContexta_ctxa_optionsacheck_hostnameaTLSVersionaMINIMUM_SUPPORTEDa_minimum_versionaMAXIMUM_SUPPORTEDa_maximum_versiona_set_ctx_optionsa_openssl_to_stdlib_verifyaget_verify_modeaset_verifya_stdlib_to_openssl_verifya_verify_callbackaset_default_verify_pathsaset_cipher_listaload_verify_locationsaBytesIOuunable to load trusted certificates: ause_certificate_chain_fileaset_passwd_cbu<lambda>uPyOpenSSLContext.load_cert_chain.<locals>.<lambda>ause_privatekey_fileuUnable to load certificate chain: apasswordato_bytesaasciiaset_alpn_protosaConnectionais_ipaddressaserver_hostnameaset_tlsext_host_nameacnxaset_connect_stateado_handshakeasockTuselect timed outubad handshake: aWrappedSocketaset_optionsa_openssl_to_ssl_minimum_versiona_openssl_to_ssl_maximum_versionu
Module for using pyOpenSSL as a TLS backend. This module was relevant before
the standard library ``ssl`` module supported SNI, but now that we've dropped
support for Python 2.7 all relevant Python versions support SNI so
**this module is no longer recommended**.

This needs the following packages installed:

* `pyOpenSSL`_ (tested with 16.0.0)
* `cryptography`_ (minimum 1.3.4, from pyopenssl)
* `idna`_ (minimum 2.0)

However, pyOpenSSL depends on cryptography, so while we use all three directly here we
end up having relatively few packages required.

You can install them with the following command:

.. code-block:: bash

    $ python -m pip install pyopenssl cryptography idna

To activate certificate checking, call
:func:`~urllib3.contrib.pyopenssl.inject_into_urllib3` from your Python code
before you begin making HTTP requests. This can be done in a ``sitecustomize``
module, or at any other time before your application begins using ``urllib3``,
like this:

.. code-block:: python

    try:
        import urllib3.contrib.pyopenssl
        urllib3.contrib.pyopenssl.inject_into_urllib3()
    except ImportError:
        pass

.. _pyopenssl: https://www.pyopenssl.org
.. _cryptography: https://cryptography.io
.. _idna: https://github.com/kjd/idna
a__doc__a__file__a__spec__aoriginahas_locationa__cached__a__annotations__aannotationsuOpenSSL.SSLacryptographyTax509ucryptography.x509TaUnsupportedExtensionTEExceptiona__prepare__a__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>uurllib3.contrib.pyopenssla__module__a__qualname__a__orig_bases__aloggingatypingTasocketasocket_clsTatimeoutTautillainject_into_urllib3aextract_from_urllib3a__all__aPROTOCOL_TLSaSSLv23_METHODaPROTOCOL_TLS_CLIENTaPROTOCOL_TLSv1aTLSv1_METHODaPROTOCOL_TLSv1_1aTLSv1_1_METHODaPROTOCOL_TLSv1_2aTLSv1_2_METHODaCERT_NONEaVERIFY_NONEaCERT_OPTIONALaVERIFY_PEERaCERT_REQUIREDaVERIFY_FAIL_IF_NO_PEER_CERTutoo many values to unpack (expected 2)aOP_NO_SSLv2aOP_NO_SSLv3a_OP_NO_SSLv2_OR_SSLv3aintaOP_NO_TLSv1a_OP_NO_TLSv1aOP_NO_TLSv1_1a_OP_NO_TLSv1_1aOP_NO_TLSv1_2a_OP_NO_TLSv1_2aOP_NO_TLSv1_3a_OP_NO_TLSv1_3aTLSv1aTLSv1_1aTLSv1_2aTLSv1_3udict[int, int]l@agetLoggerTuurllib3.contrib.pyopensslDareturnaNoneDanameareturnastrustr | NoneDapeer_certareturnaX509ulist[tuple[str, str]]uAPI-compatibility wrapper for Python OpenSSL's Connection-class.TtDaconnectionasocketasuppress_ragged_eofsareturnuOpenSSL.SSL.Connectionasocket_clsaboolaNonea__init__uWrappedSocket.__init__DareturnaintuWrappedSocket.filenoa_decref_socketiosuWrappedSocket._decref_socketiosDaargsakwargsareturnutyping.Anyutyping.AnyabytesuWrappedSocket.recvDaargsakwargsareturnutyping.Anyutyping.AnyaintuWrappedSocket.recv_intoDatimeoutareturnafloataNoneuWrappedSocket.settimeoutDadataareturnabytesaintuWrappedSocket._send_until_doneDadataareturnabytesaNoneasendalluWrappedSocket.sendalluWrappedSocket.shutdownuWrappedSocket.closeuWrappedSocket._real_closeTFDabinary_formareturnabooludict[str, list[typing.Any]] | NoneagetpeercertuWrappedSocket.getpeercertDareturnastraversionuWrappedSocket.versionTamakefileu
    I am a wrapper class for the PyOpenSSL ``Context`` object. I am responsible
    for translating the interface of the standard library ``SSLContext`` object
    to calls into PyOpenSSL.
    DaprotocolareturnaintaNoneuPyOpenSSLContext.__init__aoptionsuPyOpenSSLContext.optionsasetterDavalueareturnaintaNoneaverify_modeuPyOpenSSLContext.verify_modeDavalueareturnussl.VerifyModeaNoneuPyOpenSSLContext.set_default_verify_pathsDaciphersareturnubytes | straNoneaset_ciphersuPyOpenSSLContext.set_ciphersTnnnDacafileacapathacadataareturnustr | Noneustr | Noneubytes | NoneaNoneuPyOpenSSLContext.load_verify_locationsTnnDacertfileakeyfileapasswordareturnastrustr | Noneustr | NoneaNoneaload_cert_chainuPyOpenSSLContext.load_cert_chainDaprotocolsareturnulist[bytes | str]aNoneaset_alpn_protocolsuPyOpenSSLContext.set_alpn_protocolsTFtpnDasockaserver_sideado_handshake_on_connectasuppress_ragged_eofsaserver_hostnameareturnasocket_clsaboolppubytes | str | NoneaWrappedSocketawrap_socketuPyOpenSSLContext.wrap_socketuPyOpenSSLContext._set_ctx_optionsaminimum_versionuPyOpenSSLContext.minimum_versionDaminimum_versionareturnaintaNoneamaximum_versionuPyOpenSSLContext.maximum_versionDamaximum_versionareturnaintaNoneDacnxax509aerr_noaerr_depthareturn_codeareturnuOpenSSL.SSL.ConnectionaX509aintppabooluurllib3\contrib\pyopenssl.pyTa.0anameTw_apasswordTapasswordu<module urllib3.contrib.pyopenssl>Ta__class__Taselfaconnectionasocketasuppress_ragged_eofsTaselfaprotocolTaselfTanameaidna_encodeaencoded_nameTaselfadataweTaExtensionsaX509ax509Tacnxax509aerr_noaerr_depthareturn_codeTapeer_certacertaextweanamesTaselfabinary_formax509TanameaidnaaprefixTaselfacertfileakeyfileapasswordweTaselfacafileacapathacadataweTaselfamaximum_versionTaselfaminimum_versionTaselfavalueTaselfaargsakwargsadataweTaselfaargsakwargsweTaselfadataatotal_sentasentTaselfaprotocolsTaselfaciphersTaselfatimeoutTaselfasockaserver_sideado_handshake_on_connectasuppress_ragged_eofsaserver_hostnameacnxwe.urllib3.contrib.socksza_socks_optionsa__class__a__init__asource_addressasocket_optionsaextra_kwasocksacreate_connectionahostaportaproxy_typeasocks_versionaproxy_addraproxy_hostaproxy_portaproxy_usernameausernameaproxy_passwordapasswordaproxy_rdnsardnsatimeoutaSocketTimeoutaConnectTimeoutErroruConnection to uu timed out. (connect timeout=w)aProxyErrorasocket_erraNewConnectionErroruFailed to establish a new connection: u
        Establish a new connection via the SOCKS proxy.
        aparse_urlaauthaparsedasplitTw:utoo many values to unpack (expected 2)aschemeasocks5aPROXY_TYPE_SOCKS5asocks5hasocks4aPROXY_TYPE_SOCKS4asocks4auUnable to determine SOCKS version from aproxy_urlaSOCKSProxyManagerapool_classes_by_schemeu
This module contains provisional support for SOCKS proxies from within
urllib3. This module supports SOCKS4, SOCKS4A (an extension of SOCKS4), and
SOCKS5. To enable its functionality, either install PySocks or install this
module with the ``socks`` extra.

The SOCKS implementation supports the full range of urllib3 features. It also
supports the following SOCKS features:

- SOCKS4A (``proxy_url='socks4a://...``)
- SOCKS4 (``proxy_url='socks4://...``)
- SOCKS5 with remote DNS (``proxy_url='socks5h://...``)
- SOCKS5 with local DNS (``proxy_url='socks5://...``)
- Usernames and passwords for the SOCKS proxy

.. note::
   It is recommended to use ``socks5h://`` or ``socks4a://`` schemes in
   your ``proxy_url`` to ensure that DNS resolution is done from the remote
   server instead of client-side when connecting to a domain name.

SOCKS4 supports IPv4 and domain names with the SOCKS4A extension. SOCKS5
supports IPv4, IPv6, and domain names.

When connecting to a SOCKS4 proxy the ``username`` portion of the ``proxy_url``
will be sent as the ``userid`` section of the SOCKS request:

.. code-block:: python

    proxy_url="socks4a://<userid>@proxy-host"

When connecting to a SOCKS5 proxy the ``username`` and ``password`` portion
of the ``proxy_url`` will be sent as the username/password to authenticate
with the proxy:

.. code-block:: python

    proxy_url="socks5h://<username>:<password>@proxy-host"

a__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationslawarningsaexceptionsTaDependencyWarninglaDependencyWarningawarnuSOCKS support in urllib3 requires the installation of optional dependencies: specifically, PySocks.  For more information, see https://urllib3.readthedocs.io/en/latest/contrib.html#socks-proxiesatypingasocketTatimeoutaconnectionTaHTTPConnectionaHTTPSConnectionaHTTPConnectionaHTTPSConnectionaconnectionpoolTaHTTPConnectionPoolaHTTPSConnectionPoolaHTTPConnectionPoolaHTTPSConnectionPoolTaConnectTimeoutErroraNewConnectionErrorapoolmanagerTaPoolManageraPoolManageruutil.urlTaparse_urlasslaTypedDicta__prepare__a_TYPE_SOCKS_OPTIONSa__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>uurllib3.contrib.socksa__module__a__qualname__a__annotations__aintustr | Noneaboola__orig_bases__aSOCKSConnectionu
    A plain-text HTTP connection that connects via a SOCKS proxy.
    Da_socks_optionsaargsakwargsareturna_TYPE_SOCKS_OPTIONSutyping.Anyutyping.AnyaNoneuSOCKSConnection.__init__Dareturnusocks.socksocketa_new_connuSOCKSConnection._new_connaSOCKSHTTPSConnectionaSOCKSHTTPConnectionPoolaConnectionClsaSOCKSHTTPSConnectionPoolu
    A version of the urllib3 ProxyManager that routes connections via the
    defined SOCKS proxy.
    ahttpahttpsTnnl
nDaproxy_urlausernameapasswordanum_poolsaheadersaconnection_pool_kwastrustr | Noneustr | Noneaintutyping.Mapping[str, str] | Noneutyping.AnyuSOCKSProxyManager.__init__uurllib3\contrib\socks.pyu<module urllib3.contrib.socks>Ta__class__Taselfa_socks_optionsaargsakwargsa__class__T
aselfaproxy_urlausernameapasswordanum_poolsaheadersaconnection_pool_kwaparsedasplitasocks_versionardnsasocks_optionsa__class__Taselfaextra_kwaconnweaerror.urllib3.exceptionsapoola__class__a__init__uu: Tnnaurlaoriginal_errorareasonuMax retries exceeded with url: u (Caused by w)uTried to open a foreign host with url: aretriesaconnawarningsawarnuThe 'pool' property is deprecated and will be removed in urllib3 v2.1.0. Use 'conn' instead.aDeprecationWarningDastacklevelluFailed to resolve 'u' (uFailed to parse: alocationuNot supported URL scheme aschemeapartialaexpecteduIncompleteRead(%i bytes read, %i more expected)atellalength_remainingaresponsealengthuInvalidChunkLength(got length %r, %i bytes read)alocalhostuProxy URL had no scheme, should start with http:// or https://uProxy URL had unsupported scheme u, should use http:// or https://aUnknownu, unparsed data: a__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationsasocketlatypinguemail.errorsTaMessageDefectaMessageDefectuhttp.clientTaIncompleteReadaIncompleteReadahttplib_IncompleteReadTEExceptiona__prepare__aHTTPErrora__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>uurllib3.exceptionsa__module__uBase exception used by this module.a__qualname__a__orig_bases__aWarningaHTTPWarninguBase warning used by this module.aTupleaCallableTQOobjectTOobjectQa_TYPE_REDUCE_RESULTaPoolErroruBase exception for errors caused within a pool.DapoolamessageareturnaConnectionPoolastraNoneuPoolError.__init__Dareturna_TYPE_REDUCE_RESULTa__reduce__uPoolError.__reduce__aRequestErroruBase exception for PoolErrors that have associated URLs.DapoolaurlamessageareturnaConnectionPoolastrpaNoneuRequestError.__init__uRequestError.__reduce__aSSLErroruRaised when SSL certificate fails in an HTTPS connection.aProxyErroruRaised when the connection to a proxy fails.a__annotations__aExceptionDamessageaerrorareturnastraExceptionaNoneuProxyError.__init__aDecodeErroruRaised when automatic decoding based on Content-Type fails.aProtocolErroruRaised when something unexpected happens mid-request/response.aConnectionErroraMaxRetryErroruRaised when the maximum number of retries is exceeded.

    :param pool: The connection pool
    :type pool: :class:`~urllib3.connectionpool.HTTPConnectionPool`
    :param str url: The requested Url
    :param reason: The underlying error
    :type reason: :class:`Exception`

    TnDapoolaurlareasonareturnaConnectionPoolastruException | NoneaNoneuMaxRetryError.__init__aHostChangedErroruRaised when an existing pool gets a request for a foreign host.TlDapoolaurlaretriesareturnaConnectionPoolastruRetry | intaNoneuHostChangedError.__init__aTimeoutStateErroruRaised when passing an invalid state to a timeoutaTimeoutErroruRaised when a socket timeout error occurs.

    Catching this error will catch both :exc:`ReadTimeoutErrors
    <ReadTimeoutError>` and :exc:`ConnectTimeoutErrors <ConnectTimeoutError>`.
    aReadTimeoutErroruRaised when a socket timeout occurs while receiving data from a serveraConnectTimeoutErroruRaised when a socket timeout occurs while connecting to a serveraNewConnectionErroruRaised when we fail to establish a new connection. Usually ECONNREFUSED.DaconnamessageareturnaHTTPConnectionastraNoneuNewConnectionError.__init__apropertyDareturnaHTTPConnectionuNewConnectionError.poolaNameResolutionErroruRaised when host name resolution fails.DahostaconnareasonastraHTTPConnectionusocket.gaierroruNameResolutionError.__init__aEmptyPoolErroruRaised when a pool runs out of connections and no more are allowed.aFullPoolErroruRaised when we try to add a connection to a full pool in blocking mode.aClosedPoolErroruRaised when a request enters a pool after the pool has been closed.aLocationValueErroruRaised when there is something wrong with a given URL input.aLocationParseErroruRaised when get_host or similar fails to parse the URL input.DalocationareturnastraNoneuLocationParseError.__init__aURLSchemeUnknownuRaised when a URL input has an unsupported scheme.DaschemeastruURLSchemeUnknown.__init__aResponseErroruUsed as a container for an error reason supplied in a MaxRetryError.utoo many error responsesaGENERIC_ERRORutoo many {status_code} error responsesaSPECIFIC_ERRORaSecurityWarninguWarned when performing security reducing actionsaInsecureRequestWarninguWarned when making an unverified HTTPS request.aNotOpenSSLWarninguWarned when using unsupported SSL libraryaSystemTimeWarninguWarned when system time is suspected to be wrongaInsecurePlatformWarninguWarned when certain TLS/SSL configuration is not available on a platform.aDependencyWarningu
    Warned when an attempt is made to import a module with missing optional
    dependencies.
    aResponseNotChunkeduResponse needs to be chunked in order to read it as chunks.aBodyNotHttplibCompatibleu
    Body should be :class:`http.client.HTTPResponse` like
    (have an fp attribute which returns raw chunks) for read_chunked().
    u
    Response length doesn't match expected Content-Length

    Subclass of :class:`http.client.IncompleteRead` to allow int value
    for ``partial`` to avoid creating large objects on streamed reads.
    DapartialaexpectedareturnaintpaNoneuIncompleteRead.__init__Dareturnastra__repr__uIncompleteRead.__repr__aInvalidChunkLengthuInvalid chunk length in a chunked response.DaresponsealengthareturnaHTTPResponseabytesaNoneuInvalidChunkLength.__init__uInvalidChunkLength.__repr__aInvalidHeaderuThe header provided was somehow invalid.aProxySchemeUnknownuProxyManager does not support the supplied schemeDaschemeareturnustr | NoneaNoneuProxySchemeUnknown.__init__TEValueErroraProxySchemeUnsupporteduFetching HTTPS resources through HTTPS proxies is unsupportedaHeaderParsingErroruRaised by assert_header_parsing, but we convert it to a log.warning statement.Dadefectsaunparsed_dataareturnulist[MessageDefect]ubytes | str | NoneaNoneuHeaderParsingError.__init__aUnrewindableBodyErroruurllib3 encountered an error when trying to rewind a bodyuurllib3\exceptions.pyu<module urllib3.exceptions>Ta__class__Taselfaconnamessagea__class__Taselfadefectsaunparsed_dataamessagea__class__Taselfahostaconnareasonamessagea__class__Taselfalocationamessagea__class__Taselfamessageaerrora__class__TaselfapartialaexpectedTaselfapoolamessagea__class__Taselfapoolaurlamessagea__class__Taselfapoolaurlareasonamessagea__class__Taselfapoolaurlaretriesamessagea__class__TaselfaresponsealengthTaselfaschemeamessagea__class__Taself.urllib3.fields@|amimetypesaguess_typelu
    Guess the "Content-Type" of a file.

    :param filename:
        The filename to guess the "Content-Type" of using :mod:`mimetypes`.
    :param default:
        If no "Content-Type" can be guessed, default to `default`.
    awarningsawarnu'format_header_param_rfc2231' is deprecated and will be removed in urllib3 v2.1.0. This is not valid for multipart/form-data header parameters.aDeprecationWarningDastacklevelladecodeTuutf-8u"\
uu="avaluew"aasciiTEUnicodeEncodeErrorEUnicodeDecodeErroraemailautilsaencode_rfc2231uutf-8u*=u
    Helper function to format and quote a single header parameter using the
    strategy defined in RFC 2231.

    Particularly useful for header parameters which might contain
    non-ASCII values, like file names. This follows
    `RFC 2388 Section 4.4 <https://tools.ietf.org/html/rfc2388#section-4.4>`_.

    :param name:
        The name of the parameter, a string expected to be ASCII only.
    :param value:
        The value of the parameter, provided as ``bytes`` or `str``.
    :returns:
        An RFC-2231-formatted unicode string.

    .. deprecated:: 2.0.0
        Will be removed in urllib3 v2.1.0. This is not valid for
        ``multipart/form-data`` header parameters.
    u<genexpr>uformat_header_param_rfc2231.<locals>.<genexpr>atranslateDl
l
l"u%0Au%0Du%22u
    Format and quote a single multipart header parameter.

    This follows the `WHATWG HTML Standard`_ as of 2021/06/10, matching
    the behavior of current browser and curl versions. Values are
    assumed to be UTF-8. The ``\n``, ``\r``, and ``"`` characters are
    percent encoded.

    .. _WHATWG HTML Standard:
        https://html.spec.whatwg.org/multipage/
        form-control-infrastructure.html#multipart-form-data

    :param name:
        The name of the parameter, an ASCII-only ``str``.
    :param value:
        The value of the parameter, a ``str`` or UTF-8 encoded
        ``bytes``.
    :returns:
        A string ``name="value"`` with the escaped value.

    .. versionchanged:: 2.0.0
        Matches the WHATWG HTML Standard as of 2021/06/10. Control
        characters are no longer percent encoded.

    .. versionchanged:: 2.0.0
        Renamed from ``format_header_param_html5`` and
        ``format_header_param``. The old names will be removed in
        urllib3 v2.1.0.
    u'format_header_param_html5' has been renamed to 'format_multipart_header_param'. The old name will be removed in urllib3 v2.1.0.aformat_multipart_header_paramu
    .. deprecated:: 2.0.0
        Renamed to :func:`format_multipart_header_param`. Will be
        removed in urllib3 v2.1.0.
    u'format_header_param' has been renamed to 'format_multipart_header_param'. The old name will be removed in urllib3 v2.1.0.a_namea_filenameadataaheadersuThe 'header_formatter' parameter is deprecated and will be removed in urllib3 v2.1.0.aheader_formatteratypingacastaTuplea_TYPE_FIELD_VALUEutoo many values to unpack (expected 3)utoo many values to unpack (expected 2)aguess_content_typeafilenameTafilenameaheader_formatteramake_multipartTacontent_typeu
        A :class:`~urllib3.fields.RequestField` factory from old-style tuple parameters.

        Supports constructing :class:`~urllib3.fields.RequestField` from
        parameter of key/value strings AND key/filetuple. A filetuple is a
        (filename, data, MIME type) tuple where the MIME type is optional.
        For example::

            'foo': 'bar',
            'fakefile': ('foofile.txt', 'contents of foofile'),
            'realfile': ('barfile.txt', open('realfile').read()),
            'typedfile': ('bazfile.bin', open('bazfile').read(), 'image/jpeg'),
            'nonamefile': 'contents of nonamefile field',

        Field names and filenames must be unicode.
        u
        Override this method to change how each multipart header
        parameter is formatted. By default, this calls
        :func:`format_multipart_header_param`.

        :param name:
            The name of the parameter, an ASCII-only ``str``.
        :param value:
            The value of the parameter, a ``str`` or UTF-8 encoded
            ``bytes``.

        :meta public:
        aitemsapartsaappendaselfa_render_partu; u
        Helper function to format and quote a single header.

        Useful for single headers that are composed of multiple items. E.g.,
        'Content-Disposition' fields.

        :param header_parts:
            A sequence of (k, v) tuples or a :class:`dict` of (k, v) to format
            as `k1="v1"; k2="v2"; ...`.
        uContent-DispositionuContent-TypeuContent-Locationagetalinesu: Tu
u
u
        Renders the headers for this request field.
        uform-dataa_render_partsanameu
        Makes this request field into a multipart request field.

        This method overrides "Content-Disposition", "Content-Type" and
        "Content-Location" headers to the request parameter.

        :param content_disposition:
            The 'Content-Disposition' of the request body. Defaults to 'form-data'
        :param content_type:
            The 'Content-Type' of the request body.
        :param content_location:
            The 'Content-Location' of the request body.

        a__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationsuemail.utilsaUnionTOstrObytesa_TYPE_FIELD_VALUE_TUPLETuapplication/octet-streamDafilenameadefaultareturnustr | NoneastrpDanameavalueareturnastra_TYPE_FIELD_VALUEastraformat_header_param_rfc2231aformat_header_param_html5aformat_header_paramuurllib3.fieldsa__module__u
    A data container for request body parameters.

    :param name:
        The name of this request field. Must be unicode.
    :param data:
        The data/value body.
    :param filename:
        An optional filename of the request field. Must be unicode.
    :param headers:
        An optional dict-like object of headers to initially use for the field.

    .. versionchanged:: 2.0.0
        The ``header_formatter`` parameter is deprecated and will
        be removed in urllib3 v2.1.0.
    aRequestFielda__qualname__TnnnDanameadataafilenameaheadersaheader_formatterastra_TYPE_FIELD_VALUEustr | Noneutyping.Mapping[str, str] | Noneutyping.Callable[[str, _TYPE_FIELD_VALUE], str] | Nonea__init__uRequestField.__init__TnDafieldnameavalueaheader_formatterareturnastra_TYPE_FIELD_VALUE_TUPLEutyping.Callable[[str, _TYPE_FIELD_VALUE], str] | NoneaRequestFieldafrom_tuplesuRequestField.from_tuplesuRequestField._render_partDaheader_partsareturnudict[str, _TYPE_FIELD_VALUE | None] | typing.Sequence[tuple[str, _TYPE_FIELD_VALUE | None]]astruRequestField._render_partsDareturnastrarender_headersuRequestField.render_headersDacontent_dispositionacontent_typeacontent_locationareturnustr | Noneustr | Noneustr | NoneaNoneuRequestField.make_multipartTuurllib3\fields.pyTa.0achavalueu<module urllib3.fields>Taselfanameadataafilenameaheadersaheader_formatterawarningsTaselfanameavalueTaselfaheader_partsaiterableapartsanameavalueTanameavalueawarningsTanameavalueawarningsaresultTanameavalueTaclsafieldnameavalueaheader_formatterafilenameacontent_typeadataarequest_paramTafilenameadefaultTaselfacontent_dispositionacontent_typeacontent_locationTaselfalinesasort_keysasort_keyaheader_nameaheader_value.urllib3.filepost<abinasciiahexlifyaosaurandomTladecodeu
    Our embarrassingly-simple replacement for mimetools.choose_boundary.
    u
    Iterate over fields.

    Supports list of (k, v) tuples and dicts, and lists of
    :class:`~urllib3.fields.RequestField`.

    afieldsatypingaMappingaitemsaRequestFieldafrom_tuplesaiter_field_objectsaBytesIOachoose_boundaryabodyawriteu--uu
ulatin-1awriterarender_headersadataTc
u--
umultipart/form-data; boundary=agetvalueu
    Encode a dictionary of ``fields`` using the multipart/form-data MIME format.

    :param fields:
        Dictionary of fields or list of (key, :class:`~urllib3.fields.RequestField`).
        Values are processed by :func:`urllib3.fields.RequestField.from_tuples`.

    :param boundary:
        If not specified, then a random boundary will be generated using
        :func:`urllib3.filepost.choose_boundary`.
    a__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationslacodecsTa_TYPE_FIELD_VALUE_TUPLEaRequestFieldla_TYPE_FIELD_VALUE_TUPLEalookupTuutf-8laSequenceaUnionaTuplea_TYPE_FIELDS_SEQUENCEa_TYPE_FIELDSDareturnastrDafieldsareturna_TYPE_FIELDSutyping.Iterable[RequestField]TnDafieldsaboundaryareturna_TYPE_FIELDSustr | Noneutuple[bytes, str]aencode_multipart_formdatauurllib3\filepost.pyu<module urllib3.filepost>Tafieldsaboundaryabodyafieldadataacontent_typeTafieldsaiterableafield.urllib3.poolmanager51acopyaschemealowerahostTaheadersa_proxy_headersa_socks_optionsacontextaitemsagetTasocket_optionsasocket_optionsakeysapopakey_a_fieldsTakey_blocksizea_DEFAULT_BLOCKSIZEakey_blocksizeu
    Create a pool key out of a request context dictionary.

    According to RFC 3986, both the scheme and host are case-insensitive.
    Therefore, this function normalizes both before constructing the pool
    key for an HTTPS request. If you wish to change this behaviour, provide
    alternate callables to ``key_fn_by_scheme``.

    :param key_class:
        The class to use when constructing the key. This should be a namedtuple
        with the ``scheme`` and ``host`` keys at a minimum.
    :type  key_class: namedtuple
    :param request_context:
        A dictionary-like object that contain the context for a request.
    :type  request_context: dict

    :return: A namedtuple that can be used as a connection pool key.
    :rtype:  PoolKey
    a__class__a__init__aconnection_pool_kwaRecentlyUsedContainerapoolsapool_classes_by_schemeakey_fn_by_schemeaclearTablocksizeablocksizeTaschemeahostaportarequest_contextahttpaSSL_KEYWORDSu
        Create a new :class:`urllib3.connectionpool.ConnectionPool` based on host, port, scheme, and
        any additional pool keyword arguments.

        If ``request_context`` is provided, it is provided as keyword arguments
        to the pool class used. This method is used to actually create the
        connection pools handed out by :meth:`connection_from_url` and
        companion methods. It is intended to be overridden for customization.
        u
        Empty our store of pools and direct them all to close.

        This will not affect in-flight connections, but they will not be
        re-used after completion.
        aLocationValueErrorTuNo host specified.a_merge_pool_kwargsaport_by_schemelPaportaconnection_from_contextu
        Get a :class:`urllib3.connectionpool.ConnectionPool` based on the host, port, and scheme.

        If ``port`` isn't given, it will be derived from the ``scheme`` using
        ``urllib3.connectionpool.port_by_scheme``. If ``pool_kwargs`` is
        provided, it is merged with the instance's ``connection_pool_kw``
        variable and used to create the new connection pool, if one is
        needed.
        astrictawarningsawarnuThe 'strict' parameter is no longer needed on Python 3+. This will raise an error in urllib3 v2.1.0.aDeprecationWarningTastrictaURLSchemeUnknownaconnection_from_pool_keyTarequest_contextu
        Get a :class:`urllib3.connectionpool.ConnectionPool` based on the request context.

        ``request_context`` must at least contain the ``scheme`` key and its
        value must be a key in ``key_fn_by_scheme`` instance variable.
        alocka__enter__a__exit__a_new_poolTnnnapoolu
        Get a :class:`urllib3.connectionpool.ConnectionPool` based on the provided pool key.

        ``pool_key`` should be a namedtuple that only contains immutable
        objects. At a minimum it must have the ``scheme``, ``host``, and
        ``port`` fields.
        aparse_urlaconnection_from_hostTaportaschemeapool_kwargsu
        Similar to :func:`urllib3.connectionpool.connection_from_url`.

        If ``pool_kwargs`` is not provided and a new pool needs to be
        constructed, ``self.connection_pool_kw`` is used to initialize
        the :class:`urllib3.connectionpool.ConnectionPool`. If ``pool_kwargs``
        is provided, it is used instead. Note that if a new pool does not
        need to be created for the request, the provided ``pool_kwargs`` are
        not used.
        utoo many values to unpack (expected 2)abase_pool_kwargsu
        Merge a dictionary of override values for self.connection_pool_kw.

        This does not modify self.connection_pool_kw and returns a new dict.
        Any keys in the override dictionary with a value of ``None`` are
        removed from the merged dictionary.
        aproxyaconnection_requires_http_tunnelaproxy_configu
        Indicates if the proxy requires the complete destination URL in the
        request.  Normally this is only needed when not using an HTTP CONNECT
        tunnel.
        lTuURLs without a scheme (ie 'https://') are deprecated and will raise an error in a future version of urllib3. To avoid this DeprecationWarning ensure all URLs start with 'https://' or 'http://'. Read more in this issue: https://github.com/urllib3/urllib3/issues/2920TacategoryastacklevelTaportaschemeaassert_same_hostaredirectaheadersa_proxy_requires_url_absolute_formaurlopenarequest_uriaget_redirect_locationaresponseaurljoinastatusl/aGETabodyaHTTPHeaderDicta_prepare_for_method_changeTaretriesaRetryafrom_intTaredirectaremove_headers_on_redirectaconnais_same_hostaretriesanew_headersaincrementTaresponsea_poolaMaxRetryErroraraise_on_redirectadrain_connalogainfouRedirecting %s -> %saredirect_locationu
        Same as :meth:`urllib3.HTTPConnectionPool.urlopen`
        with custom cross-host redirect logic and only sends the request-uri
        portion of the ``url``.

        The given ``url`` parameter must be absolute, such that an appropriate
        :class:`urllib3.connectionpool.ConnectionPool` can be chosen for it.
        aHTTPConnectionPooluu://w:TahttpahttpsaProxySchemeUnknowna_replaceTaportaproxy_headersaproxy_ssl_contextaProxyConfiga_proxya_proxy_headersa_proxy_configahttpsTapool_kwargsDaAcceptu*/*anetlocaHostaheaders_u
        Sets headers needed by proxies: specifically, the Accept and Host
        headers. Only sets headers not provided by the user.
        a_set_proxy_headersakwuSame as HTTP(S)ConnectionPool.urlopen, ``url`` must be absolute.aProxyManageraproxy_urla__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationsafunctoolsalogginglatypingaTracebackTypeuurllib.parseTaurljoina_collectionsTaHTTPHeaderDictaRecentlyUsedContainerla_request_methodsTaRequestMethodsaRequestMethodsaconnectionTaProxyConfigaconnectionpoolTaHTTPConnectionPoolaHTTPSConnectionPoolaport_by_schemeaHTTPSConnectionPoolaexceptionsTaLocationValueErroraMaxRetryErroraProxySchemeUnknownaURLSchemeUnknownTaBaseHTTPResponseaBaseHTTPResponseuutil.connectionTa_TYPE_SOCKET_OPTIONSa_TYPE_SOCKET_OPTIONSuutil.proxyTaconnection_requires_http_tunneluutil.retryTaRetryuutil.timeoutTaTimeoutaTimeoutuutil.urlTaUrlaparse_urlaUrlaPoolManageraproxy_from_urla__all__agetLoggerTuurllib3.poolmanagerTakey_fileacert_fileacert_reqsaca_certsaca_cert_dataassl_versionassl_minimum_versionassl_maximum_versionaca_cert_dirassl_contextakey_passwordaserver_hostnamel@aTypeVarTa_SelfTa_SelfTaNamedTuplea__prepare__aPoolKeya__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>uurllib3.poolmanagera__module__u
    All known keyword arguments that could be provided to the pool manager, its
    pools, or the underlying connections.

    All custom key schemes should include the fields in this key at a minimum.
    a__qualname__a__annotations__astrakey_schemeakey_hostuint | Noneakey_portuTimeout | float | int | Noneakey_timeoutuRetry | bool | int | Noneakey_retriesubool | Noneakey_blockutuple[str, int] | Noneakey_source_addressustr | Noneakey_key_fileakey_key_passwordakey_cert_fileakey_cert_reqsakey_ca_certsustr | bytes | Noneakey_ca_cert_datauint | str | Noneakey_ssl_versionussl.TLSVersion | Noneakey_ssl_minimum_versionakey_ssl_maximum_versionakey_ca_cert_dirussl.SSLContext | Noneakey_ssl_contextakey_maxsizeufrozenset[tuple[str, str]] | Noneakey_headersuUrl | Noneakey__proxyakey__proxy_headersuProxyConfig | Noneakey__proxy_configu_TYPE_SOCKET_OPTIONS | Noneakey_socket_optionsakey__socks_optionsubool | str | Noneakey_assert_hostnameakey_assert_fingerprintakey_server_hostnamea__orig_bases__Dakey_classarequest_contextareturnutype[PoolKey]udict[str, typing.Any]aPoolKeya_default_key_normalizerapartialu
    Allows for arbitrary requests while transparently keeping track of
    necessary connection pools for you.

    :param num_pools:
        Number of connection pools to cache before discarding the least
        recently used pool.

    :param headers:
        Headers to include with all requests, unless other headers are given
        explicitly.

    :param \**connection_pool_kw:
        Additional parameters are used to create fresh
        :class:`urllib3.connectionpool.ConnectionPool` instances.

    Example:

    .. code-block:: python

        import urllib3

        http = urllib3.PoolManager(num_pools=2)

        resp1 = http.request("GET", "https://google.com/")
        resp2 = http.request("GET", "https://google.com/mail")
        resp3 = http.request("GET", "https://yahoo.com/")

        print(len(http.pools))
        # 2

    Tl
nDanum_poolsaheadersaconnection_pool_kwareturnaintutyping.Mapping[str, str] | Noneutyping.AnyaNoneuPoolManager.__init__Daselfareturna_SelfTpuPoolManager.__enter__Daexc_typeaexc_valaexc_tbareturnutype[BaseException] | NoneuBaseException | NoneuTracebackType | NoneuLiteral[False]uPoolManager.__exit__TnDaschemeahostaportarequest_contextareturnastrpaintudict[str, typing.Any] | NoneaHTTPConnectionPooluPoolManager._new_poolDareturnaNoneuPoolManager.clearTnahttpnDahostaportaschemeapool_kwargsareturnustr | Noneuint | Noneustr | Noneudict[str, typing.Any] | NoneaHTTPConnectionPooluPoolManager.connection_from_hostDarequest_contextareturnudict[str, typing.Any]aHTTPConnectionPooluPoolManager.connection_from_contextDapool_keyarequest_contextareturnaPoolKeyudict[str, typing.Any]aHTTPConnectionPooluPoolManager.connection_from_pool_keyDaurlapool_kwargsareturnastrudict[str, typing.Any] | NoneaHTTPConnectionPoolaconnection_from_urluPoolManager.connection_from_urlDaoverrideareturnudict[str, typing.Any] | Noneudict[str, typing.Any]uPoolManager._merge_pool_kwargsDaparsed_urlareturnaUrlabooluPoolManager._proxy_requires_url_absolute_formTtDamethodaurlaredirectakwareturnastrpaboolutyping.AnyaBaseHTTPResponseuPoolManager.urlopenu
    Behaves just like :class:`PoolManager`, but sends all requests through
    the defined proxy, using the CONNECT method for HTTPS URLs.

    :param proxy_url:
        The URL of the proxy to be used.

    :param proxy_headers:
        A dictionary containing headers that will be sent to the proxy. In case
        of HTTP they are being sent with each request, while in the
        HTTPS/CONNECT case they are sent only once. Could be used for proxy
        authentication.

    :param proxy_ssl_context:
        The proxy SSL context is used to establish the TLS connection to the
        proxy when using HTTPS proxies.

    :param use_forwarding_for_https:
        (Defaults to False) If set to True will forward requests to the HTTPS
        proxy to be made on behalf of the client instead of creating a TLS
        tunnel via the CONNECT method. **Enabling this flag means that request
        and response headers and content will be visible from the HTTPS proxy**
        whereas tunneling keeps request and response headers and content
        private.  IP address, target hostname, SNI, and port are always visible
        to an HTTPS proxy even when this flag is disabled.

    :param proxy_assert_hostname:
        The hostname of the certificate to verify against.

    :param proxy_assert_fingerprint:
        The fingerprint of the certificate to verify against.

    Example:

    .. code-block:: python

        import urllib3

        proxy = urllib3.ProxyManager("https://localhost:3128/")

        resp1 = proxy.request("GET", "https://google.com/")
        resp2 = proxy.request("GET", "https://httpbin.org/")

        print(len(proxy.pools))
        # 1

        resp3 = proxy.request("GET", "https://httpbin.org/")
        resp4 = proxy.request("GET", "https://twitter.com/")

        print(len(proxy.pools))
        # 3

    Tl
nnnFnnD
aproxy_urlanum_poolsaheadersaproxy_headersaproxy_ssl_contextause_forwarding_for_httpsaproxy_assert_hostnameaproxy_assert_fingerprintaconnection_pool_kwareturnastraintutyping.Mapping[str, str] | Noneutyping.Mapping[str, str] | Noneussl.SSLContext | NoneabooluNone | str | Literal[False]ustr | Noneutyping.AnyaNoneuProxyManager.__init__uProxyManager.connection_from_hostDaurlaheadersareturnastrutyping.Mapping[str, str] | Noneutyping.Mapping[str, str]uProxyManager._set_proxy_headersuProxyManager.urlopenDaurlakwareturnastrutyping.AnyaProxyManageruurllib3\poolmanager.pyu<module urllib3.poolmanager>Ta__class__TaselfTaselfaexc_typeaexc_valaexc_tbTaselfanum_poolsaheadersaconnection_pool_kwa__class__Taselfaproxy_urlanum_poolsaheadersaproxy_headersaproxy_ssl_contextause_forwarding_for_httpsaproxy_assert_hostnameaproxy_assert_fingerprintaconnection_pool_kwastr_proxy_urlaproxyaporta__class__Takey_classarequest_contextacontextakeyasocket_optsafieldTaselfaoverrideabase_pool_kwargsakeyavalueTaselfaschemeahostaportarequest_contextapool_clsakeyakwTaselfaparsed_urlTaselfaurlaheadersaheaders_anetlocTaselfarequest_contextaschemeapool_key_constructorapool_keyTaselfahostaportaschemeapool_kwargsa__class__Taselfahostaportaschemeapool_kwargsarequest_contextTaselfapool_keyarequest_contextapoolaschemeahostaportTaselfaurlapool_kwargswuTaurlakwTaselfamethodaurlaredirectakwwuaconnaresponsearedirect_locationaretriesanew_headersaheaderTaselfamethodaurlaredirectakwwuaheadersa__class__.urllib3.response>a_first_tryca_dataazlibadecompressobja_objadecompressaerroraMAX_WBITSaflushlaGzipDecoderStateaFIRST_MEMBERa_stateBaSWALLOW_DATAaretaselfadataaOTHER_MEMBERSaunused_dataabrotliaDecompressoraprocessazstdaZstdDecompressoraeofadata_partsaappendaDecodeErrorTuZstandard data is incompleteasplitTw,a_get_decoderastripa_decoderslw,aMultiDecoderTagzipux-gzipaGzipDecoderabraBrotliDecoderaZstdDecoderaDeflateDecoderacollectionsadequeabuffera_sizeubuffer is emptyun should be > 0aBytesIOafetchedwnapopleftutoo many values to unpack (expected 2)awriteaappendleftagetvalueaHTTPHeaderDictaheadersastatusaversionareasonadecode_contenta_has_decoded_contenta_request_urlaretriesachunkedagetTutransfer-encodingualowera_decoderu<genexpr>uBaseHTTPResponse.__init__.<locals>.<genexpr>aREDIRECT_STATUSESTalocationu
        Should we redirect and where to?

        :returns: Truthy redirect location string if we got a redirect status
            code and valid location. ``None`` if redirect status and no
            location. ``False`` if not a redirect status code.
        adecodeTuutf-8a_jsonaloadsu
        Parses the body of the HTTP response as JSON.

        To use a custom JSON decoder pass the result of :attr:`HTTPResponse.data` to the decoder.

        This method can raise either `UnicodeDecodeError` or `json.JSONDecodeError`.

        Read more :ref:`here <json>`.
        a_retriesahistorylaredirect_locationaurlTucontent-encodinguaCONTENT_DECODERSu
        Set-up the _decoder attribute if necessary.
        uCalling read(decode_content=False) is not supported after read(decode_content=True) was called.aDECODER_ERROR_CLASSESuReceived response with content-encoding: %s, but failed to decode it.a_flush_decoderu
        Decode the data passed in and potentially flush the decoder.
        Tcu
        Flushes the decoder. Should only be called if the decoder is actually
        being used.
        areadawarningsawarnaDeprecationWarninglTuHTTPResponse.getheaders() is deprecated and will be removed in urllib3 v2.1.0. Instead access HTTPResponse.headers directly.TacategoryastacklevelTuHTTPResponse.getheader() is deprecated and will be removed in urllib3 v2.1.0. Instead use HTTPResponse.headers.get(name, default).a__class__a__init__Taheadersastatusaversionareasonadecode_contentarequest_urlaretriesaenforce_content_lengthaauto_closea_bodya_fpa_original_responsea_fp_bytes_readamsgTOstrObytesa_poola_connectionabodyachunk_lefta_init_lengthalength_remainingaBytesQueueBuffera_decoded_bufferTadecode_contenta_put_connaHTTPErroraBaseSSLErroraHTTPExceptionu
        Read and discard any remaining HTTP response data in the response connection.

        Unread data in the HTTPResponse connection blocks the connection from being released back to the pool.
        TtTacache_contentais_fp_closedu
        Obtain the number of bytes pulled over the wire so far. May differ from
        the amount of content returned by :meth:``urllib3.response.HTTPResponse.read``
        if bytes are encoded on the wire (e.g, compressed).
        Tucontent-lengthalogawarningTuReceived response with both Content-Length and Transfer-Encoding set. This is expressly forbidden by RFC 7230 sec 3.3.2. Ignoring Content-Length and attempting to process response as Transfer-Encoding: chunked.aInvalidHeaderuContent-Length contained multiple unmatching values (%s)apopTll0ldlaHEADu
        Set initial length value for Response content if available.
        u
        Catch low-level python exceptions, instead re-raising urllib3
        variants, so that low-level exceptions are not leaked in the
        high-level api.

        On exit, release the connection back to the pool.
        aSocketTimeoutaReadTimeoutErroruRead timed out.uread operation timed outaSSLErroraProtocolErroruConnection broken: uacloseaisclosedarelease_conna_error_catcheruHTTPResponse._error_catcherqautilaIS_PYOPENSSLqaamtaminamax_chunk_amtachunk_amtu
        Read a response with the thought that reading the number of bytes
        larger than can fit in a 32-bit int at a time via SSL in some
        known cases leads to an overflow error that has to be prevented
        if `amt` or `self.length_remaining` indicate that a problem may
        happen.

        The known cases:
          * 3.8 <= CPython < 3.9.7 because of a bug
            https://github.com/urllib3/urllib3/issues/2513#issuecomment-1152559900.
          * urllib3 injected with pyOpenSSL-backed SSL-support.
          * CPython < 3.10 only when `amt` does not fit 32-bit int.
        acloseda__enter__a__exit__a_fp_readaIncompleteReadTnnnu
        Reads `amt` of bytes from the socket.
        a_init_decodera_raw_reada_decodeaputaflush_decoderu
        Similar to :meth:`http.client.HTTPResponse.read`, but with two additional
        parameters: ``decode_content`` and ``cache_content``.

        :param amt:
            How much of the content to read. If specified, caching is skipped
            because it doesn't make sense to cache partial content as the full
            response.

        :param decode_content:
            If True, will attempt to decode the body based on the
            'content-encoding' header.

        :param cache_content:
            If True, will save the returned data such that the same result is
            returned despite of the state of the underlying file object. This
            is useful if you want the ``.data`` property to continue working
            after having ``.read()`` the file object. (Overridden if ``amt`` is
            set.)
        u
        A generator wrapper for the read() method. A call will block until
        ``amt`` bytes have been read from the connection or until the
        connection is closed.

        :param amt:
            How much of the content to read. The generator will return up to
            much data per iteration, but may return less. This is particularly
            likely when using compressed data. However, the empty string will
            never be returned.

        :param decode_content:
            If True, will attempt to decode the body based on the
            'content-encoding' header.
        asupports_chunked_readsaread_chunkedTaamtadecode_contentastreamuHTTPResponse.streamaioaIOBasea__get__uHTTPResponse has no file to get a fileno fromafilenouThe file-like object this HTTPResponse is wrapped around has no file descriptorafpu
        Checks if the underlying file-like object looks like a
        :class:`http.client.HTTPResponse` object. We do this by testing for
        the fp attribute. If it is present we assume it returns raw chunks as
        processed by read_chunked().
        areadlineTd;laInvalidChunkLengtha_safe_readTlu
        Similar to :meth:`HTTPResponse.read`, but with an additional
        parameter: ``decode_content``.

        :param amt:
            How much of the content to read. If specified, caching is skipped
            because it doesn't make sense to cache partial content as the full
            response.

        :param decode_content:
            If True, will attempt to decode the body based on the
            'content-encoding' header.
        aResponseNotChunkedTuResponse is not chunked. Header 'transfer-encoding: chunked' is missing.aBodyNotHttplibCompatibleTuBody should be http.client.HTTPResponse like. It should have have an fp attribute which returns raw chunks.ais_response_to_heada_update_chunk_lengtha_handle_chunkTadecode_contentaflush_decoderc
uHTTPResponse.read_chunkedu
        Returns the URL that was the source of this response.
        If the request that generated this response redirected, this method
        will return the final redirect location.
        d
Td
:llna__iter__uHTTPResponse.__iter__a__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationsajsonaloggingareasysatypingacontextlibTacontextmanageracontextmanageruhttp.clientTaHTTPMessageaHTTPMessagea_HttplibHTTPMessageTaHTTPResponseaHTTPResponsea_HttplibHTTPResponseasocketTatimeoutatimeoutabrotlicffiazstandardasearchu^([0-9]+)\.([0-9]+)a__version__agroupsa_zstd_versionTllTEAttributeErrorEImportErrorEValueErrorTautilla_base_connectionTa_TYPE_BODYa_TYPE_BODYa_collectionsTaHTTPHeaderDictaconnectionTaBaseSSLErroraHTTPConnectionaHTTPExceptionaHTTPConnectionaexceptionsT
aBodyNotHttplibCompatibleaDecodeErroraHTTPErroraIncompleteReadaInvalidChunkLengthaInvalidHeaderaProtocolErroraReadTimeoutErroraResponseNotChunkedaSSLErroruutil.responseTais_fp_closedais_response_to_headuutil.retryTaRetryaRetryagetLoggerTuurllib3.responseuurllib3.responsea__module__aContentDecodera__qualname__DadataareturnabytespuContentDecoder.decompressDareturnabytesuContentDecoder.flushTa__prepare__a__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>DareturnaNoneuDeflateDecoder.__init__uDeflateDecoder.decompressuDeflateDecoder.flusha__orig_bases__uGzipDecoder.__init__uGzipDecoder.decompressuGzipDecoder.flushuBrotliDecoder.__init__uBrotliDecoder.flushuZstdDecoder.__init__uZstdDecoder.decompressuZstdDecoder.flushu
    From RFC7231:
        If one or more encodings have been applied to a representation, the
        sender that applied the encodings MUST generate a Content-Encoding
        header field that lists the content codings in the order in which
        they were applied.
    DamodesareturnastraNoneuMultiDecoder.__init__uMultiDecoder.flushuMultiDecoder.decompressDamodeareturnastraContentDecoderuMemory-efficient bytes buffer

    To return decoded data in read() and still follow the BufferedIOBase API, we need a
    buffer to always return the correct amount of bytes.

    This buffer should be filled using calls to put()

    Our maximum memory usage is determined by the sum of the size of:

     * self.buffer, which contains the full data
     * the largest chunk that we will copy in get()

    The worst case scenario is a single chunk, in which case we'll make a full copy of
    the data inside get().
    uBytesQueueBuffer.__init__Dareturnainta__len__uBytesQueueBuffer.__len__DadataareturnabytesaNoneuBytesQueueBuffer.putDwnareturnaintabytesuBytesQueueBuffer.getaBaseHTTPResponsea__annotations__agzipux-gzipadeflateLl-l.l/l3l4aIOErrorutuple[type[Exception], ...]aZstdErrorDaheadersaretriesnnDaheadersastatusaversionareasonadecode_contentarequest_urlaretriesareturnutyping.Mapping[str, str] | typing.Mapping[bytes, bytes] | Noneaintpustr | Noneaboolustr | NoneuRetry | NoneaNoneuBaseHTTPResponse.__init__Dareturnustr | None | Literal[False]aget_redirect_locationuBaseHTTPResponse.get_redirect_locationapropertyuBaseHTTPResponse.dataDareturnutyping.AnyuBaseHTTPResponse.jsonDareturnustr | NoneuBaseHTTPResponse.urlasetterDaurlareturnustr | NoneaNoneDareturnuHTTPConnection | NoneuBaseHTTPResponse.connectionDareturnuRetry | NoneuBaseHTTPResponse.retriesDaretriesareturnuRetry | NoneaNoneTlnDaamtadecode_contentareturnuint | Noneubool | Noneutyping.Iterator[bytes]uBaseHTTPResponse.streamTnnFDaamtadecode_contentacache_contentareturnuint | Noneubool | NoneaboolabytesuBaseHTTPResponse.readTnnuBaseHTTPResponse.read_chunkeduBaseHTTPResponse.release_connadrain_connuBaseHTTPResponse.drain_connuBaseHTTPResponse.closeuBaseHTTPResponse._init_decoderDadataadecode_contentaflush_decoderareturnabytesubool | NoneaboolabytesuBaseHTTPResponse._decodeuBaseHTTPResponse._flush_decoderDwbareturnabytearrayaintareadintouBaseHTTPResponse.readintoDareturnaHTTPHeaderDictagetheadersuBaseHTTPResponse.getheadersTnDanameadefaultareturnastrustr | Noneustr | NoneagetheaderuBaseHTTPResponse.getheaderainfouBaseHTTPResponse.infoageturluBaseHTTPResponse.geturlu
    HTTP Response container.

    Backwards-compatible with :class:`http.client.HTTPResponse` but the response ``body`` is
    loaded and decoded on-demand when the ``data`` property is accessed.  This
    class is also compatible with the Python standard library's :mod:`io`
    module, and can hence be treated as a readable object in the context of that
    framework.

    Extra parameters for behaviour not present in :class:`http.client.HTTPResponse`:

    :param preload_content:
        If True, the response's body will be preloaded during construction.

    :param decode_content:
        If True, will attempt to decode the body based on the
        'content-encoding' header.

    :param original_response:
        When this HTTPResponse wrapper is generated from an :class:`http.client.HTTPResponse`
        object, it's convenient to include the original for debug purposes. It's
        otherwise unused.

    :param retries:
        The retries contains the last :class:`~urllib3.util.retry.Retry` that
        was used during the request.

    :param enforce_content_length:
        Enforce content length checking. Body returned by server must match
        value of Content-Length header, if present. Otherwise, raise error.
    TunlpntpnnnnntnntDabodyaheadersastatusaversionareasonapreload_contentadecode_contentaoriginal_responseapoolaconnectionamsgaretriesaenforce_content_lengtharequest_methodarequest_urlaauto_closeareturna_TYPE_BODYutyping.Mapping[str, str] | typing.Mapping[bytes, bytes] | Noneaintpustr | Noneaboolpu_HttplibHTTPResponse | NoneuHTTPConnectionPool | NoneuHTTPConnection | Noneu_HttplibHTTPMessage | NoneuRetry | Noneaboolustr | Noneustr | NoneaboolaNoneuHTTPResponse.__init__uHTTPResponse.release_connuHTTPResponse.drain_connuHTTPResponse.datauHTTPResponse.connectionDareturnabooluHTTPResponse.isclosedatelluHTTPResponse.tellDarequest_methodareturnustr | Noneuint | NoneuHTTPResponse._init_lengthDareturnutyping.Generator[None, None, None]Daamtareturnuint | NoneabytesuHTTPResponse._fp_readuHTTPResponse._raw_readuHTTPResponse.readDaamtadecode_contentareturnuint | Noneubool | Noneutyping.Generator[bytes, None, None]areadableuHTTPResponse.readableuHTTPResponse.closeuHTTPResponse.closeduHTTPResponse.filenouHTTPResponse.flushuHTTPResponse.supports_chunked_readsuHTTPResponse._update_chunk_lengthuHTTPResponse._handle_chunkuHTTPResponse.urlDaurlareturnastraNoneDareturnutyping.Iterator[bytes]uurllib3\response.pyTa.0aencu<module urllib3.response>Ta__class__TaselfTaselfabodyaheadersastatusaversionareasonapreload_contentadecode_contentaoriginal_responseapoolaconnectionamsgaretriesaenforce_content_lengtharequest_methodarequest_urlaauto_closea__class__T
aselfaheadersastatusaversionareasonadecode_contentarequest_urlaretriesatr_encaencodingsTaselfamodesTaselfabufferachunkachunkswxTaselfadataadecode_contentaflush_decoderweacontent_encodingTaselfaclean_exitweTaselfaamtac_int_maxabufferamax_chunk_amtachunk_amtadataTamodeTaselfaamtareturned_chunkachunkavalueTaselfacontent_encodingaencodingsTaselfarequest_methodalengthacontent_lengthalengthsastatusTaselfaamtafp_closedadataTaselfalineTaselfadataTaselfadatawdTaselfadataadata_partsaunused_dataTaselfadataadecompressedTaselfadataaretaprevious_stateTaselfaretTaselfwnafetchedaretaremainingachunkachunk_lengthaleft_chunkaright_chunkTaselfanameadefaultTaselfaamtadecode_contentacache_contentTaselfaamtadecode_contentacache_contentadataaflush_decoderadecoded_dataTaselfaamtadecode_contentTaselfaamtadecode_contentachunkadecodedalineTaselfwbatempTaselfaretriesTaselfaamtadecode_contentadataTaselfaurl.urllib3.util.connection>Lais_connectedu
    Returns True if the connection is dropped and should be closed.
    :param conn: :class:`urllib3.connection.HTTPConnection` object.
    utoo many values to unpack (expected 2)astartswithTw[astripTu[]aallowed_gai_familyahostaencodeTaidnaaLocationParseErrorw'uu', label empty or too longasocketagetaddrinfoaSOCK_STREAMutoo many values to unpack (expected 5)a_set_socket_optionsasocket_optionsatimeouta_DEFAULT_TIMEOUTasettimeoutasource_addressabindaconnectasockacloseaerrugetaddrinfo returns an empty listuConnect to *address* and return the socket object.

    Convenience function.  Connect to *address* (a 2-tuple ``(host,
    port)``) and return the socket object.  Passing the optional
    *timeout* parameter will set the timeout on the socket instance
    before attempting to connect.  If no *timeout* is supplied, the
    global default timeout setting returned by :func:`socket.getdefaulttimeout`
    is used.  If *source_address* is set it must be a tuple of (host, port)
    for the socket to bind as a source address before making the connection.
    An host of '' or port 0 tells the OS to use the default.
    asetsockoptaAF_INETaHAS_IPV6aAF_UNSPECuThis function is designed to work in the context of
    getaddrinfo, where family=socket.AF_UNSPEC is the default and
    will perform a DNS search for both IPv6 and IPv4 records.ahas_ipv6aAF_INET6luReturns True if the system can bind an IPv6 address.a__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationsatypingaexceptionsTaLocationParseErrorlTa_DEFAULT_TIMEOUTa_TYPE_TIMEOUTla_TYPE_TIMEOUTaSequenceaTupleaUnionTOintObytesa_TYPE_SOCKET_OPTIONSDaconnareturnaBaseHTTPConnectionaboolais_connection_droppedDaaddressatimeoutasource_addressasocket_optionsareturnutuple[str, int]a_TYPE_TIMEOUTutuple[str, int] | Noneu_TYPE_SOCKET_OPTIONS | Noneusocket.socketacreate_connectionDasockaoptionsareturnusocket.socketu_TYPE_SOCKET_OPTIONS | NoneaNoneDareturnusocket.AddressFamilyDahostareturnastraboola_has_ipv6Tu::1uurllib3\util\connection.pyu<module urllib3.util.connection>Tahostasockahas_ipv6TasockaoptionsaoptTafamilyTaaddressatimeoutasource_addressasocket_optionsahostaportaerrafamilyaresaafasocktypeaprotoacanonnameasaasockw_Taconn.urllib3.util;a__doc__a__file__apathadirnameajoinaenvironagetTaNUITKA_PACKAGE_urllib3u\not_existingautilTaNUITKA_PACKAGE_urllib3_utilu\not_existinga__path__a__spec__aoriginahas_locationasubmodule_search_locationsa__cached__aannotationsaconnectionTais_connection_droppedlais_connection_droppedlarequestTaSKIP_HEADERaSKIPPABLE_HEADERSamake_headersaSKIP_HEADERaSKIPPABLE_HEADERSamake_headersaresponseTais_fp_closedais_fp_closedaretryTaRetryaRetryassl_TaALPN_PROTOCOLSaIS_PYOPENSSLaSSLContextaassert_fingerprintacreate_urllib3_contextaresolve_cert_reqsaresolve_ssl_versionassl_wrap_socketaALPN_PROTOCOLSaIS_PYOPENSSLaSSLContextaassert_fingerprintacreate_urllib3_contextaresolve_cert_reqsaresolve_ssl_versionassl_wrap_socketatimeoutTaTimeoutaTimeoutaurlTaUrlaparse_urlaUrlaparse_urlawaitTawait_for_readawait_for_writeawait_for_readawait_for_writeTaIS_PYOPENSSLaSSLContextaALPN_PROTOCOLSaRetryaTimeoutaUrlaassert_fingerprintacreate_urllib3_contextais_connection_droppedais_fp_closedaparse_urlamake_headersaresolve_cert_reqsaresolve_ssl_versionassl_wrap_socketawait_for_readawait_for_writeaSKIP_HEADERaSKIPPABLE_HEADERSa__all__uurllib3\util\__init__.pyu<module urllib3.util>u.urllib3.util.proxyahttpaschemeahttpsause_forwarding_for_httpsu
    Returns True if the connection requires an HTTP CONNECT through the proxy.

    :param URL proxy_url:
        URL of the proxy.
    :param ProxyConfig proxy_config:
        Proxy configuration from poolmanager.py
    :param str destination_scheme:
        The scheme of the destination. (i.e https, http, etc)
    a__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationsatypingaurlTaUrllaUrllTnnnDaproxy_urlaproxy_configadestination_schemeareturnuUrl | NoneuProxyConfig | Noneustr | Noneaboolaconnection_requires_http_tunneluurllib3\util\proxy.pyu<module urllib3.util.proxy>Taproxy_urlaproxy_configadestination_schemeu.urllib3.util.requestww,aACCEPT_ENCODINGuaccept-encodingaheadersuuser-agentukeep-aliveaconnectionuBasic ab64encodeaencodeTulatin-1adecodeuaauthorizationuproxy-authorizationuno-cacheucache-controlu
    Shortcuts for generating request headers.

    :param keep_alive:
        If ``True``, adds 'connection: keep-alive' header.

    :param accept_encoding:
        Can be a boolean, list, or string.
        ``True`` translates to 'gzip,deflate'.  If either the ``brotli`` or
        ``brotlicffi`` package is installed 'gzip,deflate,br' is used instead.
        List will get joined by comma.
        String will be used as provided.

    :param user_agent:
        String representing the user-agent you want, such as
        "python-urllib3/0.6"

    :param basic_auth:
        Colon-separated username:password string for 'authorization: basic ...'
        auth header.

    :param proxy_basic_auth:
        Colon-separated username:password string for 'proxy-authorization: basic ...'
        auth header.

    :param disable_cache:
        If ``True``, adds 'cache-control: no-cache' header.

    Example:

    .. code-block:: python

        import urllib3

        print(urllib3.util.make_headers(keep_alive=True, user_agent="Batman/1.0"))
        # {'connection': 'keep-alive', 'user-agent': 'Batman/1.0'}
        print(urllib3.util.make_headers(accept_encoding=True))
        # {'accept-encoding': 'gzip,deflate'}
    arewind_bodyatella_FAILEDTELLaposu
    If a position is provided, move file to that point.
    Otherwise, we'll attempt to record a position for future use.
    aseekaUnrewindableBodyErrorTuAn error occurred when rewinding request body for redirect/retry.TuUnable to record file position for rewinding request body during a redirect/retry.ubody_pos must be of type integer, instead it was w.u
    Attempt to rewind body to a certain position.
    Primarily used for request redirects and retries.

    :param body:
        File-like object that supports seek.

    :param int pos:
        Position to seek to in file.
    auppera_METHODS_NOT_EXPECTING_BODYlTOstrObytesato_bytesareadDareturnutyping.Iterable[bytes]achunk_readableubody_to_chunks.<locals>.chunk_readableu'body' must be a bytes-like object, file-like object, or iterable. Instead was abodyamvanbytesaChunksAndContentLengthachunksacontent_lengthTachunksacontent_lengthuTakes the HTTP request method, body, and blocksize and
    transforms them into an iterable of chunks to pass to
    socket.sendall() and an optional 'Content-Length' header.

    A 'Content-Length' of 'None' indicates the length of the body
    can't be determined so should use 'Transfer-Encoding: chunked'
    for framing instead.
    aioaTextIOBaseablocksizeTuiso-8859-1a__doc__a__file__a__spec__aoriginahas_locationa__cached__a__annotations__aannotationsatypingabase64Tab64encodeaenumTaEnumaEnumaexceptionsTaUnrewindableBodyErrorlautilTato_byteslu@@@SKIP_HEADER@@@aSKIP_HEADERPahostuuser-agentuaccept-encodingaSKIPPABLE_HEADERSugzip,deflateabrotlicffia_unused_module_brotliabrotliu,brazstandarda_unused_module_zstdu,zstda__prepare__a_TYPE_FAILEDTELLa__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>uurllib3.util.requesta__module__a__qualname__atokena__orig_bases__uFinal[_TYPE_FAILEDTELL]aUniona_TYPE_BODY_POSITIONSaGETaCONNECTaDELETEaHEADaOPTIONSaTRACETnnnnnnDakeep_aliveaaccept_encodingauser_agentabasic_authaproxy_basic_authadisable_cacheareturnubool | Noneubool | list[str] | str | Noneustr | Noneustr | Noneustr | Noneubool | Noneudict[str, str]amake_headersDabodyaposareturnutyping.Anyu_TYPE_BODY_POSITION | Noneu_TYPE_BODY_POSITION | Noneaset_file_positionDabodyabody_posareturnutyping.IO[typing.AnyStr]a_TYPE_BODY_POSITIONaNoneaNamedTupleutyping.Iterable[bytes] | Noneuint | NoneDabodyamethodablocksizeareturnutyping.Any | NoneastraintaChunksAndContentLengthabody_to_chunksuurllib3\util\request.pyu<module urllib3.util.request>Ta__class__Tabodyamethodablocksizeachunksacontent_lengthachunk_readableamvTabodyablocksizeaencodeadatablockTablocksizeabodyTakeep_aliveaaccept_encodingauser_agentabasic_authaproxy_basic_authadisable_cacheaheadersTabodyabody_posabody_seekweTabodyapos.urllib3.util.response1aisclosedaclosedafpuUnable to determine whether fp is closed.u
    Checks whether a given file-like object is closed.

    :param obj:
        The file-like object to check.
    ahttplibaHTTPMessageuexpected httplib.Message, got uw.ais_multipartaget_payloadTObytesOstradefectsaStartBoundaryNotFoundDefectaMultipartInvariantViolationDefectaHeaderParsingErrorTadefectsaunparsed_datau
    Asserts whether all headers have been successfully parsed.
    Extracts encountered errors from the result of parsing headers.

    Only works on Python 3.

    :param http.client.HTTPMessage headers: Headers to verify.

    :raises urllib3.exceptions.HeaderParsingError:
        If parsing errors are found.
    a_methodaupperaHEADu
    Checks whether the request of a response has been a HEAD-request.

    :param http.client.HTTPResponse response:
        Response to check if the originating request
        used 'HEAD' as a method.
    a__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationsuhttp.clientlaclientuemail.errorsTaMultipartInvariantViolationDefectaStartBoundaryNotFoundDefectaexceptionsTaHeaderParsingErrorlDaobjareturnaobjectaboolais_fp_closedDaheadersareturnuhttplib.HTTPMessageaNoneaassert_header_parsingDaresponseareturnuhttplib.HTTPResponseaboolais_response_to_headuurllib3\util\response.pyu<module urllib3.util.response>Taheadersaunparsed_dataapayloadadefectsTaobjTaresponseamethod_str.urllib3.util.retry,atotalaconnectareadastatusaotherlaredirectastatus_forcelistaallowed_methodsabackoff_factorabackoff_maxaraise_on_redirectaraise_on_statusTahistoryarespect_retry_after_headeraremove_headers_on_redirectabackoff_jitteraloweru<genexpr>uRetry.__init__.<locals>.<genexpr>aDEFAULTaRetryaclsTaredirectalogadebuguConverted retries value: %r -> %ruBackwards-compatibility for the old retries format.atakewhileu<lambda>uRetry.get_backoff_time.<locals>.<lambda>lZarandomamaxaminuFormula for computing the current backoff

        :rtype: float
        aredirect_locationareamatchu^\s*[0-9]+\s*$aemailautilsaparsedate_tzaInvalidHeaderuInvalid Retry-After header: uamktime_tzatimeaheadersagetTuRetry-Afteraparse_retry_afteruGet the value of Retry-After in seconds.aget_retry_afterasleepaget_backoff_timeasleep_for_retrya_sleep_backoffuSleep between retry attempts.

        This method will respect a server's ``Retry-After`` response header
        and sleep the duration of the time requested. If that is not present, it
        will use an exponential backoff. By default, the backoff factor is 0 and
        this method will return immediately.
        aProxyErroraoriginal_erroraConnectTimeoutErroruErrors when we're fairly sure that the server did not receive the
        request, so it should be safe to retry.
        aReadTimeoutErroraProtocolErroruErrors that occur after the request has been started, so we should
        assume that the server began processing it.
        aupperuChecks if a given HTTP method should be retried upon, depending if
        it is included in the allowed_methods
        a_is_method_retryableaRETRY_AFTER_STATUS_CODESuIs this method/status code retryable? (Based on allowlists and control
        variables such as the number of total retries to allow, whether to
        respect the Retry-After header, whether this header is present, and
        whether the returned status code is on the list of status codes to
        be retried upon on the presence of the aforementioned header)
        uAre we out of retries?areraiselaunknowna_is_connection_erroraerrora_is_read_erroraget_redirect_locationutoo many redirectsaresponseaResponseErroraGENERIC_ERRORaSPECIFIC_ERRORaformatTastatus_codeaRequestHistoryamethodanewTatotalaconnectareadaredirectastatusaotherahistoryais_exhaustedaMaxRetryErroruIncremented Retry for (url='%s'): %ruReturn a new Retry object with incremented retry counters.

        :param response: A response object, or None, if the server did not
            return a response.
        :type response: :class:`~urllib3.response.BaseHTTPResponse`
        :param Exception error: An error encountered during the request, or
            None if the response was received successfully.

        :return: A new ``Retry`` object.
        a__name__u(total=u, connect=u, read=u, redirect=u, status=w)a__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationsaloggingatypingaitertoolsTatakewhileaTracebackTypeaexceptionsTaConnectTimeoutErroraInvalidHeaderaMaxRetryErroraProtocolErroraProxyErroraReadTimeoutErroraResponseErrorautilTareraiseagetLoggerTuurllib3.util.retryaNamedTuplea__prepare__a__getitem__u%s.__prepare__() must return a mapping, not %su<metaclass>uurllib3.util.retrya__module__a__qualname__a__annotations__ustr | NoneaurluException | Noneuint | Nonea__orig_bases__uRetry configuration.

    Each retry attempt will create a new Retry object with updated values, so
    they can be safely reused.

    Retries can be defined as a default for a pool:

    .. code-block:: python

        retries = Retry(connect=5, read=2, redirect=5)
        http = PoolManager(retries=retries)
        response = http.request("GET", "https://example.com/")

    Or per-request (which overrides the default for the pool):

    .. code-block:: python

        response = http.request("GET", "https://example.com/", retries=Retry(10))

    Retries can be disabled by passing ``False``:

    .. code-block:: python

        response = http.request("GET", "https://example.com/", retries=False)

    Errors will be wrapped in :class:`~urllib3.exceptions.MaxRetryError` unless
    retries are disabled, in which case the causing exception will be raised.

    :param int total:
        Total number of retries to allow. Takes precedence over other counts.

        Set to ``None`` to remove this constraint and fall back on other
        counts.

        Set to ``0`` to fail on the first retry.

        Set to ``False`` to disable and imply ``raise_on_redirect=False``.

    :param int connect:
        How many connection-related errors to retry on.

        These are errors raised before the request is sent to the remote server,
        which we assume has not triggered the server to process the request.

        Set to ``0`` to fail on the first retry of this type.

    :param int read:
        How many times to retry on read errors.

        These errors are raised after the request was sent to the server, so the
        request may have side-effects.

        Set to ``0`` to fail on the first retry of this type.

    :param int redirect:
        How many redirects to perform. Limit this to avoid infinite redirect
        loops.

        A redirect is a HTTP response with a status code 301, 302, 303, 307 or
        308.

        Set to ``0`` to fail on the first retry of this type.

        Set to ``False`` to disable and imply ``raise_on_redirect=False``.

    :param int status:
        How many times to retry on bad status codes.

        These are retries made on responses, where status code matches
        ``status_forcelist``.

        Set to ``0`` to fail on the first retry of this type.

    :param int other:
        How many times to retry on other errors.

        Other errors are errors that are not connect, read, redirect or status errors.
        These errors might be raised after the request was sent to the server, so the
        request might have side-effects.

        Set to ``0`` to fail on the first retry of this type.

        If ``total`` is not set, it's a good idea to set this to 0 to account
        for unexpected edge cases and avoid infinite retry loops.

    :param Collection allowed_methods:
        Set of uppercased HTTP method verbs that we should retry on.

        By default, we only retry on methods which are considered to be
        idempotent (multiple requests with the same parameters end with the
        same state). See :attr:`Retry.DEFAULT_ALLOWED_METHODS`.

        Set to a ``None`` value to retry on any verb.

    :param Collection status_forcelist:
        A set of integer HTTP status codes that we should force a retry on.
        A retry is initiated if the request method is in ``allowed_methods``
        and the response status code is in ``status_forcelist``.

        By default, this is disabled with ``None``.

    :param float backoff_factor:
        A backoff factor to apply between attempts after the second try
        (most errors are resolved immediately by a second try without a
        delay). urllib3 will sleep for::

            {backoff factor} * (2 ** ({number of previous retries}))

        seconds. If `backoff_jitter` is non-zero, this sleep is extended by::

            random.uniform(0, {backoff jitter})

        seconds. For example, if the backoff_factor is 0.1, then :func:`Retry.sleep` will
        sleep for [0.0s, 0.2s, 0.4s, 0.8s, ...] between retries. No backoff will ever
        be longer than `backoff_max`.

        By default, backoff is disabled (factor set to 0).

    :param bool raise_on_redirect: Whether, if the number of redirects is
        exhausted, to raise a MaxRetryError, or to return a response with a
        response code in the 3xx range.

    :param bool raise_on_status: Similar meaning to ``raise_on_redirect``:
        whether we should raise an exception, or return a response,
        if status falls in ``status_forcelist`` range and retries have
        been exhausted.

    :param tuple history: The history of the request encountered during
        each call to :meth:`~Retry.increment`. The list is in the order
        the requests occurred. Each list item is of class :class:`RequestHistory`.

    :param bool respect_retry_after_header:
        Whether to respect Retry-After header on status codes defined as
        :attr:`Retry.RETRY_AFTER_STATUS_CODES` or not.

    :param Collection remove_headers_on_redirect:
        Sequence of headers to remove from the request when a response
        indicating a redirect is returned before firing off the redirected
        request.
    PaGETaPUTaDELETEaHEADaOPTIONSaTRACEaDEFAULT_ALLOWED_METHODSPlllPaAuthorizationaCookieaDEFAULT_REMOVE_HEADERS_ON_REDIRECTlxaDEFAULT_BACKOFF_MAXutyping.ClassVar[Retry]l
Datotalaconnectareadaredirectastatusaotheraallowed_methodsastatus_forcelistabackoff_factorabackoff_maxaraise_on_redirectaraise_on_statusahistoryarespect_retry_after_headeraremove_headers_on_redirectabackoff_jitterareturnubool | int | Noneuint | Noneuint | Noneubool | int | Noneuint | Noneuint | Noneutyping.Collection[str] | Noneutyping.Collection[int] | Noneafloatpaboolputuple[RequestHistory, ...] | Noneaboolutyping.Collection[str]afloataNonea__init__uRetry.__init__Dakwareturnutyping.AnyaRetryuRetry.newTtnDaretriesaredirectadefaultareturnuRetry | bool | int | Noneubool | int | NoneuRetry | bool | int | NoneaRetryafrom_intuRetry.from_intDareturnafloatuRetry.get_backoff_timeDaretry_afterareturnastrafloatuRetry.parse_retry_afterDaresponseareturnaBaseHTTPResponseufloat | NoneuRetry.get_retry_afterDaresponseareturnaBaseHTTPResponseabooluRetry.sleep_for_retryDareturnaNoneuRetry._sleep_backoffTnDaresponseareturnuBaseHTTPResponse | NoneaNoneuRetry.sleepDaerrareturnaExceptionabooluRetry._is_connection_erroruRetry._is_read_errorDamethodareturnastrabooluRetry._is_method_retryableTFDamethodastatus_codeahas_retry_afterareturnastraintaboolpais_retryuRetry.is_retryDareturnabooluRetry.is_exhaustedTnnnnnnDamethodaurlaresponseaerrora_poola_stacktraceareturnustr | Noneustr | NoneuBaseHTTPResponse | NoneuException | NoneuConnectionPool | NoneuTracebackType | NoneaRetryaincrementuRetry.incrementDareturnastra__repr__uRetry.__repr__Tluurllib3\util\retry.pyTa.0whTwxu<module urllib3.util.retry>Ta__class__Taselfatotalaconnectareadaredirectastatusaotheraallowed_methodsastatus_forcelistabackoff_factorabackoff_maxaraise_on_redirectaraise_on_statusahistoryarespect_retry_after_headeraremove_headers_on_redirectabackoff_jitterTaselfTaselfaerrTaselfamethodTaselfabackoffTaclsaretriesaredirectadefaultanew_retriesTaselfaconsecutive_errors_lenabackoff_valueTaselfaresponsearetry_afterTaselfamethodaurlaresponseaerrora_poola_stacktraceatotalaconnectareadaredirectastatus_countaotheracauseastatusaredirect_locationaresponse_redirect_locationahistoryanew_retryareasonTaselfaretry_countsTaselfamethodastatus_codeahas_retry_afterTaselfakwaparamsTaselfaretry_afterasecondsaretry_date_tuplearetry_dateTaselfaresponseaslept.urllib3.util.ssl_#apypyTlllacpython:nlnlTlllTlllTll
uReturn True for CPython 3.8.9+, 3.9.3+ or 3.10+ and PyPy 7.3.8+ where
    setting SSLContext.hostname_checks_common_name to False works.

    Outside of CPython and PyPy we don't know which implementations work
    or not so we conservatively use our hostname matching as we know that works
    on all implementations.

    https://github.com/urllib3/urllib3/issues/2192#issuecomment-821832963
    https://foss.heptapod.net/pypy/pypy/-/issues/3539
    astartswithTuOpenSSL qa_is_bpo_43522_fixedaSSLErrorTuNo certificate for the peer.areplaceTw:ualoweraHASHFUNC_MAPagetuFingerprint of invalid length: uaunhexlifyaencodeadigestahmacacompare_digestuFingerprints did not match. Expected "u", got "ahexw"u
    Checks if given fingerprint matches the supplied certificate.

    :param cert:
        Certificate as bytes object.
    :param fingerprint:
        Fingerprint as string of hexdigits, can be interspersed by colons.
    aCERT_REQUIREDasslaCERT_u
    Resolves the argument to a numeric constant, which can be passed to
    the wrap_socket function/method from the ssl module.
    Defaults to :data:`ssl.CERT_REQUIRED`.
    If given a string it is assumed to be the name of the constant in the
    :mod:`ssl` module or its abbreviation.
    (So you can specify `REQUIRED` instead of `CERT_REQUIRED`.
    If it's neither `None` nor a string we assume it is already the numeric
    constant which can directly be passed to wrap_socket.
    aPROTOCOL_TLSaPROTOCOL_atypingacastu
    like resolve_cert_reqs
    aSSLContextuCan't create an SSLContext object without an ssl moduleaPROTOCOL_TLS_CLIENTuCan't specify both 'ssl_version' and either 'ssl_minimum_version' or 'ssl_maximum_version'a_SSL_VERSION_TO_TLS_VERSIONaTLSVersionaMINIMUM_SUPPORTEDaMAXIMUM_SUPPORTEDawarningsawarnaDeprecationWarningTu'ssl_version' option is deprecated and will be removed in urllib3 v2.1.0. Instead use 'ssl_minimum_version'Tacategoryastacklevelaminimum_versionaTLSv1_2acontextamaximum_versionaset_cipherslaOP_NO_SSLv2aOP_NO_SSLv3aOP_NO_COMPRESSIONaOP_NO_TICKETaoptionsapost_handshake_authaIS_PYOPENSSLaverify_modeacheck_hostnameahostname_checks_common_nameakeylog_filenameaosaenvironTaSSLKEYLOGFILEuCreates and configures an :class:`ssl.SSLContext` instance for use with urllib3.

    :param ssl_version:
        The desired protocol version to use. This will default to
        PROTOCOL_SSLv23 which will negotiate the highest protocol that both
        the server and your installation of OpenSSL support.

        This parameter is deprecated instead use 'ssl_minimum_version'.
    :param ssl_minimum_version:
        The minimum version of TLS to be used. Use the 'ssl.TLSVersion' enum for specifying the value.
    :param ssl_maximum_version:
        The maximum version of TLS to be used. Use the 'ssl.TLSVersion' enum for specifying the value.
        Not recommended to set to anything other than 'ssl.TLSVersion.MAXIMUM_SUPPORTED' which is the
        default value.
    :param cert_reqs:
        Whether to require the certificate verification. This defaults to
        ``ssl.CERT_REQUIRED``.
    :param options:
        Specific OpenSSL options. These default to ``ssl.OP_NO_SSLv2``,
        ``ssl.OP_NO_SSLv3``, ``ssl.OP_NO_COMPRESSION``, and ``ssl.OP_NO_TICKET``.
    :param ciphers:
        Which cipher suites to allow the server to select. Defaults to either system configured
        ciphers if OpenSSL 1.1.1+, otherwise uses a secure default set of ciphers.
    :returns:
        Constructed SSLContext object with specified options
    :rtype: SSLContext
    acreate_urllib3_contextTaciphersaload_verify_locationsaload_default_certsa_is_key_file_encryptedTuClient private key is encrypted, password is requiredaload_cert_chainakeyfileaset_alpn_protocolsaALPN_PROTOCOLSa_ssl_wrap_socket_implu
    All arguments except for server_hostname, ssl_context, tls_in_tls, ca_cert_data and
    ca_cert_dir have the same meaning as they do when using
    :func:`ssl.create_default_context`, :meth:`ssl.SSLContext.load_cert_chain`,
    :meth:`ssl.SSLContext.set_ciphers` and :meth:`ssl.SSLContext.wrap_socket`.

    :param server_hostname:
        When SNI is supported, the expected hostname of the certificate
    :param ssl_context:
        A pre-made :class:`SSLContext` object. If none is provided, one will
        be created using :func:`create_urllib3_context`.
    :param ciphers:
        A string of ciphers we wish the client to support.
    :param ca_cert_dir:
        A directory containing CA certificates in multiple separate files, as
        supported by OpenSSL's -CApath flag or the capath argument to
        SSLContext.load_verify_locations().
    :param key_password:
        Optional password if the keyfile is encrypted.
    :param ca_cert_data:
        Optional string containing CA certificates in PEM format suitable for
        passing as the cadata parameter to SSLContext.load_verify_locations()
    :param tls_in_tls:
        Use SSLTransport to wrap the existing socket.
    adecodeTaasciia_IPV4_REamatcha_BRACELESS_IPV6_ADDRZ_REuDetects whether the hostname given is an IPv4 or IPv6 address.
    Also detects IPv6 addresses with Zone IDs.

    :param str hostname: Hostname to examine.
    :return: True if the hostname is an IP address, False otherwise.
    a__enter__a__exit__aENCRYPTEDTnnnuDetects if a key file is encrypted or not.aSSLTransportaProxySchemeUnsupportedTuTLS in TLS requires support for the 'ssl' modulea_validate_ssl_context_for_tls_in_tlsawrap_socketTaserver_hostnamea__doc__a__file__a__spec__aoriginahas_locationa__cached__a__annotations__aannotationsasocketasysabinasciiTaunhexlifyahashlibTamd5asha1asha256amd5asha1asha256aexceptionsTaProxySchemeUnsupportedaSSLErroraurlTa_BRACELESS_IPV6_ADDRZ_REa_IPV4_RElaHAS_NEVER_CHECK_COMMON_NAMEuhttp/1.1aTupleTOintppOstrOinta_TYPE_VERSION_INFOl l(l@Daimplementation_nameaversion_infoapypy_version_infoareturnastra_TYPE_VERSION_INFOu_TYPE_VERSION_INFO | NoneaboolDaopenssl_versionaopenssl_version_numberaimplementation_nameaversion_infoapypy_version_infoareturnastraintastra_TYPE_VERSION_INFOu_TYPE_VERSION_INFO | Noneaboola_is_has_never_check_common_name_reliableudict[int, int]TaCERT_REQUIREDaHAS_NEVER_CHECK_COMMON_NAMEaOP_NO_COMPRESSIONaOP_NO_TICKETaOPENSSL_VERSIONaOPENSSL_VERSION_NUMBERaPROTOCOL_TLSaPROTOCOL_TLS_CLIENTaOP_NO_SSLv2aOP_NO_SSLv3aSSLContextaTLSVersionaOPENSSL_VERSIONaOPENSSL_VERSION_NUMBERaPROTOCOL_SSLv23aimplementationanameapypy_version_infoTaTLSv1aTLSv1_1aTLSv1_2aattrassltransportTaSSLTransportll@lllaUnionTa_TYPE_PEER_CERT_RET_DICTObytesna_TYPE_PEER_CERT_RETDacertafingerprintareturnubytes | NoneastraNoneaassert_fingerprintDacandidateareturnuNone | int | straVerifyModearesolve_cert_reqsDacandidateareturnuNone | int | straintaresolve_ssl_versionTnnnnnnDassl_versionacert_reqsaoptionsaciphersassl_minimum_versionassl_maximum_versionareturnuint | Noneuint | Noneuint | Noneustr | Noneuint | Noneuint | Noneussl.SSLContextaoverloadTQQQQQQQQQQQQDasockakeyfileacertfileacert_reqsaca_certsaserver_hostnameassl_versionaciphersassl_contextaca_cert_dirakey_passwordaca_cert_dataatls_in_tlsareturnusocket.socketustr | Noneustr | Noneuint | Noneustr | Noneustr | Noneuint | Noneustr | Noneussl.SSLContext | Noneustr | Noneustr | NoneuNone | str | bytesuLiteral[False]ussl.SSLSocketassl_wrap_socketDasockakeyfileacertfileacert_reqsaca_certsaserver_hostnameassl_versionaciphersassl_contextaca_cert_dirakey_passwordaca_cert_dataatls_in_tlsareturnusocket.socketustr | Noneustr | Noneuint | Noneustr | Noneustr | Noneuint | Noneustr | Noneussl.SSLContext | Noneustr | Noneustr | NoneuNone | str | bytesaboolussl.SSLSocket | SSLTransportTypeTnnnnnnnnnnnFDahostnameareturnustr | bytesaboolais_ipaddressDakey_fileareturnastraboolTnDasockassl_contextatls_in_tlsaserver_hostnameareturnusocket.socketussl.SSLContextaboolustr | Noneussl.SSLSocket | SSLTransportTypeuurllib3\util\ssl_.pyu<module urllib3.util.ssl_>Taimplementation_nameaversion_infoapypy_version_infoamajor_minoramicroTaopenssl_versionaopenssl_version_numberaimplementation_nameaversion_infoapypy_version_infoais_opensslais_openssl_issue_14579_fixedTakey_filewfalineTasockassl_contextatls_in_tlsaserver_hostnameTacertafingerprintadigest_lengthahashfuncafingerprint_bytesacert_digestTassl_versionacert_reqsaoptionsaciphersassl_minimum_versionassl_maximum_versionacontextasslkeylogfileTahostnameTacandidatearesT
asockakeyfileacertfileacert_reqsaca_certsaserver_hostnameassl_versionaciphersassl_contextaca_cert_dirakey_passwordaca_cert_dataatls_in_tlsTasockakeyfileacertfileacert_reqsaca_certsaserver_hostnameassl_versionaciphersassl_contextaca_cert_dirakey_passwordaca_cert_dataatls_in_tlsacontextweassl_sock.urllib3.util.ssl_match_hostname[asplitTw.l:lnnacountTw*aCertificateErrorutoo many wildcards in certificate DNS name: alowerw*u[^.]+astartswithTuxn--areaescapeareplaceTu\*u[^.]*apatsaappendacompileu\Au\.u\ZaIGNORECASEamatchahostnameuMatching according to RFC 6125, section 6.4.3

    http://tools.ietf.org/html/rfc6125#section-6.4.3
    aipaddressaip_addressarstripapackeduExact matching of IP addresses.

    RFC 9110 section 4.3.5: "A reference identity of IP-ID contains the decoded
    bytes of the IP address. An IP version 4 address is 4 octets, and an IP
    version 6 address is 16 octets. [...] A reference identity of type IP-ID
    matches if the address is identical to an iPAddress value of the
    subjectAltName extension of the certificate."
    uempty or no certificate, match_hostname needs a SSL socket or SSL context with either CERT_OPTIONAL or CERT_REQUIREDw%arfindTw%agetTasubjectAltNameTutoo many values to unpack (expected 2)aDNSahost_ipa_dnsname_matchadnsnamesavalueuIP Addressa_ipaddress_matchTasubjectTacommonNameuhostname %r doesn't match either of %su, arepruhostname uu doesn't match Tuno appropriate subjectAltName fields were founduVerify that *cert* (in decoded format as returned by
    SSLSocket.getpeercert()) matches the *hostname*.  RFC 2818 and RFC 6125
    rules are followed, but IP addresses are not accepted for *hostname*.

    CertificateError is raised on failure. On success, the function
    returns nothing.
    uThe match_hostname() function from Python 3.5, essential when using SSL.a__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationsatypingTaIPv4AddressaIPv6AddressaIPv4AddressaIPv6Addressu3.5.0.1a__version__TEValueErrora__prepare__a__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>uurllib3.util.ssl_match_hostnamea__module__a__qualname__a__orig_bases__TlDadnahostnameamax_wildcardsareturnutyping.Anyastraintutyping.Match[str] | None | boolDaipnameahost_ipareturnastruIPv4Address | IPv6AddressaboolTFDacertahostnameahostname_checks_common_nameareturnu_TYPE_PEER_CERT_RET_DICT | NoneastraboolaNoneamatch_hostnameuurllib3\util\ssl_match_hostname.pyu<module urllib3.util.ssl_match_hostname>T
adnahostnameamax_wildcardsapatsapartsaleftmostaremainderawildcardsafragapatTaipnameahost_ipaipTacertahostnameahostname_checks_common_nameasanakeyavalueahost_ipadnsnamesasub.urllib3.util.ssltransportNawrap_bioaProxySchemeUnsupportedTuTLS in TLS requires SSLContext.wrap_bio() which isn't available on non-native SSLContextu
        Raises a ProxySchemeUnsupported if the provided ssl_context can't be used
        for TLS in TLS.

        The only requirement is that the ssl_context provides the 'wrap_bio'
        methods.
        asslaMemoryBIOaincomingaoutgoingasuppress_ragged_eofsasocketTaserver_hostnameasslobja_ssl_io_loopado_handshakeu
        Create an SSLTransport around socket using the provided ssl_context.
        acloseafilenoa_wrap_ssl_readlunon-zero flags not allowed in calls to recvunon-zero flags not allowed in calls to recv_intoareadabufferunon-zero flags not allowed in calls to sendalla__enter__a__exit__acastTwBacountaselfasendTnnnunon-zero flags not allowed in calls to sendawriteSwrwbwwuinvalid mode uu (only r, w, b allowed)wwwrwbaSocketIOa_io_refsllaioaDEFAULT_BUFFER_SIZEuunbuffered streams must be binaryaBufferedRWPairaBufferedReaderaBufferedWriteraTextIOWrapperamodeu
        Python's httpclient uses makefile and buffered io when reading HTTP
        messages and we need to support it.

        This is unfortunately a copy and paste of socket.py makefile with small
        changes to point to the socket directly.
        aunwrapagetpeercertaversionacipheraselected_alpn_protocolaselected_npn_protocolashared_ciphersacompressionasettimeoutagettimeouta_decref_socketiosaSSLErroraerrnoaSSL_ERROR_EOFashould_loopaarg1aarg2afuncaSSL_ERROR_WANT_READaSSL_ERROR_WANT_WRITEasendallarecvaSSL_BLOCKSIZEawrite_eofatypinga_ReturnValuearetuPerforms an I/O loop between incoming/outgoing and the socket.a__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationsaexceptionsTaProxySchemeUnsupportedlaTypeVarTa_SelfTaSSLTransportTabounda_SelfTaUnionTObytearrayOmemoryviewa_WriteBufferTa_ReturnValuel@uurllib3.util.ssltransporta__module__u
    The SSLTransport wraps an existing socket and establishes an SSL connection.

    Contrary to Python's implementation of SSLSocket, it allows you to chain
    multiple TLS connections together. It's particularly useful if you need to
    implement TLS within TLS.

    The class supports most of the socket API operations.
    aSSLTransporta__qualname__Dassl_contextareturnussl.SSLContextaNonea_validate_ssl_context_for_tls_in_tlsuSSLTransport._validate_ssl_context_for_tls_in_tlsTntDasocketassl_contextaserver_hostnameasuppress_ragged_eofsareturnusocket.socketussl.SSLContextustr | NoneaboolaNonea__init__uSSLTransport.__init__Daselfareturna_SelfTpuSSLTransport.__enter__Dw_areturnutyping.AnyaNoneuSSLTransport.__exit__DareturnaintuSSLTransport.filenoTlnDalenabufferareturnaintutyping.Any | Noneuint | bytesuSSLTransport.readTllDabuflenaflagsareturnaintpuint | bytesuSSLTransport.recvTnlDabufferanbytesaflagsareturna_WriteBufferuint | NoneaintuNone | int | bytesarecv_intouSSLTransport.recv_intoTlDadataaflagsareturnabytesaintaNoneuSSLTransport.sendallDadataaflagsareturnabytesaintpuSSLTransport.sendTnDaencodingaerrorsanewlinennnDamodeabufferingaencodingaerrorsanewlineareturnastruint | Noneustr | Noneustr | Noneustr | Noneutyping.BinaryIO | typing.TextIO | socket.SocketIOamakefileuSSLTransport.makefileDareturnaNoneuSSLTransport.unwrapuSSLTransport.closeaoverloadTQDabinary_formareturnuLiteral[False]u_TYPE_PEER_CERT_RET_DICT | NoneuSSLTransport.getpeercertDabinary_formareturnuLiteral[True]ubytes | NoneTFDabinary_formareturnaboola_TYPE_PEER_CERT_RETDareturnustr | NoneuSSLTransport.versionDareturnutuple[str, str, int] | NoneuSSLTransport.cipheruSSLTransport.selected_alpn_protocoluSSLTransport.selected_npn_protocolDareturnulist[tuple[str, str, int]] | NoneuSSLTransport.shared_ciphersuSSLTransport.compressionDavalueareturnufloat | NoneaNoneuSSLTransport.settimeoutDareturnufloat | NoneuSSLTransport.gettimeoutuSSLTransport._decref_socketiosDalenabufferareturnaintubytearray | Noneuint | bytesuSSLTransport._wrap_ssl_readDafuncareturnutyping.Callable[[], None]aNoneuSSLTransport._ssl_io_loopDafuncaarg1areturnutyping.Callable[[bytes], int]abytesaintDafuncaarg1aarg2areturnutyping.Callable[[int, bytearray | None], bytes]aintubytearray | NoneabytesTnnDafuncaarg1aarg2areturnutyping.Callable[..., _ReturnValue]uNone | bytes | intubytearray | Nonea_ReturnValueTuurllib3\util\ssltransport.pyu<module urllib3.util.ssltransport>Ta__class__TaselfTaselfw_Taselfasocketassl_contextaserver_hostnameasuppress_ragged_eofsTaselfafuncTaselfafuncaarg1Taselfafuncaarg1aarg2Taselfafuncaarg1aarg2ashould_looparetaerrnoweabufTassl_contextTaselfalenabufferweTaselfabinary_formT
aselfamodeabufferingaencodingaerrorsanewlineabufferawritingareadingabinaryarawmodearawatextTaselfalenabufferTaselfabuflenaflagsTaselfabufferanbytesaflagsTaselfadataaflagsTaselfadataaflagsacountaviewabyte_viewaamountwvTaselfavalue.urllib3.util.timeout?ma_validate_timeoutaconnecta_connectareada_readatotala_start_connecta__name__uu(connect=u, read=u, total=w)a_DEFAULT_TIMEOUTagetdefaulttimeoutuTimeout cannot be a boolean value. It must be an int, float or None.TETypeErrorEValueErroruTimeout value %s was %s, but it must be an int, float or None.luAttempted to set %s timeout to %s, but the timeout cannot be set to a value less than or equal to 0.uCheck that a timeout attribute is valid.

        :param value: The timeout value to validate
        :param name: The name of the timeout attribute to validate. This is
            used to specify in error messages.
        :return: The validated and casted version of the given value.
        :raises ValueError: If it is a numeric value less than or equal to
            zero, or the type is not an integer, float, or None.
        aTimeoutTareadaconnectuCreate a new Timeout from a legacy timeout value.

        The timeout value used by httplib.py sets the same timeout on the
        connect(), and recv() socket requests. This creates a :class:`Timeout`
        object that sets the individual timeouts to the ``timeout`` value
        passed to this function.

        :param timeout: The legacy timeout value.
        :type timeout: integer, float, :attr:`urllib3.util.Timeout.DEFAULT_TIMEOUT`, or None
        :return: Timeout object
        :rtype: :class:`Timeout`
        TaconnectareadatotaluCreate a copy of the timeout object

        Timeout properties are stored per-pool but each request needs a fresh
        Timeout object to ensure each one has its own start/stop configured.

        :return: a copy of the timeout object
        :rtype: :class:`Timeout`
        aTimeoutStateErrorTuTimeout timer has already been started.atimeamonotonicuStart the timeout clock, used during a connect() attempt

        :raises urllib3.exceptions.TimeoutStateError: if you attempt
            to start a timer that has been started already.
        TuCan't get connect duration for timer that has not started.uGets the time elapsed since the call to :meth:`start_connect`.

        :return: Elapsed time in seconds.
        :rtype: float
        :raises urllib3.exceptions.TimeoutStateError: if you attempt
            to get duration for a timer that hasn't been started.
        aminuGet the value to use when setting a connection timeout.

        This will be a positive float or integer, the value None
        (never timeout), or the default system timeout.

        :return: Connect timeout.
        :rtype: int, float, :attr:`Timeout.DEFAULT_TIMEOUT` or None
        amaxaget_connect_durationaresolve_default_timeoutuGet the value for the read timeout.

        This assumes some time has elapsed in the connection timeout and
        computes the read timeout appropriately.

        If self.total is set, the read timeout is dependent on the amount of
        time taken by the connect timeout. If the connection time has not been
        established, a :exc:`~urllib3.exceptions.TimeoutStateError` will be
        raised.

        :return: Value to use for the read timeout.
        :rtype: int, float or None
        :raises urllib3.exceptions.TimeoutStateError: If :meth:`start_connect`
            has not yet been called on this object.
        a__doc__a__file__a__spec__aoriginahas_locationa__cached__a__annotations__aannotationsatypingaenumTaEnumaEnumasocketTagetdefaulttimeoutaexceptionsTaTimeoutStateErrorla__prepare__a_TYPE_DEFAULTa__getitem__u%s.__prepare__() must return a mapping, not %su<metaclass>uurllib3.util.timeouta__module__a__qualname__latokena__orig_bases__uFinal[_TYPE_DEFAULT]aOptionalaUniona_TYPE_TIMEOUTuTimeout configuration.

    Timeouts can be defined as a default for a pool:

    .. code-block:: python

        import urllib3

        timeout = urllib3.util.Timeout(connect=2.0, read=7.0)

        http = urllib3.PoolManager(timeout=timeout)

        resp = http.request("GET", "https://example.com/")

        print(resp.status)

    Or per-request (which overrides the default for the pool):

    .. code-block:: python

       response = http.request("GET", "https://example.com/", timeout=Timeout(10))

    Timeouts can be disabled by setting all the parameters to ``None``:

    .. code-block:: python

       no_timeout = Timeout(connect=None, read=None)
       response = http.request("GET", "https://example.com/", timeout=no_timeout)


    :param total:
        This combines the connect and read timeouts into one; the read timeout
        will be set to the time leftover from the connect attempt. In the
        event that both a connect timeout and a total are specified, or a read
        timeout and a total are specified, the shorter timeout will be applied.

        Defaults to None.

    :type total: int, float, or None

    :param connect:
        The maximum amount of time (in seconds) to wait for a connection
        attempt to a server to succeed. Omitting the parameter will default the
        connect timeout to the system default, probably `the global default
        timeout in socket.py
        <http://hg.python.org/cpython/file/603b4d593758/Lib/socket.py#l535>`_.
        None will set an infinite timeout for connection attempts.

    :type connect: int, float, or None

    :param read:
        The maximum amount of time (in seconds) to wait between consecutive
        read operations for a response from the server. Omitting the parameter
        will default the read timeout to the system default, probably `the
        global default timeout in socket.py
        <http://hg.python.org/cpython/file/603b4d593758/Lib/socket.py#l535>`_.
        None will set an infinite timeout.

    :type read: int, float, or None

    .. note::

        Many factors can affect the total amount of time for urllib3 to return
        an HTTP response.

        For example, Python's DNS resolver does not obey the timeout specified
        on the socket. Other factors that can affect total request time include
        high CPU load, high swap, the program running at a low priority level,
        or other behaviors.

        In addition, the read and total timeouts only measure the time between
        read operations on the socket connecting the client and the server,
        not the total amount of time for the request to return a complete
        response. For most requests, the timeout is raised because the server
        has not sent the first byte in the specified time. This is not always
        the case; if a server streams one byte every fifteen seconds, a timeout
        of 20 seconds will not trigger, even though the request will take
        several minutes to complete.

        If your goal is to cut off any request after a set amount of wall clock
        time, consider having a second "watcher" thread to cut off a slow
        request.
    aDEFAULT_TIMEOUTDatotalaconnectareadareturna_TYPE_TIMEOUTppaNonea__init__uTimeout.__init__Dareturnastra__repr__uTimeout.__repr__a__str__Datimeoutareturna_TYPE_TIMEOUTufloat | NoneuTimeout.resolve_default_timeoutDavalueanameareturna_TYPE_TIMEOUTastra_TYPE_TIMEOUTuTimeout._validate_timeoutDatimeoutareturna_TYPE_TIMEOUTaTimeoutafrom_floatuTimeout.from_floatDareturnaTimeoutacloneuTimeout.cloneDareturnafloatastart_connectuTimeout.start_connectuTimeout.get_connect_durationDareturna_TYPE_TIMEOUTaconnect_timeoutuTimeout.connect_timeoutDareturnufloat | Nonearead_timeoutuTimeout.read_timeoutTuurllib3\util\timeout.pyu<module urllib3.util.timeout>Ta__class__TaselfatotalaconnectareadTaselfTaclsavalueanameTaclsatimeoutTatimeout.urllib3.util.urlastartswithTw/w/apathalowera__class__a__new__ahostuFor backwards-compatibility with urlparse. We're nice like that.aqueryw?uAbsolute path including the query string.aauthanetlocuw@u
        Authority component as defined in RFC 3986 3.2.
        This includes userinfo (auth), host and port.

        i.e.
            userinfo@host:port
        aportw:u
        Network location including host and port.

        If you need the equivalent of urllib.parse's ``netloc``,
        use the ``authority`` property instead.
        utoo many values to unpack (expected 7)u://w#u
        Convert self into a url

        This function should more or less round-trip with :func:`.parse_url`. The
        returned url may not be exactly the same as the url inputted to
        :func:`.parse_url`, but it should be equivalent by the RFC (e.g., urls
        with a blank port will have : removed).

        Example:

        .. code-block:: python

            import urllib3

            U = urllib3.util.parse_url("https://google.com/mail/")

            print(U.url)
            # "https://google.com/mail/"

            print( urllib3.util.Url("https", "username:password",
                                    "host.com", 80, "/path", "query", "fragment"
                                    ).url
                )
            # "https://username:password@host.com:80/path?query#fragment"
        aurlato_stra_PERCENT_REasubnu<lambda>u_encode_invalid_chars.<locals>.<lambda>utoo many values to unpack (expected 2)aencodeTuutf-8asurrogatepassacountTd%Blld%ladecodeaencoded_componentabyteaextend:lnnazfillTlaupperuPercent-encodes a URI component without reapplying
    onto an already percent-encoded component.
    agroupTlasplitw.u..aoutputaappendapopainsertTluaendswithTTu/.u/..Tua_NORMALIZABLE_SCHEMESa_IPV6_ADDRZ_REamatcha_ZONE_ID_REasearchaspanTlTu%25u%25:lnn:lnna_encode_invalid_charsa_UNRESERVED_CHARSw%a_IPV4_REd.Tw.a_idna_encodeaasciiaisasciiaidnaaLocationParseErrorTuUnable to parse URL without the 'idna' moduleDastrictastd3_rulestpaIDNAErroruName 'u' is not a valid IDNA labelTaasciia_TARGET_REu is not a valid request URIagroupsa_PATH_CHARSa_QUERY_CHARSuPercent-encodes a request target so that there are no invalid characters

    Pre-condition for this function is that 'target' must start with '/'.
    If that is the case then _TARGET_RE will always produce a match.
    aUrla_SCHEME_REu//a_URI_REutoo many values to unpack (expected 5)aschemearpartitionTw@utoo many values to unpack (expected 3)a_HOST_PORT_REa_USERINFO_CHARSTnnnla_normalize_hosta_remove_path_dot_segmentsa_FRAGMENT_CHARSTEValueErrorEAttributeErroraport_intTaschemeaauthahostaportapathaqueryafragmentu
    Given a url, return a parsed :class:`.Url` namedtuple. Best-effort is
    performed to parse incomplete urls. Fields not provided will be None.
    This parser is RFC 3986 and RFC 6874 compliant.

    The parser logic and helper functions are based heavily on
    work done in the ``rfc3986`` module.

    :param str url: URL to parse into a :class:`.Url` namedtuple.

    Partly backwards-compatible with :mod:`urllib.parse`.

    Example:

    .. code-block:: python

        import urllib3

        print( urllib3.util.parse_url('http://google.com/mail/'))
        # Url(scheme='http', host='google.com', port=None, path='/mail/', ...)

        print( urllib3.util.parse_url('google.com:80'))
        # Url(scheme=None, host='google.com', port=80, path=None, ...)

        print( urllib3.util.parse_url('/foo?bar'))
        # Url(scheme=None, host=None, port=None, path='/foo', query='bar', ...)
    a__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationsareatypingaexceptionsTaLocationParseErrorlautilTato_strTahttpahttpsnacompileTu%[a-fA-F0-9]{2}Tu^(?:[a-zA-Z][a-zA-Z0-9+-]*:|/)u^(?:([a-zA-Z][a-zA-Z0-9+.-]*):)?(?://([^\\/?#]*))?([^?#]*)(?:\?([^#]*))?(?:#(.*))?$aUNICODEaDOTALLu(?:[0-9]{1,3}\.){3}[0-9]{1,3}a_IPV4_PATu[0-9A-Fa-f]{1,4}a_HEX_PATu(?:{hex}:{hex}|{ipv4})Tahexaipv4a_LS32_PATahexals32a_subsLu(?:%(hex)s:){6}%(ls32)su::(?:%(hex)s:){5}%(ls32)su(?:%(hex)s)?::(?:%(hex)s:){4}%(ls32)su(?:(?:%(hex)s:)?%(hex)s)?::(?:%(hex)s:){3}%(ls32)su(?:(?:%(hex)s:){0,2}%(hex)s)?::(?:%(hex)s:){2}%(ls32)su(?:(?:%(hex)s:){0,3}%(hex)s)?::%(hex)s:%(ls32)su(?:(?:%(hex)s:){0,4}%(hex)s)?::%(ls32)su(?:(?:%(hex)s:){0,5}%(hex)s)?::%(hex)su(?:(?:%(hex)s:){0,6}%(hex)s)?::a_variationsuABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789._\-~a_UNRESERVED_PATu(?:w|w)a_IPV6_PATu(?:%25|%)(?:[u]|%[a-fA-F0-9]{2})+a_ZONE_ID_PATu\[u)?\]a_IPV6_ADDRZ_PATu(?:[^\[\]%:/?#]|%[a-fA-F0-9]{2})*a_REG_NAME_PATTu^(/[^?#]*)(?:\?([^#]*))?(?:#.*)?$w^w$a_IPV6_RE:llna_BRACELESS_IPV6_ADDRZ_REw(u)\]$u^(%s|%s|%s)(?::0*?(|0|[1-9][0-9]{0,4}))?$a_HOST_PORT_PATSBwMwFwYwiwrwxw2wpw-wdw8wXwKwQwcwDwywlwEwSwOw3whwHwmw.wPwBwUwJwzwVwAw6wIwLwkw5wowaw0wZw7w_wgw1wTwGwfwewWwuwwwRwnwbwqwjwtw~wNwsw4w9wvwCSw=w(w*w!w$w;w&w,w)w+w'a_SUB_DELIM_CHARSSw:Sw@w/Sw?aNamedTupleaOptionalafragmenta__prepare__a__getitem__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>uurllib3.util.urla__module__u
    Data structure for representing an HTTP URL. Used as a return value for
    :func:`parse_url`. Both the scheme and host are normalized as they are
    both case-insensitive according to RFC 3986.
    a__qualname__TnnnnnnnDaschemeaauthahostaportapathaqueryafragmentustr | Noneustr | Noneustr | Noneuint | Noneustr | Noneustr | Noneustr | NoneuUrl.__new__apropertyDareturnustr | NoneahostnameuUrl.hostnameDareturnastrarequest_uriuUrl.request_uriaauthorityuUrl.authorityuUrl.netlocuUrl.urla__str__uUrl.__str__a__orig_bases__aoverloadDacomponentaallowed_charsareturnastrutyping.Container[str]astrDacomponentaallowed_charsareturnaNoneutyping.Container[str]aNoneDacomponentaallowed_charsareturnustr | Noneutyping.Container[str]ustr | NoneDapathareturnastrpDahostaschemeareturnaNoneustr | NoneaNoneDahostaschemeareturnastrustr | NoneastrDahostaschemeareturnustr | Noneustr | Noneustr | NoneDanameareturnastrabytesDatargetareturnastrpa_encode_targetDaurlareturnastraUrlaparse_urluurllib3\util\url.pyTamatchu<module urllib3.util.url>Ta__class__Taclsaschemeaauthahostaportapathaqueryafragmenta__class__TaselfTacomponentaallowed_charsTacomponentaallowed_charsapercent_encodingsauri_bytesais_percent_encodedaencoded_componentwiabyteabyte_ordTatargetamatchapathaqueryaencoded_targetTanameaidnaTahostaschemeTahostaschemeais_ipv6amatchastartaendazone_idTapathasegmentsaoutputasegmentTaselfauserinfoanetlocTaurlaschemeaauthorityaauthahostaportaport_intapathaqueryafragmentasource_urlanormalize_uriw_ahost_portweTaselfauriTaselfaschemeaauthahostaportapathaqueryafragmentaurl.urllib3.util.utilL unot expecting type a__name__uaencodeuutf-8astrictTaerrorsadecodea__traceback__awith_tracebacka__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationsatypingaTracebackTypelTnnDwxaencodingaerrorsareturnustr | bytesustr | Noneustr | Noneabytesato_bytesDwxaencodingaerrorsareturnustr | bytesustr | Noneustr | Noneastrato_strTnDatpavalueatbareturnutype[BaseException] | NoneaBaseExceptionuTracebackType | Noneutyping.NoReturnareraiseuurllib3\util\util.pyu<module urllib3.util.util>TatpavalueatbTwxaencodingaerrors.urllib3.util.wait4umust specify at least one of read=True, write=Trueasockapartialaselectarcheckawcheckutoo many values to unpack (expected 3)laPOLLINaPOLLOUTapollaregisterDwtareturnufloat | Noneulist[tuple[int, int]]ado_pollupoll_wait_for_socket.<locals>.do_polllapoll_objTlTEAttributeErrorEOSErrora_have_working_pollapoll_wait_for_socketawait_for_socketaselect_wait_for_socketTareadatimeoutuWaits for reading to be available on a given socket.
    Returns True if the socket is readable, or False if the timeout expired.
    TawriteatimeoutuWaits for writing to be available on a given socket.
    Returns True if the socket is readable, or False if the timeout expired.
    a__doc__a__file__a__spec__aoriginahas_locationa__cached__aannotationsasocketawait_for_readawait_for_writea__all__TFpnDasockareadawriteatimeoutareturnusocket.socketaboolpufloat | NoneaboolDareturnaboolTnDasockatimeoutareturnusocket.socketufloat | Noneabooluurllib3\util\wait.pyu<module urllib3.util.wait>Tapoll_objTwtapoll_objTasockareadawriteatimeoutamaskapoll_objado_pollT
asockareadawriteatimeoutarcheckawcheckafnarreadyawreadyaxreadyTasockatimeoutTasockareadawriteatimeoutu.zstandard.backend_cffiTKaosacpu_countlasysconfTaSC_NPROCESSORS_ONLNTEAttributeErrorEValueErroruThe byte offset of this segment within its parent buffer.uObtain the length of the segment, in bytes.uObtain bytes copy of this segment.uTotal sizein bytes of the backing buffer.uObtains a segment within the buffer.

        The returned object references memory within this buffer.

        :param i:
           Integer index of segment to retrieve.
        :return:
           :py:class:`BufferSegment`
        uObtain the array of ``(offset, length)`` segments in the buffer.

        :return:
           :py:class:`BufferSegments`
        uObtain bytes copy of this instance.uThe number of segments within all ``BufferWithSegments``.uObtain the ``BufferSegment`` at an offset.affiastringalibaZSTD_getErrorNameadecodeTuutf-8aZSTD_createCCtxParamsaNULLagcaZSTD_freeCCtxParamsaZSTD_c_formataformataZSTD_c_compressionLevelacompression_levelaZSTD_c_windowLogawindow_logaZSTD_c_hashLogahash_logaZSTD_c_chainLogachain_logaZSTD_c_searchLogasearch_logaZSTD_c_minMatchamin_matchaZSTD_c_targetLengthatarget_lengthaZSTD_c_strategyastrategyaZSTD_c_contentSizeFlagawrite_content_sizeaZSTD_c_checksumFlagawrite_checksumaZSTD_c_dictIDFlagawrite_dict_idaZSTD_c_nbWorkersathreadsaZSTD_c_jobSizeajob_sizeaZSTD_c_overlapLogaoverlap_logaZSTD_c_forceMaxWindowaforce_max_windowaZSTD_c_enableLongDistanceMatchingaenable_ldmaZSTD_c_ldmHashLogaldm_hash_logaZSTD_c_ldmMinMatchaldm_min_matchaZSTD_c_ldmBucketSizeLogaldm_bucket_size_logaZSTD_c_ldmHashRateLogaldm_hash_rate_logutoo many values to unpack (expected 2)a_set_compression_parameteraresaZSTD_getCParamsDawindow_logachain_logahash_logasearch_logamin_matchatarget_lengthastrategyawindowLogachainLogahashLogasearchLogaminMatchatargetLengthastrategyakwargsaZstdCompressionParametersuCreate compression parameters from a compression level.

        :param level:
           Integer compression level.
        :param source_size:
           Integer size in bytes of source to be compressed.
        :param dict_size:
           Integer size in bytes of compression dictionary to use.
        :return:
           :py:class:`ZstdCompressionParameters`
        a_paramsa_cpu_countla_get_compression_parameteraZSTD_estimateCCtxSize_usingCCtxParamsuEstimated size in bytes needed to compress with these parameters.aZSTD_estimateDCtxSizeuEstimate the memory size requirements for a decompressor instance.

    :return:
       Integer number of bytes.
    aZSTD_CCtxParams_setParameteraZSTD_isErroraZstdErroruunable to set compression context parameter: %sa_zstd_erroranewTuint *aZSTD_CCtxParams_getParameteruunable to get compression context parameter: %sa_compressora_writera_write_sizea_write_return_reada_closefda_entereda_closinga_closeda_bytes_compresseduchar[]a_dst_bufferTuZSTD_outBuffer *a_out_bufferadstasizeaposaZSTD_CCtx_setPledgedSrcSizea_cctxuerror setting source size: %sustream is closedTucannot __enter__ multiple timesacloseaioaUnsupportedOperationaZSTD_sizeof_CCtxafilenoufileno not available on underlying writeraflushaFLUSH_FRAMEuwritelines() is not yet implementedafrom_bufferTuZSTD_inBuffer *asrcain_bufferaZSTD_compressStream2aselfaout_bufferaZSTD_e_continueuzstd compress error: %sawriteabuffer:nnnatotal_writeuSend data to the compressor and possibly to the inner stream.aFLUSH_BLOCKaZSTD_e_flushaZSTD_e_enduunknown flush_mode: %ruEvict data from compressor's internal state and write it to inner stream.

        Calling this method may result in 0 or more ``write()`` calls to the
        inner stream.

        This method will also call ``flush()`` on the inner stream, if such a
        method exists.

        :param flush_mode:
           How to flush the zstd compressor.

           ``zstandard.FLUSH_BLOCK`` will flush data already sent to the
           compressor but not emitted to the inner stream. The stream is still
           writable after calling this. This is the default behavior.

           See documentation for other ``zstandard.FLUSH_*`` constants for more
           flushing options.
        :return:
           Integer number of bytes written to the inner stream.
        a_finishedTucannot call compress() after compressor finishedasourcea_outachunksaappendcuSend data to the compressor.

        This method receives bytes to feed to the compressor and returns
        bytes constituting zstd compressed data.

        The zstd compressor accumulates bytes and the returned bytes may be
        substantially smaller or larger than the size of the input data on
        any given call. The returned value may be the empty byte string
        (``b""``).

        :param data:
           Data to write to the compressor.
        :return:
           Compressed data.
        aCOMPRESSOBJ_FLUSH_FINISHaCOMPRESSOBJ_FLUSH_BLOCKuflush mode not recognizedTucompressor object already finishedTuunhandled flush modeaz_flush_modeuerror ending compression stream: %suEmit data accumulated in the compressor that hasn't been outputted yet.

        The ``flush_mode`` argument controls how to end the stream.

        ``zstandard.COMPRESSOBJ_FLUSH_FINISH`` (the default) ends the
        compression stream and finishes a zstd frame. Once this type of flush
        is performed, ``compress()`` and ``flush()`` can no longer be called.
        This type of flush **must** be called to end the compression context. If
        not called, the emitted data may be incomplete and may not be readable
        by a decompressor.

        ``zstandard.COMPRESSOBJ_FLUSH_BLOCK`` will flush a zstd block. This
        ensures that all data fed to this instance will have been omitted and
        can be decoded by a decompressor. Flushes of this type can be performed
        multiple times. The next call to ``compress()`` will begin a new zstd
        block.

        :param flush_mode:
           How to flush the zstd compressor.
        :return:
           Compressed data.
        a_inuFeed new input data into the compressor.

        :param data:
           Data to feed to compressor.
        :return:
           Iterator of ``bytes`` representing chunks of compressed data.
        Tucannot call compress() after compression finishedTucannot perform operation before consuming output from previous operationadataacompressuZstdCompressionChunker.compressuFlushes all data currently in the compressor.

        :return:
           Iterator of ``bytes`` of compressed data.
        Tucannot call flush() after compression finishedTucannot call flush() before consuming output from previous operationuZstdCompressionChunker.flushuSignals the end of input data.

        No new data can be compressed after this method is called.

        This method will flush buffered data and finish the zstd frame.

        :return:
           Iterator of ``bytes`` of compressed data.
        Tucannot call finish() after compression finishedTucannot call finish() before consuming output from previous operationafinishuZstdCompressionChunker.finisha_sourcea_read_sizea_finished_inputa_finished_outputa_in_buffera_source_bufferucannot __enter__ multiple timesustream is not writableareadTlucannot read negative amounts less than -1areadalla_compress_into_buffera_read_inputaCOMPRESSION_RECOMMENDED_OUTPUT_SIZEamemmoveaZSTD_maxCLevelulevel must be less than %ducannot define compression_params and write_checksumucannot define compression_params and write_content_sizeucannot define compression_params and write_dict_iducannot define compression_params and threadsa_make_cctx_paramslaZSTD_createCCtxa_dict_dataa_setup_cctxaZSTD_freeCCtxTasizeaZSTD_CCtx_setParametersUsingCCtxParamsucould not set compression parameters: %sa_cdictaZSTD_CCtx_refCDictaZSTD_CCtx_loadDictionary_advancedaas_bytesaZSTD_dlm_byRefa_dict_typeucould not load compression dictionary: %suObtain the memory usage of this compressor, in bytes.

        >>> cctx = zstandard.ZstdCompressor()
        >>> memory = cctx.memory_size()
        aZSTD_CCtx_resetaZSTD_reset_session_onlyaZSTD_compressBoundanew_nonzeroucannot compress: %sTuunexpected partial frame flushu
        Compress data in a single operation.

        This is the simplest mechanism to perform compression: simply pass in a
        value and get a compressed value back. It is almost the most prone to
        abuse.

        The input and output values must fit in memory, so passing in very large
        values can result in excessive memory usage. For this reason, one of the
        streaming based APIs is preferred for larger values.

        :param data:
           Source data to compress
        :return:
           Compressed data

        >>> cctx = zstandard.ZstdCompressor()
        >>> compressed = cctx.compress(b"data to compress")
        aZSTD_CONTENTSIZE_UNKNOWNaZstdCompressionObju
        Obtain a compressor exposing the Python standard library compression API.

        See :py:class:`ZstdCompressionObj` for the full documentation.

        :param size:
           Size in bytes of data that will be compressed.
        :return:
           :py:class:`ZstdCompressionObj`
        aZstdCompressionChunkerTachunk_sizeu
        Create an object for iterative compressing to same-sized chunks.

        This API is similar to :py:meth:`ZstdCompressor.compressobj` but has
        better performance properties.

        :param size:
           Size in bytes of data that will be compressed.
        :param chunk_size:
           Size of compressed chunks.
        :return:
           :py:class:`ZstdCompressionChunker`
        ufirst argument must have a read() methodusecond argument must have a write() methodTlpaifharead_sizeatotal_readaofhu
        Copy data between 2 streams while compressing it.

        Data will be read from ``ifh``, compressed, and written to ``ofh``.
        ``ifh`` must have a ``read(size)`` method. ``ofh`` must have a
        ``write(data)``
        method.

        >>> cctx = zstandard.ZstdCompressor()
        >>> with open(input_path, "rb") as ifh, open(output_path, "wb") as ofh:
        ...     cctx.copy_stream(ifh, ofh)

        It is also possible to declare the size of the source stream:

        >>> cctx = zstandard.ZstdCompressor()
        >>> cctx.copy_stream(ifh, ofh, size=len_of_input)

        You can also specify how large the chunks that are ``read()``
        and ``write()`` from and to the streams:

        >>> cctx = zstandard.ZstdCompressor()
        >>> cctx.copy_stream(ifh, ofh, read_size=32768, write_size=16384)

        The stream copier returns a 2-tuple of bytes read and written:

        >>> cctx = zstandard.ZstdCompressor()
        >>> read_count, write_count = cctx.copy_stream(ifh, ofh)

        :param ifh:
           Source stream to read from
        :param ofh:
           Destination stream to write to
        :param size:
           Size in bytes of the source stream. If defined, compression
           parameters will be tuned for this size.
        :param read_size:
           Chunk sizes that source stream should be ``read()`` from.
        :param write_size:
           Chunk sizes that destination stream should be ``write()`` to.
        :return:
           2-tuple of ints of bytes read and written, respectively.
        aZstdCompressionReaderTaclosefdu
        Wrap a readable source with a stream that can read compressed data.

        This will produce an object conforming to the ``io.RawIOBase``
        interface which can be ``read()`` from to retrieve compressed data
        from a source.

        The source object can be any object with a ``read(size)`` method
        or an object that conforms to the buffer protocol.

        See :py:class:`ZstdCompressionReader` for type documentation and usage
        examples.

        :param source:
           Object to read source data from
        :param size:
           Size in bytes of source object.
        :param read_size:
           How many bytes to request when ``read()``'ing from the source.
        :param closefd:
           Whether to close the source stream when the returned stream is
           closed.
        :return:
           :py:class:`ZstdCompressionReader`
        umust pass an object with a write() methodaZstdCompressionWriteru
        Create a stream that will write compressed data into another stream.

        The argument to ``stream_writer()`` must have a ``write(data)`` method.
        As compressed data is available, ``write()`` will be called with the
        compressed data as its argument. Many common Python types implement
        ``write()``, including open file handles and ``io.BytesIO``.

        See :py:class:`ZstdCompressionWriter` for more documentation, including
        usage examples.

        :param writer:
           Stream to write compressed data to.
        :param size:
           Size in bytes of data to be compressed. If set, it will be used
           to influence compression parameter tuning and could result in the
           size being written into the header of the compressed data.
        :param write_size:
           How much data to ``write()`` to ``writer`` at a time.
        :param write_return_read:
           Whether ``write()`` should return the number of bytes that were
           consumed from the input.
        :param closefd:
           Whether to ``close`` the ``writer`` when this stream is closed.
        :return:
           :py:class:`ZstdCompressionWriter`
        u
        Read uncompressed data from a reader and return an iterator

        Returns an iterator of compressed data produced from reading from
        ``reader``.

        This method provides a mechanism to stream compressed data out of a
        source as an iterator of data chunks.

        Uncompressed data will be obtained from ``reader`` by calling the
        ``read(size)`` method of it or by reading a slice (if ``reader``
        conforms to the *buffer protocol*). The source data will be streamed
        into a compressor. As compressed data is available, it will be exposed
        to the iterator.

        Data is read from the source in chunks of ``read_size``. Compressed
        chunks are at most ``write_size`` bytes. Both values default to the
        zstd input and and output defaults, respectively.

        If reading from the source via ``read()``, ``read()`` will be called
        until it raises or returns an empty bytes (``b""``). It is perfectly
        valid for the source to deliver fewer bytes than were what requested
        by ``read(size)``.

        The caller is partially in control of how fast data is fed into the
        compressor by how it consumes the returned iterator. The compressor
        will not consume from the reader unless the caller consumes from the
        iterator.

        >>> cctx = zstandard.ZstdCompressor()
        >>> for chunk in cctx.read_to_iter(fh):
        ...     # Do something with emitted data.

        ``read_to_iter()`` accepts a ``size`` argument declaring the size of
        the input stream:

        >>> cctx = zstandard.ZstdCompressor()
        >>> for chunk in cctx.read_to_iter(fh, size=some_int):
        >>>     pass

        You can also control the size that data is ``read()`` from the source
        and the ideal size of output chunks:

        >>> cctx = zstandard.ZstdCompressor()
        >>> for chunk in cctx.read_to_iter(fh, read_size=16384, write_size=8192):
        >>>     pass

        ``read_to_iter()`` does not give direct control over the sizes of chunks
        fed into the compressor. Instead, chunk sizes will be whatever the object
        being read from delivers. These will often be of a uniform size.

        :param reader:
           Stream providing data to be compressed.
        :param size:
           Size in bytes of input data.
        :param read_size:
           Controls how many bytes are ``read()`` from the source.
        :param write_size:
           Controls the output size of emitted chunks.
        :return:
           Iterator of ``bytes``.
        areadera__getitem__umust pass an object with a read() method or conforms to buffer protocolawrite_sizeabuffer_offsetaminaread_to_iteruZstdCompressor.read_to_iteru
        Compress multiple pieces of data as a single function call.

        (Experimental. Not yet supported by CFFI backend.)

        This function is optimized to perform multiple compression operations
        as as possible with as little overhead as possible.

        Data to be compressed can be passed as a ``BufferWithSegmentsCollection``,
        a ``BufferWithSegments``, or a list containing byte like objects. Each
        element of the container will be compressed individually using the
        configured parameters on the ``ZstdCompressor`` instance.

        The ``threads`` argument controls how many threads to use for
        compression. The default is ``0`` which means to use a single thread.
        Negative values use the number of logical CPUs in the machine.

        The function returns a ``BufferWithSegmentsCollection``. This type
        represents N discrete memory allocations, each holding 1 or more
        compressed frames.

        Output data is written to shared memory buffers. This means that unlike
        regular Python objects, a reference to *any* object within the collection
        keeps the shared buffer and therefore memory backing it alive. This can
        have undesirable effects on process memory usage.

        The API and behavior of this function is experimental and will likely
        change. Known deficiencies include:

        * If asked to use multiple threads, it will always spawn that many
          threads, even if the input is too small to use them. It should
          automatically lower the thread count when the extra threads would
          just add overhead.
        * The buffer allocation strategy is fixed. There is room to make it
          dynamic, perhaps even to allow one output buffer per input,
          facilitating a variation of the API to return a list without the
          adverse effects of shared memory buffers.

        :param data:
           Source to read discrete pieces of data to compress.

           Can be a ``BufferWithSegmentsCollection``, a ``BufferWithSegments``,
           or a ``list[bytes]``.
        :return:
           BufferWithSegmentsCollection holding compressed data.
        aZSTD_getFrameProgressionaingestedaconsumedaproducedu
        Return information on how much work the compressor has done.

        Returns a 3-tuple of (ingested, consumed, produced).

        >>> cctx = zstandard.ZstdCompressor()
        >>> (ingested, consumed, produced) = cctx.frame_progression()
        aframeContentSizeacontent_sizeawindowSizeawindow_sizeadictIDadict_idachecksumFlagahas_checksumaZSTD_getFrameContentSizeaZSTD_CONTENTSIZE_ERRORTuerror when determining content sizeuObtain the decompressed size of a frame.

    The returned value is usually accurate. But strictly speaking it should
    not be trusted.

    :return:
       ``-1`` if size unknown and a non-negative integer otherwise.
    aZSTD_frameHeaderSizeucould not determine frame header size: %suObtain the size of a frame header.

    :return:
       Integer size in bytes.
    TuZSTD_frameHeader *aZSTD_getFrameHeaderucannot get frame parameters: %sunot enough data for frame parameters; need %d bytesaFrameParametersu
    Parse a zstd frame header into frame parameters.

    Depending on which fields are present in the frame and their values, the
    length of the frame parameters varies. If insufficient bytes are passed
    in to fully parse the frame parameters, ``ZstdError`` is raised. To ensure
    frame parameters can be parsed, pass in at least 18 bytes.

    :param data:
       Data from which to read frame parameters.
    :return:
       :py:class:`FrameParameters`
    a_datawkwdaDICT_TYPE_AUTOaDICT_TYPE_RAWCONTENTaDICT_TYPE_FULLDICTuinvalid dictionary load mode: %d; must use DICT_TYPE_* constantsaZDICT_getDictIDuObtain the integer ID of the dictionary.uObtain the ``bytes`` representation of the dictionary.umust only specify one of level or compression_paramsumust specify one of level or compression_paramsTaZSTD_compressionParametersachainLogahashLogaminMatchasearchLogatargetLengthawindowLogaZSTD_createCDict_advancedacparamsaZSTD_defaultCMemTuunable to precompute dictionaryaZSTD_freeCDictaZSTD_sizeof_CDictuPrecompute a dictionary os it can be used by multiple compressors.

        Calling this method on an instance that will be used by multiple
        :py:class:`ZstdCompressor` instances will improve performance.
        aZSTD_createDDict_advancedTucould not create decompression dictaZSTD_freeDDictaZSTD_sizeof_DDicta_ddictusamples must be a listlllalenusize_t[]usamples must be bytesasamples_bufferaoffsetasample_sizesTuZDICT_fastCover_params_t *wfastepsanbThreadsasplitPointaaccelazParamsanotificationLevelacompressionLevelaZDICT_optimizeTrainFromBuffer_fastCoveraaddressofaZDICT_isErroraZDICT_getErrorNameucannot train dict: %saZstdCompressionDictTadict_typewkwduTrain a dictionary from sample data using the COVER algorithm.

    A compression dictionary of size ``dict_size`` will be created from the
    iterable of ``samples``. The raw dictionary bytes will be returned.

    The dictionary training mechanism is known as *cover*. More details about it
    are available in the paper *Effective Construction of Relative Lempel-Ziv
    Dictionaries* (authors: Liao, Petri, Moffat, Wirth).

    The cover algorithm takes parameters ``k`` and ``d``. These are the
    *segment size* and *dmer size*, respectively. The returned dictionary
    instance created by this function has ``k`` and ``d`` attributes
    containing the values for these parameters. If a ``ZstdCompressionDict``
    is constructed from raw bytes data (a content-only dictionary), the
    ``k`` and ``d`` attributes will be ``0``.

    The segment and dmer size parameters to the cover algorithm can either be
    specified manually or ``train_dictionary()`` can try multiple values
    and pick the best one, where *best* means the smallest compressed data size.
    This later mode is called *optimization* mode.

    Under the hood, this function always calls
    ``ZDICT_optimizeTrainFromBuffer_fastCover()``. See the corresponding C library
    documentation for more.

    If neither ``steps`` nor ``threads`` is defined, defaults for ``d``, ``steps``,
    and ``level`` will be used that are equivalent with what
    ``ZDICT_trainFromBuffer()`` would use.


    :param dict_size:
       Target size in bytes of the dictionary to generate.
    :param samples:
       A list of bytes holding samples the dictionary will be trained from.
    :param k:
       Segment size : constraint: 0 < k : Reasonable range [16, 2048+]
    :param d:
       dmer size : constraint: 0 < d <= k : Reasonable range [6, 16]
    :param f:
       log of size of frequency array : constraint: 0 < f <= 31 : 1 means
       default(20)
    :param split_point:
       Percentage of samples used for training: Only used for optimization.
       The first # samples * ``split_point`` samples will be used to training.
       The last # samples * (1 - split_point) samples will be used for testing.
       0 means default (0.75), 1.0 when all samples are used for both training
       and testing.
    :param accel:
       Acceleration level: constraint: 0 < accel <= 10. Higher means faster
       and less accurate, 0 means default(1).
    :param dict_id:
       Integer dictionary ID for the produced dictionary. Default is 0, which uses
       a random value.
    :param steps:
       Number of steps through ``k`` values to perform when trying parameter
       variations.
    :param threads:
       Number of threads to use when trying parameter variations. Default is 0,
       which means to use a single thread. A negative value can be specified to
       use as many threads as there are detected logical CPUs.
    :param level:
       Integer target compression level when trying parameter variations.
    :param notifications:
       Controls writing of informational messages to ``stderr``. ``0`` (the
       default) means to write nothing. ``1`` writes errors. ``2`` writes
       progression info. ``3`` writes more details. And ``4`` writes all info.
    a_decompressora_read_across_framesa_unused_inputTucannot use a decompressobj multiple timesaZSTD_decompressStreama_dctxuzstd decompressor error: %suSend compressed data to the decompressor and obtain decompressed data.

        :param data:
           Data to feed into the decompressor.
        :return:
           Decompressed bytes.
        uBytes past the end of compressed data.

        If ``decompress()`` is fed additional data beyond the end of a zstd
        frame, this value will be non-empty once ``decompress()`` fully decodes
        the input frame.
        uWhether the end of the compressed data stream has been reached.a_bytes_decompresseduzstd decompress error: %suDecompress available input into an output buffer.

        Returns True if data in output buffer should be emitted.
        a_decompress_into_bufferaDECOMPRESSION_RECOMMENDED_OUTPUT_SIZEaSEEK_SETucannot seek to negative position with SEEK_SETucannot seek zstd decompression stream backwardsaSEEK_CURaSEEK_ENDuzstd decompression streams cannot be seeked with SEEK_ENDaread_amounta_ensure_dctxaZSTD_sizeof_DCtxadctxa_max_window_sizea_formataZSTD_createDCtxaZSTD_freeDCtxuSize of decompression context, in bytes.

        >>> dctx = zstandard.ZstdDecompressor()
        >>> size = dctx.memory_size()
        TuZstdDecompressor.read_across_frames=True is not yet implementedTuerror determining content size from frame headerTucould not determine content size in frame headerudecompression error: %sTudecompression error: did not decompress full frameaoutput_sizeudecompression error: decompressed %d bytes; expected %ducompressed input contains %d bytes of unused data, which is disallowedu
        Decompress data in a single operation.

        This method will decompress the input data in a single operation and
        return the decompressed data.

        The input bytes are expected to contain at least 1 full Zstandard frame
        (something compressed with :py:meth:`ZstdCompressor.compress` or
        similar). If the input does not contain a full frame, an exception will
        be raised.

        ``read_across_frames`` controls whether to read multiple zstandard
        frames in the input. When False, decompression stops after reading the
        first frame. This feature is not yet implemented but the argument is
        provided for forward API compatibility when the default is changed to
        True in a future release. For now, if you need to decompress multiple
        frames, use an API like :py:meth:`ZstdCompressor.stream_reader` with
        ``read_across_frames=True``.

        ``allow_extra_data`` controls how to handle extra input data after a
        fully decoded frame. If False, any extra data (which could be a valid
        zstd frame) will result in ``ZstdError`` being raised. If True, extra
        data is silently ignored. The default will likely change to False in a
        future release when ``read_across_frames`` defaults to True.

        If the input contains extra data after a full frame, that extra input
        data is silently ignored. This behavior is undesirable in many scenarios
        and will likely be changed or controllable in a future release (see
        #181).

        If the frame header of the compressed data does not contain the content
        size, ``max_output_size`` must be specified or ``ZstdError`` will be
        raised. An allocation of size ``max_output_size`` will be performed and an
        attempt will be made to perform decompression into that buffer. If the
        buffer is too small or cannot be allocated, ``ZstdError`` will be
        raised. The buffer will be resized if it is too large.

        Uncompressed data could be much larger than compressed data. As a result,
        calling this function could result in a very large memory allocation
        being performed to hold the uncompressed data. This could potentially
        result in ``MemoryError`` or system memory swapping. If you don't need
        the full output data in a single contiguous array in memory, consider
        using streaming decompression for more resilient memory behavior.

        Usage:

        >>> dctx = zstandard.ZstdDecompressor()
        >>> decompressed = dctx.decompress(data)

        If the compressed data doesn't have its content size embedded within it,
        decompression can be attempted by specifying the ``max_output_size``
        argument:

        >>> dctx = zstandard.ZstdDecompressor()
        >>> uncompressed = dctx.decompress(data, max_output_size=1048576)

        Ideally, ``max_output_size`` will be identical to the decompressed
        output size.

        .. important::

           If the exact size of decompressed data is unknown (not passed in
           explicitly and not stored in the zstd frame), for performance
           reasons it is encouraged to use a streaming API.

        :param data:
           Compressed data to decompress.
        :param max_output_size:
           Integer max size of response.

           If ``0``, there is no limit and we can attempt to allocate an output
           buffer of infinite size.
        :return:
           ``bytes`` representing decompressed output.
        aZstdDecompressionReaderu
        Read-only stream wrapper that performs decompression.

        This method obtains an object that conforms to the ``io.RawIOBase``
        interface and performs transparent decompression via ``read()``
        operations. Source data is obtained by calling ``read()`` on a
        source stream or object implementing the buffer protocol.

        See :py:class:`zstandard.ZstdDecompressionReader` for more documentation
        and usage examples.

        :param source:
           Source of compressed data to decompress. Can be any object
           with a ``read(size)`` method or that conforms to the buffer protocol.
        :param read_size:
           Integer number of bytes to read from the source and feed into the
           compressor at a time.
        :param read_across_frames:
           Whether to read data across multiple zstd frames. If False,
           decompression is stopped at frame boundaries.
        :param closefd:
           Whether to close the source stream when this instance is closed.
        :return:
           :py:class:`zstandard.ZstdDecompressionReader`.
        uwrite_size must be positiveaZstdDecompressionObjTawrite_sizearead_across_framesuObtain a standard library compatible incremental decompressor.

        See :py:class:`ZstdDecompressionObj` for more documentation
        and usage examples.

        :param write_size: size of internal output buffer to collect decompressed
          chunks in.
        :param read_across_frames: whether to read across multiple zstd frames.
          If False, reading stops after 1 frame and subsequent decompress
          attempts will raise an exception.
        :return:
           :py:class:`zstandard.ZstdDecompressionObj`
        uRead compressed data to an iterator of uncompressed chunks.

        This method will read data from ``reader``, feed it to a decompressor,
        and emit ``bytes`` chunks representing the decompressed result.

        >>> dctx = zstandard.ZstdDecompressor()
        >>> for chunk in dctx.read_to_iter(fh):
        ...     # Do something with original data.

        ``read_to_iter()`` accepts an object with a ``read(size)`` method that
        will return compressed bytes or an object conforming to the buffer
        protocol.

        ``read_to_iter()`` returns an iterator whose elements are chunks of the
        decompressed data.

        The size of requested ``read()`` from the source can be specified:

        >>> dctx = zstandard.ZstdDecompressor()
        >>> for chunk in dctx.read_to_iter(fh, read_size=16384):
        ...    pass

        It is also possible to skip leading bytes in the input data:

        >>> dctx = zstandard.ZstdDecompressor()
        >>> for chunk in dctx.read_to_iter(fh, skip_bytes=1):
        ...    pass

        .. tip::

           Skipping leading bytes is useful if the source data contains extra
           *header* data. Traditionally, you would need to create a slice or
           ``memoryview`` of the data you want to decompress. This would create
           overhead. It is more efficient to pass the offset into this API.

        Similarly to :py:meth:`ZstdCompressor.read_to_iter`, the consumer of the
        iterator controls when data is decompressed. If the iterator isn't consumed,
        decompression is put on hold.

        When ``read_to_iter()`` is passed an object conforming to the buffer protocol,
        the behavior may seem similar to what occurs when the simple decompression
        API is used. However, this API works when the decompressed size is unknown.
        Furthermore, if feeding large inputs, the decompressor will work in chunks
        instead of performing a single operation.

        :param reader:
           Source of compressed data. Can be any object with a
           ``read(size)`` method or any object conforming to the buffer
           protocol.
        :param read_size:
           Integer size of data chunks to read from ``reader`` and feed into
           the decompressor.
        :param write_size:
           Integer size of data chunks to emit from iterator.
        :param skip_bytes:
           Integer number of bytes to skip over before sending data into
           the decompressor.
        :return:
           Iterator of ``bytes`` representing uncompressed data.
        askip_bytesuskip_bytes must be smaller than read_sizeuskip_bytes larger than first input chunkuZstdDecompressor.read_to_iteraZstdDecompressionWriteru
        Push-based stream wrapper that performs decompression.

        This method constructs a stream wrapper that conforms to the
        ``io.RawIOBase`` interface and performs transparent decompression
        when writing to a wrapper stream.

        See :py:class:`zstandard.ZstdDecompressionWriter` for more documentation
        and usage examples.

        :param writer:
           Destination for decompressed output. Can be any object with a
           ``write(data)``.
        :param write_size:
           Integer size of chunks to ``write()`` to ``writer``.
        :param write_return_read:
           Whether ``write()`` should return the number of bytes of input
           consumed. If False, ``write()`` returns the number of bytes sent
           to the inner stream.
        :param closefd:
           Whether to ``close()`` the inner stream when this stream is closed.
        :return:
           :py:class:`zstandard.ZstdDecompressionWriter`
        u
        Copy data between streams, decompressing in the process.

        Compressed data will be read from ``ifh``, decompressed, and written
        to ``ofh``.

        >>> dctx = zstandard.ZstdDecompressor()
        >>> dctx.copy_stream(ifh, ofh)

        e.g. to decompress a file to another file:

        >>> dctx = zstandard.ZstdDecompressor()
        >>> with open(input_path, 'rb') as ifh, open(output_path, 'wb') as ofh:
        ...     dctx.copy_stream(ifh, ofh)

        The size of chunks being ``read()`` and ``write()`` from and to the
        streams can be specified:

        >>> dctx = zstandard.ZstdDecompressor()
        >>> dctx.copy_stream(ifh, ofh, read_size=8192, write_size=16384)

        :param ifh:
           Source stream to read compressed data from.

           Must have a ``read()`` method.
        :param ofh:
           Destination stream to write uncompressed data to.

           Must have a ``write()`` method.
        :param read_size:
           The number of bytes to ``read()`` from the source in a single
           operation.
        :param write_size:
           The number of bytes to ``write()`` to the destination in a single
           operation.
        :return:
           2-tuple of integers representing the number of bytes read and
           written, respectively.
        uargument must be a listuempty input chainuchunk 0 must be bytesuchunk 0 is not a valid zstd frameuchunk 0 is too small to contain a zstd frameuchunk 0 missing content size in frameTFTaload_dictucould not decompress chunk 0: %sTuchunk 0 did not decompress full framewiuchunk %d must be bytesaparamsuchunk %d is not a valid zstd frameuchunk %d is too small to contain a zstd frameuchunk %d missing content size in frameucould not decompress chunk %d: %suchunk %d did not decompress full framealast_bufferu
        Decompress a series of frames using the content dictionary chaining technique.

        Such a list of frames is produced by compressing discrete inputs where
        each non-initial input is compressed with a *prefix* dictionary consisting
        of the content of the previous input.

        For example, say you have the following inputs:

        >>> inputs = [b"input 1", b"input 2", b"input 3"]

        The zstd frame chain consists of:

        1. ``b"input 1"`` compressed in standalone/discrete mode
        2. ``b"input 2"`` compressed using ``b"input 1"`` as a *prefix* dictionary
        3. ``b"input 3"`` compressed using ``b"input 2"`` as a *prefix* dictionary

        Each zstd frame **must** have the content size written.

        The following Python code can be used to produce a *prefix dictionary chain*:

        >>> def make_chain(inputs):
        ...    frames = []
        ...
        ...    # First frame is compressed in standalone/discrete mode.
        ...    zctx = zstandard.ZstdCompressor()
        ...    frames.append(zctx.compress(inputs[0]))
        ...
        ...    # Subsequent frames use the previous fulltext as a prefix dictionary
        ...    for i, raw in enumerate(inputs[1:]):
        ...        dict_data = zstandard.ZstdCompressionDict(
        ...            inputs[i], dict_type=zstandard.DICT_TYPE_RAWCONTENT)
        ...        zctx = zstandard.ZstdCompressor(dict_data=dict_data)
        ...        frames.append(zctx.compress(raw))
        ...
        ...    return frames

        ``decompress_content_dict_chain()`` returns the uncompressed data of the last
        element in the input chain.

        .. note::

           It is possible to implement *prefix dictionary chain* decompression
           on top of other APIs. However, this function will likely be faster -
           especially for long input chains - as it avoids the overhead of
           instantiating and passing around intermediate objects between
           multiple functions.

        :param frames:
           List of ``bytes`` holding compressed zstd frames.
        :return:
        u
        Decompress multiple zstd frames to output buffers as a single operation.

        (Experimental. Not available in CFFI backend.)

        Compressed frames can be passed to the function as a
        ``BufferWithSegments``, a ``BufferWithSegmentsCollection``, or as a
        list containing objects that conform to the buffer protocol. For best
        performance, pass a ``BufferWithSegmentsCollection`` or a
        ``BufferWithSegments``, as minimal input validation will be done for
        that type. If calling from Python (as opposed to C), constructing one
        of these instances may add overhead cancelling out the performance
        overhead of validation for list inputs.

        Returns a ``BufferWithSegmentsCollection`` containing the decompressed
        data. All decompressed data is allocated in a single memory buffer. The
        ``BufferWithSegments`` instance tracks which objects are at which offsets
        and their respective lengths.

        >>> dctx = zstandard.ZstdDecompressor()
        >>> results = dctx.multi_decompress_to_buffer([b'...', b'...'])

        The decompressed size of each frame MUST be discoverable. It can either be
        embedded within the zstd frame or passed in via the ``decompressed_sizes``
        argument.

        The ``decompressed_sizes`` argument is an object conforming to the buffer
        protocol which holds an array of 64-bit unsigned integers in the machine's
        native format defining the decompressed sizes of each frame. If this argument
        is passed, it avoids having to scan each frame for its decompressed size.
        This frame scanning can add noticeable overhead in some scenarios.

        >>> frames = [...]
        >>> sizes = struct.pack('=QQQQ', len0, len1, len2, len3)
        >>>
        >>> dctx = zstandard.ZstdDecompressor()
        >>> results = dctx.multi_decompress_to_buffer(frames, decompressed_sizes=sizes)

        .. note::

           It is possible to pass a ``mmap.mmap()`` instance into this function by
           wrapping it with a ``BufferWithSegments`` instance (which will define the
           offsets of frames within the memory mapped region).

        This function is logically equivalent to performing
        :py:meth:`ZstdCompressor.decompress` on each input frame and returning the
        result.

        This function exists to perform decompression on multiple frames as fast
        as possible by having as little overhead as possible. Since decompression is
        performed as a single operation and since the decompressed output is stored in
        a single buffer, extra memory allocations, Python objects, and Python function
        calls are avoided. This is ideal for scenarios where callers know up front that
        they need to access data for multiple frames, such as when  *delta chains* are
        being used.

        Currently, the implementation always spawns multiple threads when requested,
        even if the amount of work to do is small. In the future, it will be smarter
        about avoiding threads and their associated overhead when the amount of
        work to do is small.

        :param frames:
           Source defining zstd frames to decompress.
        :param decompressed_sizes:
           Array of integers representing sizes of decompressed zstd frames.
        :param threads:
           How many threads to use for decompression operations.

           Negative values will use the same number of threads as logical CPUs
           on the machine. Values ``0`` or ``1`` use a single thread.
        :return:
           ``BufferWithSegmentsCollection``
        aZSTD_DCtx_resetaZSTD_DCtx_setMaxWindowSizeuunable to set max window size: %saZSTD_DCtx_setParameteraZSTD_d_formatuunable to set decoding format: %saZSTD_DCtx_refDDictuunable to reference prepared dictionary: %suPython interface to the Zstandard (zstd) compression library.a__doc__a__file__a__spec__aoriginahas_locationa__cached__aabsolute_importaunicode_literalsLFaBufferSegmentaBufferSegmentsaBufferWithSegmentsaBufferWithSegmentsCollectionaZstdCompressionChunkeraZstdCompressionDictaZstdCompressionObjaZstdCompressionParametersaZstdCompressionReaderaZstdCompressionWriteraZstdCompressoraZstdDecompressionObjaZstdDecompressionReaderaZstdDecompressionWriteraZstdDecompressoraZstdErroraFrameParametersabackend_featuresaestimate_decompression_context_sizeaframe_content_sizeaframe_header_sizeaget_frame_parametersatrain_dictionaryaFLUSH_BLOCKaFLUSH_FRAMEaCOMPRESSOBJ_FLUSH_FINISHaCOMPRESSOBJ_FLUSH_BLOCKaZSTD_VERSIONaFRAME_HEADERaCONTENTSIZE_UNKNOWNaCONTENTSIZE_ERRORaMAX_COMPRESSION_LEVELaCOMPRESSION_RECOMMENDED_INPUT_SIZEaCOMPRESSION_RECOMMENDED_OUTPUT_SIZEaDECOMPRESSION_RECOMMENDED_INPUT_SIZEaDECOMPRESSION_RECOMMENDED_OUTPUT_SIZEaMAGIC_NUMBERaBLOCKSIZELOG_MAXaBLOCKSIZE_MAXaWINDOWLOG_MINaWINDOWLOG_MAXaCHAINLOG_MINaCHAINLOG_MAXaHASHLOG_MINaHASHLOG_MAXaMINMATCH_MINaMINMATCH_MAXaSEARCHLOG_MINaSEARCHLOG_MAXaSEARCHLENGTH_MINaSEARCHLENGTH_MAXaTARGETLENGTH_MINaTARGETLENGTH_MAXaLDM_MINMATCH_MINaLDM_MINMATCH_MAXaLDM_BUCKETSIZELOG_MAXaSTRATEGY_FASTaSTRATEGY_DFASTaSTRATEGY_GREEDYaSTRATEGY_LAZYaSTRATEGY_LAZY2aSTRATEGY_BTLAZY2aSTRATEGY_BTOPTaSTRATEGY_BTULTRAaSTRATEGY_BTULTRA2aDICT_TYPE_AUTOaDICT_TYPE_RAWCONTENTaDICT_TYPE_FULLDICTaFORMAT_ZSTD1aFORMAT_ZSTD1_MAGICLESSa__all__a_cffiTaffialibabackend_featuresaZSTD_CStreamInSizeaCOMPRESSION_RECOMMENDED_INPUT_SIZEaZSTD_CStreamOutSizeaZSTD_DStreamInSizeaDECOMPRESSION_RECOMMENDED_INPUT_SIZEaZSTD_DStreamOutSizeanew_allocatorTashould_clear_after_allocaMAX_COMPRESSION_LEVELaZSTD_MAGICNUMBERaMAGIC_NUMBERc(/aFRAME_HEADERaCONTENTSIZE_UNKNOWNaCONTENTSIZE_ERRORaZSTD_VERSION_MAJORaZSTD_VERSION_MINORaZSTD_VERSION_RELEASEaZSTD_VERSIONaZSTD_BLOCKSIZELOG_MAXaBLOCKSIZELOG_MAXaZSTD_BLOCKSIZE_MAXaBLOCKSIZE_MAXaZSTD_WINDOWLOG_MINaWINDOWLOG_MINaZSTD_WINDOWLOG_MAXaWINDOWLOG_MAXaZSTD_CHAINLOG_MINaCHAINLOG_MINaZSTD_CHAINLOG_MAXaCHAINLOG_MAXaZSTD_HASHLOG_MINaHASHLOG_MINaZSTD_HASHLOG_MAXaHASHLOG_MAXaZSTD_MINMATCH_MINaMINMATCH_MINaZSTD_MINMATCH_MAXaMINMATCH_MAXaZSTD_SEARCHLOG_MINaSEARCHLOG_MINaZSTD_SEARCHLOG_MAXaSEARCHLOG_MAXaSEARCHLENGTH_MINaSEARCHLENGTH_MAXaZSTD_TARGETLENGTH_MINaTARGETLENGTH_MINaZSTD_TARGETLENGTH_MAXaTARGETLENGTH_MAXaZSTD_LDM_MINMATCH_MINaLDM_MINMATCH_MINaZSTD_LDM_MINMATCH_MAXaLDM_MINMATCH_MAXaZSTD_LDM_BUCKETSIZELOG_MAXaLDM_BUCKETSIZELOG_MAXaZSTD_fastaSTRATEGY_FASTaZSTD_dfastaSTRATEGY_DFASTaZSTD_greedyaSTRATEGY_GREEDYaZSTD_lazyaSTRATEGY_LAZYaZSTD_lazy2aSTRATEGY_LAZY2aZSTD_btlazy2aSTRATEGY_BTLAZY2aZSTD_btoptaSTRATEGY_BTOPTaZSTD_btultraaSTRATEGY_BTULTRAaZSTD_btultra2aSTRATEGY_BTULTRA2aZSTD_dct_autoaZSTD_dct_rawContentaZSTD_dct_fullDictaZSTD_f_zstd1aFORMAT_ZSTD1aZSTD_f_zstd1_magiclessaFORMAT_ZSTD1_MAGICLESSuzstandard.backend_cffia__module__uRepresents a segment within a ``BufferWithSegments``.

    This type is essentially a reference to N bytes within a
    ``BufferWithSegments``.

    The object conforms to the buffer protocol.
    aBufferSegmenta__qualname__uBufferSegment.offseta__len__uBufferSegment.__len__atobytesuBufferSegment.tobytesTuRepresents an array of ``(offset, length)`` integers.

    This type is effectively an index used by :py:class:`BufferWithSegments`.

    The array members are 64-bit unsigned integers using host/native bit order.

    Instances conform to the buffer protocol.
    aBufferSegmentsuA memory buffer containing N discrete items of known lengths.

    This type is essentially a fixed size memory address and an array
    of 2-tuples of ``(offset, length)`` 64-bit unsigned native-endian
    integers defining the byte offset and length of each segment within
    the buffer.

    Instances behave like containers.

    Instances also conform to the buffer protocol. So a reference to the
    backing bytes can be obtained via ``memoryview(o)``. A *copy* of the
    backing bytes can be obtained via ``.tobytes()``.

    This type exists to facilitate operations against N>1 items without
    the overhead of Python object creation and management. Used with
    APIs like :py:meth:`ZstdDecompressor.multi_decompress_to_buffer`, it
    is possible to decompress many objects in parallel without the GIL
    held, leading to even better performance.
    aBufferWithSegmentsuBufferWithSegments.sizeuBufferWithSegments.__len__uBufferWithSegments.__getitem__asegmentsuBufferWithSegments.segmentsuBufferWithSegments.tobytesuA virtual spanning view over multiple BufferWithSegments.

    Instances are constructed from 1 or more :py:class:`BufferWithSegments`
    instances. The resulting object behaves like an ordered sequence whose
    members are the segments within each ``BufferWithSegments``.

    If the object is composed of 2 ``BufferWithSegments`` instances with the
    first having 2 segments and the second have 3 segments, then ``b[0]``
    and ``b[1]`` access segments in the first object and ``b[2]``, ``b[3]``,
    and ``b[4]`` access segments from the second.
    aBufferWithSegmentsCollectionuBufferWithSegmentsCollection.__len__uBufferWithSegmentsCollection.__getitem__TEExceptiona__prepare__u%s.__prepare__() must return a mapping, not %sa__name__u<metaclass>a__orig_bases__TOobjectuLow-level zstd compression parameters.

    This type represents a collection of parameters to control how zstd
    compression is performed.

    Instances can be constructed from raw parameters or derived from a
    base set of defaults specified from a compression level (recommended)
    via :py:meth:`ZstdCompressionParameters.from_level`.

    >>> # Derive compression settings for compression level 7.
    >>> params = zstandard.ZstdCompressionParameters.from_level(7)

    >>> # With an input size of 1MB
    >>> params = zstandard.ZstdCompressionParameters.from_level(7, source_size=1048576)

    Using ``from_level()``, it is also possible to override individual compression
    parameters or to define additional settings that aren't automatically derived.
    e.g.:

    >>> params = zstandard.ZstdCompressionParameters.from_level(4, window_log=10)
    >>> params = zstandard.ZstdCompressionParameters.from_level(5, threads=4)

    Or you can define low-level compression settings directly:

    >>> params = zstandard.ZstdCompressionParameters(window_log=12, enable_ldm=True)

    Once a ``ZstdCompressionParameters`` instance is obtained, it can be used to
    configure a compressor:

    >>> cctx = zstandard.ZstdCompressor(compression_params=params)

    Some of these are very low-level settings. It may help to consult the official
    zstandard documentation for their behavior. Look for the ``ZSTD_p_*`` constants
    in ``zstd.h`` (https://github.com/facebook/zstd/blob/dev/lib/zstd.h).
    astaticmethodafrom_leveluZstdCompressionParameters.from_levelTlppppppplllppllpppplla__init__uZstdCompressionParameters.__init__apropertyuZstdCompressionParameters.formatuZstdCompressionParameters.compression_leveluZstdCompressionParameters.window_loguZstdCompressionParameters.hash_loguZstdCompressionParameters.chain_loguZstdCompressionParameters.search_loguZstdCompressionParameters.min_matchuZstdCompressionParameters.target_lengthuZstdCompressionParameters.strategyuZstdCompressionParameters.write_content_sizeuZstdCompressionParameters.write_checksumuZstdCompressionParameters.write_dict_iduZstdCompressionParameters.job_sizeuZstdCompressionParameters.overlap_loguZstdCompressionParameters.force_max_windowuZstdCompressionParameters.enable_ldmuZstdCompressionParameters.ldm_hash_loguZstdCompressionParameters.ldm_min_matchuZstdCompressionParameters.ldm_bucket_size_loguZstdCompressionParameters.ldm_hash_rate_loguZstdCompressionParameters.threadsaestimated_compression_context_sizeuZstdCompressionParameters.estimated_compression_context_sizeaestimate_decompression_context_sizeuWritable compressing stream wrapper.

    ``ZstdCompressionWriter`` is a write-only stream interface for writing
    compressed data to another stream.

    This type conforms to the ``io.RawIOBase`` interface and should be usable
    by any type that operates against a *file-object* (``typing.BinaryIO``
    in Python type hinting speak). Only methods that involve writing will do
    useful things.

    As data is written to this stream (e.g. via ``write()``), that data
    is sent to the compressor. As compressed data becomes available from
    the compressor, it is sent to the underlying stream by calling its
    ``write()`` method.

    Both ``write()`` and ``flush()`` return the number of bytes written to the
    object's ``write()``. In many cases, small inputs do not accumulate enough
    data to cause a write and ``write()`` will return ``0``.

    Calling ``close()`` will mark the stream as closed and subsequent I/O
    operations will raise ``ValueError`` (per the documented behavior of
    ``io.RawIOBase``). ``close()`` will also call ``close()`` on the underlying
    stream if such a method exists and the instance was constructed with
    ``closefd=True``

    Instances are obtained by calling :py:meth:`ZstdCompressor.stream_writer`.

    Typically usage is as follows:

    >>> cctx = zstandard.ZstdCompressor(level=10)
    >>> compressor = cctx.stream_writer(fh)
    >>> compressor.write(b"chunk 0\n")
    >>> compressor.write(b"chunk 1\n")
    >>> compressor.flush()
    >>> # Receiver will be able to decode ``chunk 0\nchunk 1\n`` at this point.
    >>> # Receiver is also expecting more data in the zstd *frame*.
    >>>
    >>> compressor.write(b"chunk 2\n")
    >>> compressor.flush(zstandard.FLUSH_FRAME)
    >>> # Receiver will be able to decode ``chunk 0\nchunk 1\nchunk 2``.
    >>> # Receiver is expecting no more data, as the zstd frame is closed.
    >>> # Any future calls to ``write()`` at this point will construct a new
    >>> # zstd frame.

    Instances can be used as context managers. Exiting the context manager is
    the equivalent of calling ``close()``, which is equivalent to calling
    ``flush(zstandard.FLUSH_FRAME)``:

    >>> cctx = zstandard.ZstdCompressor(level=10)
    >>> with cctx.stream_writer(fh) as compressor:
    ...     compressor.write(b'chunk 0')
    ...     compressor.write(b'chunk 1')
    ...     ...

    .. important::

       If ``flush(FLUSH_FRAME)`` is not called, emitted data doesn't
       constitute a full zstd *frame* and consumers of this data may complain
       about malformed input. It is recommended to use instances as a context
       manager to ensure *frames* are properly finished.

    If the size of the data being fed to this streaming compressor is known,
    you can declare it before compression begins:

    >>> cctx = zstandard.ZstdCompressor()
    >>> with cctx.stream_writer(fh, size=data_len) as compressor:
    ...     compressor.write(chunk0)
    ...     compressor.write(chunk1)
    ...     ...

    Declaring the size of the source data allows compression parameters to
    be tuned. And if ``write_content_size`` is used, it also results in the
    content size being written into the frame header of the output data.

    The size of chunks being ``write()`` to the destination can be specified:

    >>> cctx = zstandard.ZstdCompressor()
    >>> with cctx.stream_writer(fh, write_size=32768) as compressor:
    ...     ...

    To see how much memory is being used by the streaming compressor:

    >>> cctx = zstandard.ZstdCompressor()
    >>> with cctx.stream_writer(fh) as compressor:
    ...     ...
    ...     byte_size = compressor.memory_size()

    Thte total number of bytes written so far are exposed via ``tell()``:

    >>> cctx = zstandard.ZstdCompressor()
    >>> with cctx.stream_writer(fh) as compressor:
    ...     ...
    ...     total_written = compressor.tell()

    ``stream_writer()`` accepts a ``write_return_read`` boolean argument to
    control the return value of ``write()``. When ``False`` (the default),
    ``write()`` returns the number of bytes that were ``write()``'en to the
    underlying object. When ``True``, ``write()`` returns the number of bytes
    read from the input that were subsequently written to the compressor.
    ``True`` is the *proper* behavior for ``write()`` as specified by the
    ``io.RawIOBase`` interface and will become the default value in a future
    release.
    TtuZstdCompressionWriter.__init__a__enter__uZstdCompressionWriter.__enter__a__exit__uZstdCompressionWriter.__exit__a__iter__uZstdCompressionWriter.__iter__a__next__uZstdCompressionWriter.__next__amemory_sizeuZstdCompressionWriter.memory_sizeuZstdCompressionWriter.filenouZstdCompressionWriter.closeacloseduZstdCompressionWriter.closedaisattyuZstdCompressionWriter.isattyareadableuZstdCompressionWriter.readableTlareadlineuZstdCompressionWriter.readlineareadlinesuZstdCompressionWriter.readlinesTnaseekuZstdCompressionWriter.seekaseekableuZstdCompressionWriter.seekableatruncateuZstdCompressionWriter.truncateawritableuZstdCompressionWriter.writableawritelinesuZstdCompressionWriter.writelinesuZstdCompressionWriter.readuZstdCompressionWriter.readallareadintouZstdCompressionWriter.readintouZstdCompressionWriter.writeuZstdCompressionWriter.flushatelluZstdCompressionWriter.telluA compressor conforming to the API in Python's standard library.

    This type implements an API similar to compression types in Python's
    standard library such as ``zlib.compressobj`` and ``bz2.BZ2Compressor``.
    This enables existing code targeting the standard library API to swap
    in this type to achieve zstd compression.

    .. important::

       The design of this API is not ideal for optimal performance.

       The reason performance is not optimal is because the API is limited to
       returning a single buffer holding compressed data. When compressing
       data, we don't know how much data will be emitted. So in order to
       capture all this data in a single buffer, we need to perform buffer
       reallocations and/or extra memory copies. This can add significant
       overhead depending on the size or nature of the compressed data how
       much your application calls this type.

       If performance is critical, consider an API like
       :py:meth:`ZstdCompressor.stream_reader`,
       :py:meth:`ZstdCompressor.stream_writer`,
       :py:meth:`ZstdCompressor.chunker`, or
       :py:meth:`ZstdCompressor.read_to_iter`, which result in less overhead
       managing buffers.

    Instances are obtained by calling :py:meth:`ZstdCompressor.compressobj`.

    Here is how this API should be used:

    >>> cctx = zstandard.ZstdCompressor()
    >>> cobj = cctx.compressobj()
    >>> data = cobj.compress(b"raw input 0")
    >>> data = cobj.compress(b"raw input 1")
    >>> data = cobj.flush()

    Or to flush blocks:

    >>> cctx.zstandard.ZstdCompressor()
    >>> cobj = cctx.compressobj()
    >>> data = cobj.compress(b"chunk in first block")
    >>> data = cobj.flush(zstandard.COMPRESSOBJ_FLUSH_BLOCK)
    >>> data = cobj.compress(b"chunk in second block")
    >>> data = cobj.flush()

    For best performance results, keep input chunks under 256KB. This avoids
    extra allocations for a large output object.

    It is possible to declare the input size of the data that will be fed
    into the compressor:

    >>> cctx = zstandard.ZstdCompressor()
    >>> cobj = cctx.compressobj(size=6)
    >>> data = cobj.compress(b"foobar")
    >>> data = cobj.flush()
    uZstdCompressionObj.compressuZstdCompressionObj.flushuCompress data to uniformly sized chunks.

    This type allows you to iteratively feed chunks of data into a compressor
    and produce output chunks of uniform size.

    ``compress()``, ``flush()``, and ``finish()`` all return an iterator of
    ``bytes`` instances holding compressed data. The iterator may be empty.
    Callers MUST iterate through all elements of the returned iterator before
    performing another operation on the object or else the compressor's
    internal state may become confused. This can result in an exception being
    raised or malformed data being emitted.

    All chunks emitted by ``compress()`` will have a length of the configured
    chunk size.

    ``flush()`` and ``finish()`` may return a final chunk smaller than
    the configured chunk size.

    Instances are obtained by calling :py:meth:`ZstdCompressor.chunker`.

    Here is how the API should be used:

    >>> cctx = zstandard.ZstdCompressor()
    >>> chunker = cctx.chunker(chunk_size=32768)
    >>>
    >>> with open(path, 'rb') as fh:
    ...     while True:
    ...         in_chunk = fh.read(32768)
    ...         if not in_chunk:
    ...             break
    ...
    ...         for out_chunk in chunker.compress(in_chunk):
    ...             # Do something with output chunk of size 32768.
    ...
    ...     for out_chunk in chunker.finish():
    ...         # Do something with output chunks that finalize the zstd frame.

    This compressor type is often a better alternative to
    :py:class:`ZstdCompressor.compressobj` because it has better performance
    properties.

    ``compressobj()`` will emit output data as it is available. This results
    in a *stream* of output chunks of varying sizes. The consistency of the
    output chunk size with ``chunker()`` is more appropriate for many usages,
    such as sending compressed data to a socket.

    ``compressobj()`` may also perform extra memory reallocations in order
    to dynamically adjust the sizes of the output chunks. Since ``chunker()``
    output chunks are all the same size (except for flushed or final chunks),
    there is less memory allocation/copying overhead.
    uZstdCompressionChunker.__init__uReadable compressing stream wrapper.

    ``ZstdCompressionReader`` is a read-only stream interface for obtaining
    compressed data from a source.

    This type conforms to the ``io.RawIOBase`` interface and should be usable
    by any type that operates against a *file-object* (``typing.BinaryIO``
    in Python type hinting speak).

    Instances are neither writable nor seekable (even if the underlying
    source is seekable). ``readline()`` and ``readlines()`` are not implemented
    because they don't make sense for compressed data. ``tell()`` returns the
    number of compressed bytes emitted so far.

    Instances are obtained by calling :py:meth:`ZstdCompressor.stream_reader`.

    In this example, we open a file for reading and then wrap that file
    handle with a stream from which compressed data can be ``read()``.

    >>> with open(path, 'rb') as fh:
    ...     cctx = zstandard.ZstdCompressor()
    ...     reader = cctx.stream_reader(fh)
    ...     while True:
    ...         chunk = reader.read(16384)
    ...         if not chunk:
    ...             break
    ...
    ...         # Do something with compressed chunk.

    Instances can also be used as context managers:

    >>> with open(path, 'rb') as fh:
    ...     cctx = zstandard.ZstdCompressor()
    ...     with cctx.stream_reader(fh) as reader:
    ...         while True:
    ...             chunk = reader.read(16384)
    ...             if not chunk:
    ...                 break
    ...
    ...             # Do something with compressed chunk.

    When the context manager exits or ``close()`` is called, the stream is
    closed, underlying resources are released, and future operations against
    the compression stream will fail.

    ``stream_reader()`` accepts a ``size`` argument specifying how large the
    input stream is. This is used to adjust compression parameters so they are
    tailored to the source size. e.g.

    >>> with open(path, 'rb') as fh:
    ...     cctx = zstandard.ZstdCompressor()
    ...     with cctx.stream_reader(fh, size=os.stat(path).st_size) as reader:
    ...         ...

    If the ``source`` is a stream, you can specify how large ``read()``
    requests to that stream should be via the ``read_size`` argument.
    It defaults to ``zstandard.COMPRESSION_RECOMMENDED_INPUT_SIZE``. e.g.

    >>> with open(path, 'rb') as fh:
    ...     cctx = zstandard.ZstdCompressor()
    ...     # Will perform fh.read(8192) when obtaining data to feed into the
    ...     # compressor.
    ...     with cctx.stream_reader(fh, read_size=8192) as reader:
    ...         ...
    uZstdCompressionReader.__init__uZstdCompressionReader.__enter__uZstdCompressionReader.__exit__uZstdCompressionReader.readableuZstdCompressionReader.writableuZstdCompressionReader.seekableuZstdCompressionReader.readlineuZstdCompressionReader.readlinesuZstdCompressionReader.writeuZstdCompressionReader.writelinesuZstdCompressionReader.isattyuZstdCompressionReader.flushuZstdCompressionReader.closeuZstdCompressionReader.closeduZstdCompressionReader.telluZstdCompressionReader.readalluZstdCompressionReader.__iter__uZstdCompressionReader.__next__anextuZstdCompressionReader._read_inputuZstdCompressionReader._compress_into_bufferuZstdCompressionReader.readaread1uZstdCompressionReader.read1uZstdCompressionReader.readintoareadinto1uZstdCompressionReader.readinto1aZstdCompressoru
    Create an object used to perform Zstandard compression.

    Each instance is essentially a wrapper around a ``ZSTD_CCtx`` from
    zstd's C API.

    An instance can compress data various ways. Instances can be used
    multiple times. Each compression operation will use the compression
    parameters defined at construction time.

    .. note:

       When using a compression dictionary and multiple compression
       operations are performed, the ``ZstdCompressionParameters`` derived
       from an integer compression ``level`` and the first compressed data's
       size will be reused for all subsequent operations. This may not be
       desirable if source data sizes vary significantly.

    ``compression_params`` is mutually exclusive with ``level``,
    ``write_checksum``, ``write_content_size``, ``write_dict_id``, and
    ``threads``.

    Assume that each ``ZstdCompressor`` instance can only handle a single
    logical compression operation at the same time. i.e. if you call a method
    like ``stream_reader()`` to obtain multiple objects derived from the same
    ``ZstdCompressor`` instance and attempt to use them simultaneously, errors
    will likely occur.

    If you need to perform multiple logical compression operations and you
    can't guarantee those operations are temporally non-overlapping, you need
    to obtain multiple ``ZstdCompressor`` instances.

    Unless specified otherwise, assume that no two methods of
    ``ZstdCompressor`` instances can be called from multiple Python
    threads simultaneously. In other words, assume instances are not thread safe
    unless stated otherwise.

    :param level:
       Integer compression level. Valid values are all negative integers
       through 22. Lower values generally yield faster operations with lower
       compression ratios. Higher values are generally slower but compress
       better. The default is 3, which is what the ``zstd`` CLI uses. Negative
       levels effectively engage ``--fast`` mode from the ``zstd`` CLI.
    :param dict_data:
       A ``ZstdCompressionDict`` to be used to compress with dictionary
        data.
    :param compression_params:
       A ``ZstdCompressionParameters`` instance defining low-level compression
       parameters. If defined, this will overwrite the ``level`` argument.
    :param write_checksum:
       If True, a 4 byte content checksum will be written with the compressed
       data, allowing the decompressor to perform content verification.
    :param write_content_size:
       If True (the default), the decompressed content size will be included
       in the header of the compressed data. This data will only be written if
       the compressor knows the size of the input data.
    :param write_dict_id:
       Determines whether the dictionary ID will be written into the compressed
       data. Defaults to True. Only adds content to the compressed data if
       a dictionary is being used.
    :param threads:
       Number of threads to use to compress data concurrently. When set,
       compression operations are performed on multiple threads. The default
       value (0) disables multi-threaded compression. A value of ``-1`` means
       to set the number of threads to the number of detected logical CPUs.
    TlnnnnnluZstdCompressor.__init__uZstdCompressor._setup_cctxuZstdCompressor.memory_sizeuZstdCompressor.compressacompressobjuZstdCompressor.compressobjachunkeruZstdCompressor.chunkeracopy_streamuZstdCompressor.copy_streamastream_readeruZstdCompressor.stream_readerastream_writeruZstdCompressor.stream_writeramulti_compress_to_bufferuZstdCompressor.multi_compress_to_bufferaframe_progressionuZstdCompressor.frame_progressionuInformation about a zstd frame.

    Instances have the following attributes:

    ``content_size``
       Integer size of original, uncompressed content. This will be ``0`` if the
       original content size isn't written to the frame (controlled with the
       ``write_content_size`` argument to ``ZstdCompressor``) or if the input
       content size was ``0``.

    ``window_size``
       Integer size of maximum back-reference distance in compressed data.

    ``dict_id``
       Integer of dictionary ID used for compression. ``0`` if no dictionary
       ID was used or if the dictionary ID was ``0``.

    ``has_checksum``
       Bool indicating whether a 4 byte content checksum is stored at the end
       of the frame.
    uFrameParameters.__init__aframe_content_sizeaframe_header_sizeaget_frame_parametersuRepresents a computed compression dictionary.

    Instances are obtained by calling :py:func:`train_dictionary` or by
    passing bytes obtained from another source into the constructor.

    Instances can be constructed from bytes:

    >>> dict_data = zstandard.ZstdCompressionDict(data)

    It is possible to construct a dictionary from *any* data. If the data
    doesn't begin with a magic header, it will be treated as a *prefix*
    dictionary. *Prefix* dictionaries allow compression operations to
    reference raw data within the dictionary.

    It is possible to force the use of *prefix* dictionaries or to require
    a dictionary header:

    >>> dict_data = zstandard.ZstdCompressionDict(data, dict_type=zstandard.DICT_TYPE_RAWCONTENT)
    >>> dict_data = zstandard.ZstdCompressionDict(data, dict_type=zstandard.DICT_TYPE_FULLDICT)

    You can see how many bytes are in the dictionary by calling ``len()``:

    >>> dict_data = zstandard.train_dictionary(size, samples)
    >>> dict_size = len(dict_data)  # will not be larger than ``size``

    Once you have a dictionary, you can pass it to the objects performing
    compression and decompression:

    >>> dict_data = zstandard.train_dictionary(131072, samples)
    >>> cctx = zstandard.ZstdCompressor(dict_data=dict_data)
    >>> for source_data in input_data:
    ...     compressed = cctx.compress(source_data)
    ...     # Do something with compressed data.
    ...
    >>> dctx = zstandard.ZstdDecompressor(dict_data=dict_data)
    >>> for compressed_data in input_data:
    ...     buffer = io.BytesIO()
    ...     with dctx.stream_writer(buffer) as decompressor:
    ...         decompressor.write(compressed_data)
    ...         # Do something with raw data in ``buffer``.

    Dictionaries have unique integer IDs. You can retrieve this ID via:

    >>> dict_id = zstandard.dictionary_id(dict_data)

    You can obtain the raw data in the dict (useful for persisting and constructing
    a ``ZstdCompressionDict`` later) via ``as_bytes()``:

    >>> dict_data = zstandard.train_dictionary(size, samples)
    >>> raw_data = dict_data.as_bytes()

    By default, when a ``ZstdCompressionDict`` is *attached* to a
    ``ZstdCompressor``, each ``ZstdCompressor`` performs work to prepare the
    dictionary for use. This is fine if only 1 compression operation is being
    performed or if the ``ZstdCompressor`` is being reused for multiple operations.
    But if multiple ``ZstdCompressor`` instances are being used with the dictionary,
    this can add overhead.

    It is possible to *precompute* the dictionary so it can readily be consumed
    by multiple ``ZstdCompressor`` instances:

    >>> d = zstandard.ZstdCompressionDict(data)
    >>> # Precompute for compression level 3.
    >>> d.precompute_compress(level=3)
    >>> # Precompute with specific compression parameters.
    >>> params = zstandard.ZstdCompressionParameters(...)
    >>> d.precompute_compress(compression_params=params)

    .. note::

       When a dictionary is precomputed, the compression parameters used to
       precompute the dictionary overwrite some of the compression parameters
       specified to ``ZstdCompressor``.

    :param data:
       Dictionary data.
    :param dict_type:
       Type of dictionary. One of the ``DICT_TYPE_*`` constants.
    uZstdCompressionDict.__init__uZstdCompressionDict.__len__uZstdCompressionDict.dict_iduZstdCompressionDict.as_bytesTlnaprecompute_compressuZstdCompressionDict.precompute_compressuZstdCompressionDict._ddictT
lppZlpppppatrain_dictionaryuA standard library API compatible decompressor.

    This type implements a compressor that conforms to the API by other
    decompressors in Python's standard library. e.g. ``zlib.decompressobj``
    or ``bz2.BZ2Decompressor``. This allows callers to use zstd compression
    while conforming to a similar API.

    Compressed data chunks are fed into ``decompress(data)`` and
    uncompressed output (or an empty bytes) is returned. Output from
    subsequent calls needs to be concatenated to reassemble the full
    decompressed byte sequence.

    If ``read_across_frames=False``, each instance is single use: once an
    input frame is decoded, ``decompress()`` will raise an exception. If
    ``read_across_frames=True``, instances can decode multiple frames.

    >>> dctx = zstandard.ZstdDecompressor()
    >>> dobj = dctx.decompressobj()
    >>> data = dobj.decompress(compressed_chunk_0)
    >>> data = dobj.decompress(compressed_chunk_1)

    By default, calls to ``decompress()`` write output data in chunks of size
    ``DECOMPRESSION_RECOMMENDED_OUTPUT_SIZE``. These chunks are concatenated
    before being returned to the caller. It is possible to define the size of
    these temporary chunks by passing ``write_size`` to ``decompressobj()``:

    >>> dctx = zstandard.ZstdDecompressor()
    >>> dobj = dctx.decompressobj(write_size=1048576)

    .. note::

       Because calls to ``decompress()`` may need to perform multiple
       memory (re)allocations, this streaming decompression API isn't as
       efficient as other APIs.
    uZstdDecompressionObj.__init__adecompressuZstdDecompressionObj.decompressTluEffectively a no-op.

        Implemented for compatibility with the standard library APIs.

        Safe to call at any time.

        :return:
           Empty bytes.
        uZstdDecompressionObj.flushaunused_datauZstdDecompressionObj.unused_datauData that has not yet been fed into the decompressor.aunconsumed_tailuZstdDecompressionObj.unconsumed_tailaeofuZstdDecompressionObj.eofuRead only decompressor that pull uncompressed data from another stream.

    This type provides a read-only stream interface for performing transparent
    decompression from another stream or data source. It conforms to the
    ``io.RawIOBase`` interface. Only methods relevant to reading are
    implemented.

    >>> with open(path, 'rb') as fh:
    >>> dctx = zstandard.ZstdDecompressor()
    >>> reader = dctx.stream_reader(fh)
    >>> while True:
    ...     chunk = reader.read(16384)
    ...     if not chunk:
    ...         break
    ...     # Do something with decompressed chunk.

    The stream can also be used as a context manager:

    >>> with open(path, 'rb') as fh:
    ...     dctx = zstandard.ZstdDecompressor()
    ...     with dctx.stream_reader(fh) as reader:
    ...         ...

    When used as a context manager, the stream is closed and the underlying
    resources are released when the context manager exits. Future operations
    against the stream will fail.

    The ``source`` argument to ``stream_reader()`` can be any object with a
    ``read(size)`` method or any object implementing the *buffer protocol*.

    If the ``source`` is a stream, you can specify how large ``read()`` requests
    to that stream should be via the ``read_size`` argument. It defaults to
    ``zstandard.DECOMPRESSION_RECOMMENDED_INPUT_SIZE``.:

    >>> with open(path, 'rb') as fh:
    ...     dctx = zstandard.ZstdDecompressor()
    ...     # Will perform fh.read(8192) when obtaining data for the decompressor.
    ...     with dctx.stream_reader(fh, read_size=8192) as reader:
    ...         ...

    Instances are *partially* seekable. Absolute and relative positions
    (``SEEK_SET`` and ``SEEK_CUR``) forward of the current position are
    allowed. Offsets behind the current read position and offsets relative
    to the end of stream are not allowed and will raise ``ValueError``
    if attempted.

    ``tell()`` returns the number of decompressed bytes read so far.

    Not all I/O methods are implemented. Notably missing is support for
    ``readline()``, ``readlines()``, and linewise iteration support. This is
    because streams operate on binary data - not text data. If you want to
    convert decompressed output to text, you can chain an ``io.TextIOWrapper``
    to the stream:

    >>> with open(path, 'rb') as fh:
    ...     dctx = zstandard.ZstdDecompressor()
    ...     stream_reader = dctx.stream_reader(fh)
    ...     text_stream = io.TextIOWrapper(stream_reader, encoding='utf-8')
    ...     for line in text_stream:
    ...         ...
    uZstdDecompressionReader.__init__uZstdDecompressionReader.__enter__uZstdDecompressionReader.__exit__uZstdDecompressionReader.readableuZstdDecompressionReader.writableuZstdDecompressionReader.seekableuZstdDecompressionReader.readlineuZstdDecompressionReader.readlinesuZstdDecompressionReader.writeuZstdDecompressionReader.writelinesuZstdDecompressionReader.isattyuZstdDecompressionReader.flushuZstdDecompressionReader.closeuZstdDecompressionReader.closeduZstdDecompressionReader.telluZstdDecompressionReader.readalluZstdDecompressionReader.__iter__uZstdDecompressionReader.__next__uZstdDecompressionReader._read_inputuZstdDecompressionReader._decompress_into_bufferuZstdDecompressionReader.readuZstdDecompressionReader.readintouZstdDecompressionReader.read1uZstdDecompressionReader.readinto1uZstdDecompressionReader.seeku
    Write-only stream wrapper that performs decompression.

    This type provides a writable stream that performs decompression and writes
    decompressed data to another stream.

    This type implements the ``io.RawIOBase`` interface. Only methods that
    involve writing will do useful things.

    Behavior is similar to :py:meth:`ZstdCompressor.stream_writer`: compressed
    data is sent to the decompressor by calling ``write(data)`` and decompressed
    output is written to the inner stream by calling its ``write(data)``
    method:

    >>> dctx = zstandard.ZstdDecompressor()
    >>> decompressor = dctx.stream_writer(fh)
    >>> # Will call fh.write() with uncompressed data.
    >>> decompressor.write(compressed_data)

    Instances can be used as context managers. However, context managers add no
    extra special behavior other than automatically calling ``close()`` when
    they exit.

    Calling ``close()`` will mark the stream as closed and subsequent I/O
    operations will raise ``ValueError`` (per the documented behavior of
    ``io.RawIOBase``). ``close()`` will also call ``close()`` on the
    underlying stream if such a method exists and the instance was created with
    ``closefd=True``.

    The size of chunks to ``write()`` to the destination can be specified:

    >>> dctx = zstandard.ZstdDecompressor()
    >>> with dctx.stream_writer(fh, write_size=16384) as decompressor:
    >>>    pass

    You can see how much memory is being used by the decompressor:

    >>> dctx = zstandard.ZstdDecompressor()
    >>> with dctx.stream_writer(fh) as decompressor:
    >>>    byte_size = decompressor.memory_size()

    ``stream_writer()`` accepts a ``write_return_read`` boolean argument to control
    the return value of ``write()``. When ``True`` (the default)``, ``write()``
    returns the number of bytes that were read from the input. When ``False``,
    ``write()`` returns the number of bytes that were ``write()`` to the inner
    stream.
    uZstdDecompressionWriter.__init__uZstdDecompressionWriter.__enter__uZstdDecompressionWriter.__exit__uZstdDecompressionWriter.__iter__uZstdDecompressionWriter.__next__uZstdDecompressionWriter.memory_sizeuZstdDecompressionWriter.closeuZstdDecompressionWriter.closeduZstdDecompressionWriter.filenouZstdDecompressionWriter.flushuZstdDecompressionWriter.isattyuZstdDecompressionWriter.readableuZstdDecompressionWriter.readlineuZstdDecompressionWriter.readlinesuZstdDecompressionWriter.seekuZstdDecompressionWriter.seekableuZstdDecompressionWriter.telluZstdDecompressionWriter.truncateuZstdDecompressionWriter.writableuZstdDecompressionWriter.writelinesuZstdDecompressionWriter.readuZstdDecompressionWriter.readalluZstdDecompressionWriter.readintouZstdDecompressionWriter.writeaZstdDecompressoru
    Context for performing zstandard decompression.

    Each instance is essentially a wrapper around a ``ZSTD_DCtx`` from zstd's
    C API.

    An instance can compress data various ways. Instances can be used multiple
    times.

    The interface of this class is very similar to
    :py:class:`zstandard.ZstdCompressor` (by design).

    Assume that each ``ZstdDecompressor`` instance can only handle a single
    logical compression operation at the same time. i.e. if you call a method
    like ``decompressobj()`` to obtain multiple objects derived from the same
    ``ZstdDecompressor`` instance and attempt to use them simultaneously, errors
    will likely occur.

    If you need to perform multiple logical decompression operations and you
    can't guarantee those operations are temporally non-overlapping, you need
    to obtain multiple ``ZstdDecompressor`` instances.

    Unless specified otherwise, assume that no two methods of
    ``ZstdDecompressor`` instances can be called from multiple Python
    threads simultaneously. In other words, assume instances are not thread safe
    unless stated otherwise.

    :param dict_data:
       Compression dictionary to use.
    :param max_window_size:
       Sets an upper limit on the window size for decompression operations in
       kibibytes. This setting can be used to prevent large memory allocations
       for inputs using large compression windows.
    :param format:
       Set the format of data for the decoder.

       By default this is ``zstandard.FORMAT_ZSTD1``. It can be set to
       ``zstandard.FORMAT_ZSTD1_MAGICLESS`` to allow decoding frames without
       the 4 byte magic header. Not all decompression APIs support this mode.
    uZstdDecompressor.__init__uZstdDecompressor.memory_sizeTlFtuZstdDecompressor.decompressuZstdDecompressor.stream_readeradecompressobjuZstdDecompressor.decompressobjuZstdDecompressor.stream_writeruZstdDecompressor.copy_streamadecompress_content_dict_chainuZstdDecompressor.decompress_content_dict_chainTnlamulti_decompress_to_bufferuZstdDecompressor.multi_decompress_to_bufferuZstdDecompressor._ensure_dctxuzstandard\backend_cffi.pyu<module zstandard.backend_cffi>Ta__class__TaselfTaselfaexc_typeaexc_valueaexc_tbTaselfwiTaselfacompressorachunk_sizeTaselfacompressorasourcearead_sizeaclosefdTaselfacompressorawriterasource_sizeawrite_sizeawrite_return_readaclosefdazresultTaselfadataadict_typewkwdTaselfadecompressorasourcearead_sizearead_across_framesaclosefdTaselfadecompressorawrite_sizearead_across_framesTaselfadecompressorawriterawrite_sizeawrite_return_readaclosefdTaselfadict_dataamax_window_sizeaformatadctxTaselfaformatacompression_levelawindow_logahash_logachain_logasearch_logamin_matchatarget_lengthastrategyawrite_content_sizeawrite_checksumawrite_dict_idajob_sizeaoverlap_logaforce_max_windowaenable_ldmaldm_hash_logaldm_min_matchaldm_bucket_size_logaldm_hash_rate_logathreadsaparamsTaselfafparamsT
aselfaleveladict_dataacompression_paramsawrite_checksumawrite_content_sizeawrite_dict_idathreadsaparamsacctxTaselfaout_bufferaold_posazresultTaselfaddictTaselfaout_bufferazresultTaselfaload_dictazresultTaparamsaparamaresultazresultTaparamsaresaattrsaparamavalueTaselfadataTaparamsaparamavalueazresultTaselfazresultadict_dataTazresultTaselfasizeachunk_sizeazresultTaselfwfTaselfadataadata_bufferadest_sizeaoutazresultaout_bufferain_bufferTaselfadataadata_bufferasourceachunksazresultTaselfadataadata_bufferazresultTaselfasizeazresultacobjT
aselfaifhaofharead_sizeawrite_sizeain_bufferaout_bufferadst_bufferatotal_readatotal_writeadataadata_bufferazresultTaselfaifhaofhasizearead_sizeawrite_sizeazresultain_bufferaout_bufferadst_bufferatotal_readatotal_writeadataadata_bufferTaselfadataain_bufferaout_bufferadata_bufferadst_bufferachunksazresultT
aselfadataamax_output_sizearead_across_framesaallow_extra_dataadata_bufferaoutput_sizearesult_bufferaresult_sizeaout_bufferain_bufferazresultacountTaselfaframesachunkachunk_bufferaparamsazresultalast_bufferaout_bufferain_bufferwiadest_bufferTaselfawrite_sizearead_across_framesTaselfazresultTaselfaflush_modeaflushatotal_writeaout_bufferain_bufferazresultwfTaselfaflush_modeaz_flush_modeain_bufferachunksazresultTaselfalengthTadataadata_bufferasizeTadataadata_bufferazresultTaselfaprogressionTalevelasource_sizeadict_sizeakwargsaparamsaargsaargaattrTadataaparamsadata_bufferazresultTaselfadataathreadsTaselfaframesadecompressed_sizesathreadsTaselfalevelacompression_paramsacparamsacdictTaselfasizeTaselfasizeadst_bufferaout_bufferTaselfasizeadst_bufferaout_bufferaold_posazresultTaselfareaderaread_sizeawrite_sizeaskip_bytesahave_readabuffer_offsetasizeain_bufferaout_bufferadst_bufferaread_resultaremainingaslice_sizearead_bufferazresultadataTaselfareaderasizearead_sizeawrite_sizeahave_readabuffer_offsetazresultain_bufferaout_bufferadst_bufferaread_resultaremainingaslice_sizearead_bufferadataTaselfachunksachunkTaselfwbTaselfwbadest_bufferaout_bufferTaselfwbadest_bufferaout_bufferaold_posazresultTaselfahintTaselfaoffsetawhenceTaselfaposawhencearead_amountaresultTaselfasourcearead_sizearead_across_framesaclosefdTaselfasourceasizearead_sizeaclosefdazresultTaselfawriterasizeawrite_sizeawrite_return_readaclosefdTaselfawriterawrite_sizeawrite_return_readaclosefdTadict_sizeasampleswkwdwfasplit_pointaaccelanotificationsadict_idalevelastepsathreadsatotal_sizeasamples_bufferasample_sizesaoffsetwiasamplewladict_dataadparamsazresultamsgTaselfadataatotal_writeadata_bufferain_bufferaout_bufferazresultTaselfadataatotal_writeain_bufferaout_bufferadata_bufferadst_bufferadctxazresultTaselfaignoredTaselfalinesu.zstandard2eareplaceTwtuTwrarbaZstdDecompressorwrarbTwwawbwaaabwxaxbaZstdCompressorwwaendswithTwbwbaValueErroruInvalid mode: {!r}ahasattraosaPathLikeastrabytesaisinstancearaw_open_modeareadawriteaboolaTypeErrorTufilename must be a str, bytes, file or PathLike objectastream_readerTaclosefdastream_writeraRuntimeErrorTulogic error in zstandard.open() handling open modeanormalized_modeaioaTextIOWrapperTaencodingaerrorsanewlineuCreate a file object with zstd (de)compression.

    The object returned from this function will be a
    :py:class:`ZstdDecompressionReader` if opened for reading in binary mode,
    a :py:class:`ZstdCompressionWriter` if opened for writing in binary mode,
    or an ``io.TextIOWrapper`` if opened for reading or writing in text mode.

    :param filename:
       ``bytes``, ``str``, or ``os.PathLike`` defining a file to open or a
       file object (with a ``read()`` or ``write()`` method).
    :param mode:
       ``str`` File open mode. Accepts any of the open modes recognized by
       ``open()``.
    :param cctx:
       ``ZstdCompressor`` to use for compression. If not specified and file
       is opened for writing, the default ``ZstdCompressor`` will be used.
    :param dctx:
       ``ZstdDecompressor`` to use for decompression. If not specified and file
       is opened for reading, the default ``ZstdDecompressor`` will be used.
    :param encoding:
        ``str`` that defines text encoding to use when file is opened in text
        mode.
    :param errors:
       ``str`` defining text encoding error handling mode.
    :param newline:
       ``str`` defining newline to use in text mode.
    :param closefd:
       ``bool`` whether to close the file when the returned object is closed.
        Only used if a file object is passed. If a filename is specified, the
        opened file is always closed when the returned object is closed.
    TalevelacompressuCompress source data using the zstd compression format.

    This performs one-shot compression using basic/default compression
    settings.

    This method is provided for convenience and is equivalent to calling
    ``ZstdCompressor(level=level).compress(data)``.

    If you find yourself calling this function in a tight loop,
    performance will be greater if you construct a single ``ZstdCompressor``
    and repeatedly call ``compress()`` on it.
    adecompressTamax_output_sizeuDecompress a zstd frame into its original data.

    This performs one-shot decompression using basic/default compression
    settings.

    This method is provided for convenience and is equivalent to calling
    ``ZstdDecompressor().decompress(data, max_output_size=max_output_size)``.

    If you find yourself calling this function in a tight loop, performance
    will be greater if you construct a single ``ZstdDecompressor`` and
    repeatedly call ``decompress()`` on it.
    uPython interface to the Zstandard (zstd) compression library.a__doc__a__file__apathadirnameaenvironagetTaNUITKA_PACKAGE_zstandardu\not_existinga__path__a__spec__aoriginahas_locationasubmodule_search_locationsa__cached__aabsolute_importaunicode_literalsabuiltinsaplatformlaByteStringTaPYTHON_ZSTANDARD_IMPORT_POLICYadefaulta_module_policyadefaultapython_implementationTaCPythonabackend_cTw*lacextabackendTaPyPyabackend_cffiacffiaImportErroracffi_fallbackarustabackend_rustuunknown module import policy: %s; use default, cffi_fallback, cext, or cffiu0.22.0a__version__a_MODE_CLOSEDa_MODE_READla_MODE_WRITETarbnnnnnnaopenTladataalevelaintareturnTlamax_output_sizeuzstandard\__init__.pyu<module zstandard>TadataalevelacctxTadataamax_output_sizeadctxTafilenameamodeacctxadctxaencodingaerrorsanewlineaclosefdanormalized_modeaopen_modearaw_open_modeatypesainner_fhafhu.
